{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A_model_has_mul_modules.ipynb",
      "provenance": [],
      "mount_file_id": "1KWtPA8a9F10EtYi1ZgaxukBHMkZj-4ix",
      "authorship_tag": "ABX9TyMQJ1HAU8Mwzrs99XMyUZ3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1995subhankar1995/Torch-Tutorials/blob/main/A_model_has_mul_modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0OovVHEFwe1"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "\n",
        "#### Finetuning the convnet ####\n",
        "# Load a pretrained model and reset final fully connected layer.\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizerâ€™s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-gvdBsho7NK",
        "outputId": "8eb0f326-6d09-4c09-aa60-dcc6744d546d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "lr = 1e-3\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmLnZrGphcZ"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset)\n",
        "num_classes = 10\n",
        "train = [[] for i in range(num_classes)]\n",
        "for i in range(num_classes):\n",
        "  for image, label in train_loader:\n",
        "    image = image.view(3, 32, 32).numpy()\n",
        "    if i == label.numpy()[0]:\n",
        "      train[i].append(image)\n",
        "  train[i] = torch.tensor(train[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSM1IHFpVNxF",
        "outputId": "e3022252-c8d4-4e87-b636-d91aa2707520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "for i in range(num_classes):\n",
        "  print('Shape:', train[i].shape, 'Label:', i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 0\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 1\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 2\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 3\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 4\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 5\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 6\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 7\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 8\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k474UhrXP0K"
      },
      "source": [
        "def next_data(data, batch_size = 64, shuffle = True):\n",
        "  data = torch.utils.data.DataLoader(data, batch_size, shuffle = True)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaljXJM-pj00",
        "outputId": "e1afac03-6eaa-4a22-ef3e-9eb728a2daa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.maxpool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.lin1 = nn.Linear(5 * 16 * 5, 120)\n",
        "    self.lin2 = nn.Linear(120, 20)\n",
        "    self.drop = nn.Dropout(p = 0.5)\n",
        "    self.lin3 = nn.Linear(20, 2)\n",
        "    self.soft = nn.LogSoftmax(dim = 1)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = x.view(-1, 16 * 5 * 5)\n",
        "    x = F.relu(self.lin1(x))\n",
        "    x = F.relu(self.lin2(x))\n",
        "    #x = self.drop(x)\n",
        "    x = self.lin3(x)\n",
        "    x = self.soft(x)\n",
        "    return x\n",
        "model_classifier = Classifier()\n",
        "model_classifier.to(device)\n",
        "print(model_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (lin1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (lin2): Linear(in_features=120, out_features=20, bias=True)\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (lin3): Linear(in_features=20, out_features=2, bias=True)\n",
            "  (soft): LogSoftmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgsh3qVe_8R9",
        "outputId": "7e917fef-cea4-4a74-f9df-7c0fe5d7f085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "batch_size = 64\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
        "model_classifier.train()\n",
        "for i in range(num_classes):\n",
        "  train_data = next_data(train[i], batch_size = 64, shuffle = True)\n",
        "  for e in range(num_epochs):\n",
        "    loss_sum = 0\n",
        "    for images in train_data:\n",
        "      optimizer.zero_grad()\n",
        "      images = images.to(device)\n",
        "      out = model_classifier(images)\n",
        "      \n",
        "      ones = torch.ones(len(images), dtype = torch.int64)\n",
        "      ones = ones.to(device)\n",
        "      loss = criterion(out, ones)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_sum += loss\n",
        "    print('Loss:', loss_sum, 'Epoch:', e + 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: tensor(55.1438, grad_fn=<AddBackward0>) Epoch: 1\n",
            "Loss: tensor(55.1401, grad_fn=<AddBackward0>) Epoch: 2\n",
            "Loss: tensor(55.1404, grad_fn=<AddBackward0>) Epoch: 3\n",
            "Loss: tensor(55.1409, grad_fn=<AddBackward0>) Epoch: 4\n",
            "Loss: tensor(55.1413, grad_fn=<AddBackward0>) Epoch: 5\n",
            "Loss: tensor(55.1374, grad_fn=<AddBackward0>) Epoch: 6\n",
            "Loss: tensor(55.1373, grad_fn=<AddBackward0>) Epoch: 7\n",
            "Loss: tensor(55.1384, grad_fn=<AddBackward0>) Epoch: 8\n",
            "Loss: tensor(55.1336, grad_fn=<AddBackward0>) Epoch: 9\n",
            "Loss: tensor(55.1401, grad_fn=<AddBackward0>) Epoch: 10\n",
            "Loss: tensor(55.1698, grad_fn=<AddBackward0>) Epoch: 1\n",
            "Loss: tensor(55.1674, grad_fn=<AddBackward0>) Epoch: 2\n",
            "Loss: tensor(55.1667, grad_fn=<AddBackward0>) Epoch: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-49fefd080d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRsonFHFNi4L",
        "outputId": "b1015563-7c71-4b51-debc-100d891a43d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    for _ in range(4):\n",
        "      self.lin1 = nn.Linear(30, 4)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.lin2 = nn.Linear(4, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.lin1(x))\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (lin1): Linear(in_features=30, out_features=4, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (lin2): Linear(in_features=4, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH6-2FcwimRJ",
        "outputId": "27725f2c-d95d-46b4-c218-509e5dae2a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def Diff_Loss(p_out, S_out):\n",
        "  \n",
        "  return torch.norm(torch.matmul(torch.transpose(p_out, 1, 0), S_out))**2\n",
        "\n",
        "a = torch.randn(30, 10)\n",
        "b = torch.randn(30, 12)  \n",
        "print(Diff_Loss(a, b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4060.5161)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYdFHWymmaKS"
      },
      "source": [
        "# models can be appended in a list. We can train separate models for each task and append them in a list so they can be used during testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqr2RtZFHjT_",
        "outputId": "e259d1a4-2704-4afa-a02d-dea71a725b86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "path = '/content/drive/My Drive/ColabNotebooks/results/save1.pth'\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.lin1 = nn.Linear(2, 10)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.lin2 = nn.Linear(10, 20)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.lin1(x))\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "xx = np.random.random((40, 2))\n",
        "x = [[]for i in range(2)]\n",
        "y = [[]for i in range(2)]\n",
        "\n",
        "for j in range(40):\n",
        "  if j % 2 == 0:\n",
        "    x[0].append(xx[j])\n",
        "    y[0].append(1)\n",
        "  else:\n",
        "    x[1].append(xx[j])\n",
        "    y[1].append(0)\n",
        "\n",
        "x[0] = torch.from_numpy(np.array(x[0]).astype(np.float32))\n",
        "x[1] = torch.from_numpy(np.array(x[1]).astype(np.float32))\n",
        "y[0] = torch.from_numpy(np.array(y[0]).astype(np.float32))\n",
        "y[1] = torch.from_numpy(np.array(y[1]).astype(np.float32))\n",
        "model = Net()\n",
        "torch.save(model.state_dict(), path)\n",
        "print(x[0].shape, x[1].shape, y[0].shape, y[1].shape, 'shape')\n",
        "save = []\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "criterion = nn.MSELoss()\n",
        "for i in range(2):\n",
        "  for e in range(10):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(x[i])\n",
        "    loss = criterion(out, y[i])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('epoch:', e + 1, 'loss:', loss)\n",
        "  save.append(deepcopy(model))\n",
        "  model.load_state_dict(torch.load(path))\n",
        "\n",
        "\n",
        "print(save)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 2]) torch.Size([20, 2]) torch.Size([20]) torch.Size([20]) shape\n",
            "epoch: 1 loss: tensor(1.0214, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(1.0166, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(1.0118, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(1.0071, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(1.0023, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(0.9977, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(0.9930, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(0.9884, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(0.9838, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(0.9792, grad_fn=<MseLossBackward>)\n",
            "epoch: 1 loss: tensor(0.0971, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(0.0965, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(0.0959, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(0.0953, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(0.0947, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(0.0942, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(0.0936, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(0.0930, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(0.0925, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(0.0919, grad_fn=<MseLossBackward>)\n",
            "[Net(\n",
            "  (lin1): Linear(in_features=2, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (lin2): Linear(in_features=10, out_features=20, bias=True)\n",
            "), Net(\n",
            "  (lin1): Linear(in_features=2, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (lin2): Linear(in_features=10, out_features=20, bias=True)\n",
            ")]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAcfjP09EYmS",
        "outputId": "dcb0b78b-befd-485a-a58b-517c1bd01506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(2):\n",
        "  for p in save[i].parameters():\n",
        "    print(p[0])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.1271,  0.0361], grad_fn=<SelectBackward>)\n",
            "tensor(-0.2887, grad_fn=<SelectBackward>)\n",
            "tensor([-0.1882, -0.0921,  0.0462,  0.1552,  0.0955, -0.1248, -0.0467,  0.1713,\n",
            "        -0.0779,  0.1496], grad_fn=<SelectBackward>)\n",
            "tensor(0.0259, grad_fn=<SelectBackward>)\n",
            "tensor([-0.1271,  0.0361], grad_fn=<SelectBackward>)\n",
            "tensor(-0.2887, grad_fn=<SelectBackward>)\n",
            "tensor([-0.1882, -0.0923,  0.0388,  0.1550,  0.0955, -0.1324, -0.0467,  0.1713,\n",
            "        -0.0815,  0.1496], grad_fn=<SelectBackward>)\n",
            "tensor(0.0160, grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvIk-blwmBJ_"
      },
      "source": [
        "# A model has 2 modules. They can be trained separately for different tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wFrhMixVx5B",
        "outputId": "fdf03ef8-820f-4573-f549-273453cb519d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.head = torch.nn.ModuleList()\n",
        "    for _ in range(2):\n",
        "      self.head.append(\n",
        "        torch.nn.Sequential(\n",
        "          nn.Linear(2, 10),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(10, 20)\n",
        "          ))\n",
        "  def forward(self, x, i):\n",
        "    return self.head[i].forward(x)\n",
        "\n",
        "xx = np.random.random((40, 2))\n",
        "x = [[]for i in range(2)]\n",
        "y = [[]for i in range(2)]\n",
        "\n",
        "for j in range(40):\n",
        "  if j % 2 == 0:\n",
        "    x[0].append(xx[j])\n",
        "    y[0].append(1)\n",
        "  else:\n",
        "    x[1].append(xx[j])\n",
        "    y[1].append(0)\n",
        "\n",
        "x[0] = torch.from_numpy(np.array(x[0]).astype(np.float32))\n",
        "x[1] = torch.from_numpy(np.array(x[1]).astype(np.float32))\n",
        "y[0] = torch.from_numpy(np.array(y[0]).astype(np.float32))\n",
        "y[1] = torch.from_numpy(np.array(y[1]).astype(np.float32))\n",
        "model = Net()\n",
        "print(x[0].shape, x[1].shape, y[0].shape, y[1].shape, 'shape')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "criterion = nn.MSELoss()\n",
        "for i in range(2):\n",
        "  for e in range(10):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(x[i], i)\n",
        "    loss = criterion(out, y[i])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('epoch:', e + 1, 'loss:', loss)\n",
        "\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 2]) torch.Size([20, 2]) torch.Size([20]) torch.Size([20]) shape\n",
            "epoch: 1 loss: tensor(1.0727, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(1.0675, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(1.0624, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(1.0573, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(1.0522, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(1.0472, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(1.0422, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(1.0373, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(1.0324, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(1.0275, grad_fn=<MseLossBackward>)\n",
            "epoch: 1 loss: tensor(0.0468, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(0.0466, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(0.0464, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(0.0462, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(0.0460, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(0.0458, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(0.0456, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(0.0455, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(0.0453, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(0.0451, grad_fn=<MseLossBackward>)\n",
            "Net(\n",
            "  (head): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=2, out_features=10, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=10, out_features=20, bias=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=2, out_features=10, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=10, out_features=20, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC6esRmOlFl4",
        "outputId": "18505e5d-6a3c-4687-9573-96416804d9d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(2):\n",
        "  for p in save[i].parameters():\n",
        "    print(p[0])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.1271,  0.0361], grad_fn=<SelectBackward>)\n",
            "tensor(-0.2887, grad_fn=<SelectBackward>)\n",
            "tensor([-0.1882, -0.0921,  0.0462,  0.1552,  0.0955, -0.1248, -0.0467,  0.1713,\n",
            "        -0.0779,  0.1496], grad_fn=<SelectBackward>)\n",
            "tensor(0.0259, grad_fn=<SelectBackward>)\n",
            "tensor([-0.1271,  0.0361], grad_fn=<SelectBackward>)\n",
            "tensor(-0.2887, grad_fn=<SelectBackward>)\n",
            "tensor([-0.1882, -0.0923,  0.0388,  0.1550,  0.0955, -0.1324, -0.0467,  0.1713,\n",
            "        -0.0815,  0.1496], grad_fn=<SelectBackward>)\n",
            "tensor(0.0160, grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}