{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A_model_has_mul_modules.ipynb",
      "provenance": [],
      "mount_file_id": "1KWtPA8a9F10EtYi1ZgaxukBHMkZj-4ix",
      "authorship_tag": "ABX9TyM3ZMUzbEFlSRVh4byWKXKR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1995subhankar1995/Torch-Tutorials/blob/main/A_model_has_mul_modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0OovVHEFwe1"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "\n",
        "#### Finetuning the convnet ####\n",
        "# Load a pretrained model and reset final fully connected layer.\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizerâ€™s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-gvdBsho7NK",
        "outputId": "8eb0f326-6d09-4c09-aa60-dcc6744d546d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "lr = 1e-3\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmLnZrGphcZ"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset)\n",
        "num_classes = 10\n",
        "train = [[] for i in range(num_classes)]\n",
        "for i in range(num_classes):\n",
        "  for image, label in train_loader:\n",
        "    image = image.view(3, 32, 32).numpy()\n",
        "    if i == label.numpy()[0]:\n",
        "      train[i].append(image)\n",
        "  train[i] = torch.tensor(train[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSM1IHFpVNxF",
        "outputId": "e3022252-c8d4-4e87-b636-d91aa2707520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "for i in range(num_classes):\n",
        "  print('Shape:', train[i].shape, 'Label:', i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 0\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 1\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 2\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 3\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 4\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 5\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 6\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 7\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 8\n",
            "Shape: torch.Size([5000, 3, 32, 32]) Label: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k474UhrXP0K"
      },
      "source": [
        "def next_data(data, batch_size = 64, shuffle = True):\n",
        "  data = torch.utils.data.DataLoader(data, batch_size, shuffle = True)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaljXJM-pj00",
        "outputId": "e1afac03-6eaa-4a22-ef3e-9eb728a2daa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.maxpool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.lin1 = nn.Linear(5 * 16 * 5, 120)\n",
        "    self.lin2 = nn.Linear(120, 20)\n",
        "    self.drop = nn.Dropout(p = 0.5)\n",
        "    self.lin3 = nn.Linear(20, 2)\n",
        "    self.soft = nn.LogSoftmax(dim = 1)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = x.view(-1, 16 * 5 * 5)\n",
        "    x = F.relu(self.lin1(x))\n",
        "    x = F.relu(self.lin2(x))\n",
        "    #x = self.drop(x)\n",
        "    x = self.lin3(x)\n",
        "    x = self.soft(x)\n",
        "    return x\n",
        "model_classifier = Classifier()\n",
        "model_classifier.to(device)\n",
        "print(model_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (lin1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (lin2): Linear(in_features=120, out_features=20, bias=True)\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (lin3): Linear(in_features=20, out_features=2, bias=True)\n",
            "  (soft): LogSoftmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgsh3qVe_8R9",
        "outputId": "7e917fef-cea4-4a74-f9df-7c0fe5d7f085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "batch_size = 64\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
        "model_classifier.train()\n",
        "for i in range(num_classes):\n",
        "  train_data = next_data(train[i], batch_size = 64, shuffle = True)\n",
        "  for e in range(num_epochs):\n",
        "    loss_sum = 0\n",
        "    for images in train_data:\n",
        "      optimizer.zero_grad()\n",
        "      images = images.to(device)\n",
        "      out = model_classifier(images)\n",
        "      \n",
        "      ones = torch.ones(len(images), dtype = torch.int64)\n",
        "      ones = ones.to(device)\n",
        "      loss = criterion(out, ones)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_sum += loss\n",
        "    print('Loss:', loss_sum, 'Epoch:', e + 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: tensor(55.1438, grad_fn=<AddBackward0>) Epoch: 1\n",
            "Loss: tensor(55.1401, grad_fn=<AddBackward0>) Epoch: 2\n",
            "Loss: tensor(55.1404, grad_fn=<AddBackward0>) Epoch: 3\n",
            "Loss: tensor(55.1409, grad_fn=<AddBackward0>) Epoch: 4\n",
            "Loss: tensor(55.1413, grad_fn=<AddBackward0>) Epoch: 5\n",
            "Loss: tensor(55.1374, grad_fn=<AddBackward0>) Epoch: 6\n",
            "Loss: tensor(55.1373, grad_fn=<AddBackward0>) Epoch: 7\n",
            "Loss: tensor(55.1384, grad_fn=<AddBackward0>) Epoch: 8\n",
            "Loss: tensor(55.1336, grad_fn=<AddBackward0>) Epoch: 9\n",
            "Loss: tensor(55.1401, grad_fn=<AddBackward0>) Epoch: 10\n",
            "Loss: tensor(55.1698, grad_fn=<AddBackward0>) Epoch: 1\n",
            "Loss: tensor(55.1674, grad_fn=<AddBackward0>) Epoch: 2\n",
            "Loss: tensor(55.1667, grad_fn=<AddBackward0>) Epoch: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-49fefd080d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRsonFHFNi4L",
        "outputId": "b1015563-7c71-4b51-debc-100d891a43d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    for _ in range(4):\n",
        "      self.lin1 = nn.Linear(30, 4)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.lin2 = nn.Linear(4, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.lin1(x))\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (lin1): Linear(in_features=30, out_features=4, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (lin2): Linear(in_features=4, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH6-2FcwimRJ",
        "outputId": "27725f2c-d95d-46b4-c218-509e5dae2a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def Diff_Loss(p_out, S_out):\n",
        "  \n",
        "  return torch.norm(torch.matmul(torch.transpose(p_out, 1, 0), S_out))**2\n",
        "\n",
        "a = torch.randn(30, 10)\n",
        "b = torch.randn(30, 12)  \n",
        "print(Diff_Loss(a, b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4060.5161)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyHXMKodjCMW",
        "outputId": "6e1cf6f9-44df-409b-8ac9-69926bbc1c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = [[] for i in range(3)]\n",
        "print(a[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqr2RtZFHjT_",
        "outputId": "71fb32dc-31f2-44b1-ab47-8db3abcac24b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "path = '/content/drive/My Drive/ColabNotebooks/results/save1.pth'\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.lin1 = nn.Linear(2, 10)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.lin2 = nn.Linear(10, 20)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.lin1(x))\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "xx = np.random.random((40, 2))\n",
        "x = [[]for i in range(2)]\n",
        "y = [[]for i in range(2)]\n",
        "\n",
        "for j in range(40):\n",
        "  if j % 2 == 0:\n",
        "    x[0].append(xx[j])\n",
        "    y[0].append(1)\n",
        "  else:\n",
        "    x[1].append(xx[j])\n",
        "    y[1].append(0)\n",
        "\n",
        "x[0] = torch.from_numpy(np.array(x[0]).astype(np.float32))\n",
        "x[1] = torch.from_numpy(np.array(x[1]).astype(np.float32))\n",
        "y[0] = torch.from_numpy(np.array(y[0]).astype(np.float32))\n",
        "y[1] = torch.from_numpy(np.array(y[1]).astype(np.float32))\n",
        "model = Net()\n",
        "torch.save(model.state_dict(), path)\n",
        "print(x[0].shape, x[1].shape, y[0].shape, y[1].shape, 'shape')\n",
        "save = []\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "criterion = nn.MSELoss()\n",
        "for i in range(2):\n",
        "  for e in range(100):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(x[i])\n",
        "    loss = criterion(out, y[i])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('epoch:', e + 1, 'loss:', loss)\n",
        "  save.append(deepcopy(model))\n",
        "  model.load_state_dict(torch.load(path))\n",
        "\n",
        "\n",
        "print(save)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 2]) torch.Size([20, 2]) torch.Size([20]) torch.Size([20]) shape\n",
            "epoch: 1 loss: tensor(1.0908, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(1.0839, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(1.0771, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(1.0703, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(1.0636, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(1.0569, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(1.0503, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(1.0438, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(1.0373, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(1.0309, grad_fn=<MseLossBackward>)\n",
            "epoch: 11 loss: tensor(1.0245, grad_fn=<MseLossBackward>)\n",
            "epoch: 12 loss: tensor(1.0182, grad_fn=<MseLossBackward>)\n",
            "epoch: 13 loss: tensor(1.0119, grad_fn=<MseLossBackward>)\n",
            "epoch: 14 loss: tensor(1.0057, grad_fn=<MseLossBackward>)\n",
            "epoch: 15 loss: tensor(0.9995, grad_fn=<MseLossBackward>)\n",
            "epoch: 16 loss: tensor(0.9934, grad_fn=<MseLossBackward>)\n",
            "epoch: 17 loss: tensor(0.9873, grad_fn=<MseLossBackward>)\n",
            "epoch: 18 loss: tensor(0.9813, grad_fn=<MseLossBackward>)\n",
            "epoch: 19 loss: tensor(0.9753, grad_fn=<MseLossBackward>)\n",
            "epoch: 20 loss: tensor(0.9693, grad_fn=<MseLossBackward>)\n",
            "epoch: 21 loss: tensor(0.9634, grad_fn=<MseLossBackward>)\n",
            "epoch: 22 loss: tensor(0.9576, grad_fn=<MseLossBackward>)\n",
            "epoch: 23 loss: tensor(0.9517, grad_fn=<MseLossBackward>)\n",
            "epoch: 24 loss: tensor(0.9460, grad_fn=<MseLossBackward>)\n",
            "epoch: 25 loss: tensor(0.9402, grad_fn=<MseLossBackward>)\n",
            "epoch: 26 loss: tensor(0.9345, grad_fn=<MseLossBackward>)\n",
            "epoch: 27 loss: tensor(0.9289, grad_fn=<MseLossBackward>)\n",
            "epoch: 28 loss: tensor(0.9233, grad_fn=<MseLossBackward>)\n",
            "epoch: 29 loss: tensor(0.9177, grad_fn=<MseLossBackward>)\n",
            "epoch: 30 loss: tensor(0.9121, grad_fn=<MseLossBackward>)\n",
            "epoch: 31 loss: tensor(0.9066, grad_fn=<MseLossBackward>)\n",
            "epoch: 32 loss: tensor(0.9012, grad_fn=<MseLossBackward>)\n",
            "epoch: 33 loss: tensor(0.8957, grad_fn=<MseLossBackward>)\n",
            "epoch: 34 loss: tensor(0.8903, grad_fn=<MseLossBackward>)\n",
            "epoch: 35 loss: tensor(0.8850, grad_fn=<MseLossBackward>)\n",
            "epoch: 36 loss: tensor(0.8796, grad_fn=<MseLossBackward>)\n",
            "epoch: 37 loss: tensor(0.8743, grad_fn=<MseLossBackward>)\n",
            "epoch: 38 loss: tensor(0.8691, grad_fn=<MseLossBackward>)\n",
            "epoch: 39 loss: tensor(0.8638, grad_fn=<MseLossBackward>)\n",
            "epoch: 40 loss: tensor(0.8586, grad_fn=<MseLossBackward>)\n",
            "epoch: 41 loss: tensor(0.8535, grad_fn=<MseLossBackward>)\n",
            "epoch: 42 loss: tensor(0.8483, grad_fn=<MseLossBackward>)\n",
            "epoch: 43 loss: tensor(0.8432, grad_fn=<MseLossBackward>)\n",
            "epoch: 44 loss: tensor(0.8381, grad_fn=<MseLossBackward>)\n",
            "epoch: 45 loss: tensor(0.8331, grad_fn=<MseLossBackward>)\n",
            "epoch: 46 loss: tensor(0.8281, grad_fn=<MseLossBackward>)\n",
            "epoch: 47 loss: tensor(0.8231, grad_fn=<MseLossBackward>)\n",
            "epoch: 48 loss: tensor(0.8181, grad_fn=<MseLossBackward>)\n",
            "epoch: 49 loss: tensor(0.8132, grad_fn=<MseLossBackward>)\n",
            "epoch: 50 loss: tensor(0.8083, grad_fn=<MseLossBackward>)\n",
            "epoch: 51 loss: tensor(0.8034, grad_fn=<MseLossBackward>)\n",
            "epoch: 52 loss: tensor(0.7985, grad_fn=<MseLossBackward>)\n",
            "epoch: 53 loss: tensor(0.7937, grad_fn=<MseLossBackward>)\n",
            "epoch: 54 loss: tensor(0.7889, grad_fn=<MseLossBackward>)\n",
            "epoch: 55 loss: tensor(0.7841, grad_fn=<MseLossBackward>)\n",
            "epoch: 56 loss: tensor(0.7794, grad_fn=<MseLossBackward>)\n",
            "epoch: 57 loss: tensor(0.7746, grad_fn=<MseLossBackward>)\n",
            "epoch: 58 loss: tensor(0.7699, grad_fn=<MseLossBackward>)\n",
            "epoch: 59 loss: tensor(0.7652, grad_fn=<MseLossBackward>)\n",
            "epoch: 60 loss: tensor(0.7606, grad_fn=<MseLossBackward>)\n",
            "epoch: 61 loss: tensor(0.7559, grad_fn=<MseLossBackward>)\n",
            "epoch: 62 loss: tensor(0.7513, grad_fn=<MseLossBackward>)\n",
            "epoch: 63 loss: tensor(0.7467, grad_fn=<MseLossBackward>)\n",
            "epoch: 64 loss: tensor(0.7422, grad_fn=<MseLossBackward>)\n",
            "epoch: 65 loss: tensor(0.7376, grad_fn=<MseLossBackward>)\n",
            "epoch: 66 loss: tensor(0.7331, grad_fn=<MseLossBackward>)\n",
            "epoch: 67 loss: tensor(0.7286, grad_fn=<MseLossBackward>)\n",
            "epoch: 68 loss: tensor(0.7241, grad_fn=<MseLossBackward>)\n",
            "epoch: 69 loss: tensor(0.7197, grad_fn=<MseLossBackward>)\n",
            "epoch: 70 loss: tensor(0.7152, grad_fn=<MseLossBackward>)\n",
            "epoch: 71 loss: tensor(0.7108, grad_fn=<MseLossBackward>)\n",
            "epoch: 72 loss: tensor(0.7064, grad_fn=<MseLossBackward>)\n",
            "epoch: 73 loss: tensor(0.7021, grad_fn=<MseLossBackward>)\n",
            "epoch: 74 loss: tensor(0.6977, grad_fn=<MseLossBackward>)\n",
            "epoch: 75 loss: tensor(0.6934, grad_fn=<MseLossBackward>)\n",
            "epoch: 76 loss: tensor(0.6890, grad_fn=<MseLossBackward>)\n",
            "epoch: 77 loss: tensor(0.6848, grad_fn=<MseLossBackward>)\n",
            "epoch: 78 loss: tensor(0.6805, grad_fn=<MseLossBackward>)\n",
            "epoch: 79 loss: tensor(0.6762, grad_fn=<MseLossBackward>)\n",
            "epoch: 80 loss: tensor(0.6720, grad_fn=<MseLossBackward>)\n",
            "epoch: 81 loss: tensor(0.6678, grad_fn=<MseLossBackward>)\n",
            "epoch: 82 loss: tensor(0.6636, grad_fn=<MseLossBackward>)\n",
            "epoch: 83 loss: tensor(0.6594, grad_fn=<MseLossBackward>)\n",
            "epoch: 84 loss: tensor(0.6552, grad_fn=<MseLossBackward>)\n",
            "epoch: 85 loss: tensor(0.6511, grad_fn=<MseLossBackward>)\n",
            "epoch: 86 loss: tensor(0.6469, grad_fn=<MseLossBackward>)\n",
            "epoch: 87 loss: tensor(0.6428, grad_fn=<MseLossBackward>)\n",
            "epoch: 88 loss: tensor(0.6387, grad_fn=<MseLossBackward>)\n",
            "epoch: 89 loss: tensor(0.6347, grad_fn=<MseLossBackward>)\n",
            "epoch: 90 loss: tensor(0.6306, grad_fn=<MseLossBackward>)\n",
            "epoch: 91 loss: tensor(0.6266, grad_fn=<MseLossBackward>)\n",
            "epoch: 92 loss: tensor(0.6225, grad_fn=<MseLossBackward>)\n",
            "epoch: 93 loss: tensor(0.6185, grad_fn=<MseLossBackward>)\n",
            "epoch: 94 loss: tensor(0.6146, grad_fn=<MseLossBackward>)\n",
            "epoch: 95 loss: tensor(0.6106, grad_fn=<MseLossBackward>)\n",
            "epoch: 96 loss: tensor(0.6066, grad_fn=<MseLossBackward>)\n",
            "epoch: 97 loss: tensor(0.6027, grad_fn=<MseLossBackward>)\n",
            "epoch: 98 loss: tensor(0.5988, grad_fn=<MseLossBackward>)\n",
            "epoch: 99 loss: tensor(0.5949, grad_fn=<MseLossBackward>)\n",
            "epoch: 100 loss: tensor(0.5910, grad_fn=<MseLossBackward>)\n",
            "epoch: 1 loss: tensor(0.0737, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(0.0732, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(0.0727, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(0.0722, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(0.0717, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(0.0712, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(0.0707, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(0.0702, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(0.0697, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(0.0693, grad_fn=<MseLossBackward>)\n",
            "epoch: 11 loss: tensor(0.0688, grad_fn=<MseLossBackward>)\n",
            "epoch: 12 loss: tensor(0.0683, grad_fn=<MseLossBackward>)\n",
            "epoch: 13 loss: tensor(0.0679, grad_fn=<MseLossBackward>)\n",
            "epoch: 14 loss: tensor(0.0674, grad_fn=<MseLossBackward>)\n",
            "epoch: 15 loss: tensor(0.0669, grad_fn=<MseLossBackward>)\n",
            "epoch: 16 loss: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0665, grad_fn=<MseLossBackward>)\n",
            "epoch: 17 loss: tensor(0.0660, grad_fn=<MseLossBackward>)\n",
            "epoch: 18 loss: tensor(0.0656, grad_fn=<MseLossBackward>)\n",
            "epoch: 19 loss: tensor(0.0652, grad_fn=<MseLossBackward>)\n",
            "epoch: 20 loss: tensor(0.0647, grad_fn=<MseLossBackward>)\n",
            "epoch: 21 loss: tensor(0.0643, grad_fn=<MseLossBackward>)\n",
            "epoch: 22 loss: tensor(0.0639, grad_fn=<MseLossBackward>)\n",
            "epoch: 23 loss: tensor(0.0634, grad_fn=<MseLossBackward>)\n",
            "epoch: 24 loss: tensor(0.0630, grad_fn=<MseLossBackward>)\n",
            "epoch: 25 loss: tensor(0.0626, grad_fn=<MseLossBackward>)\n",
            "epoch: 26 loss: tensor(0.0622, grad_fn=<MseLossBackward>)\n",
            "epoch: 27 loss: tensor(0.0618, grad_fn=<MseLossBackward>)\n",
            "epoch: 28 loss: tensor(0.0614, grad_fn=<MseLossBackward>)\n",
            "epoch: 29 loss: tensor(0.0610, grad_fn=<MseLossBackward>)\n",
            "epoch: 30 loss: tensor(0.0606, grad_fn=<MseLossBackward>)\n",
            "epoch: 31 loss: tensor(0.0602, grad_fn=<MseLossBackward>)\n",
            "epoch: 32 loss: tensor(0.0598, grad_fn=<MseLossBackward>)\n",
            "epoch: 33 loss: tensor(0.0594, grad_fn=<MseLossBackward>)\n",
            "epoch: 34 loss: tensor(0.0590, grad_fn=<MseLossBackward>)\n",
            "epoch: 35 loss: tensor(0.0586, grad_fn=<MseLossBackward>)\n",
            "epoch: 36 loss: tensor(0.0582, grad_fn=<MseLossBackward>)\n",
            "epoch: 37 loss: tensor(0.0579, grad_fn=<MseLossBackward>)\n",
            "epoch: 38 loss: tensor(0.0575, grad_fn=<MseLossBackward>)\n",
            "epoch: 39 loss: tensor(0.0571, grad_fn=<MseLossBackward>)\n",
            "epoch: 40 loss: tensor(0.0567, grad_fn=<MseLossBackward>)\n",
            "epoch: 41 loss: tensor(0.0564, grad_fn=<MseLossBackward>)\n",
            "epoch: 42 loss: tensor(0.0560, grad_fn=<MseLossBackward>)\n",
            "epoch: 43 loss: tensor(0.0556, grad_fn=<MseLossBackward>)\n",
            "epoch: 44 loss: tensor(0.0553, grad_fn=<MseLossBackward>)\n",
            "epoch: 45 loss: tensor(0.0549, grad_fn=<MseLossBackward>)\n",
            "epoch: 46 loss: tensor(0.0546, grad_fn=<MseLossBackward>)\n",
            "epoch: 47 loss: tensor(0.0542, grad_fn=<MseLossBackward>)\n",
            "epoch: 48 loss: tensor(0.0539, grad_fn=<MseLossBackward>)\n",
            "epoch: 49 loss: tensor(0.0535, grad_fn=<MseLossBackward>)\n",
            "epoch: 50 loss: tensor(0.0532, grad_fn=<MseLossBackward>)\n",
            "epoch: 51 loss: tensor(0.0529, grad_fn=<MseLossBackward>)\n",
            "epoch: 52 loss: tensor(0.0525, grad_fn=<MseLossBackward>)\n",
            "epoch: 53 loss: tensor(0.0522, grad_fn=<MseLossBackward>)\n",
            "epoch: 54 loss: tensor(0.0519, grad_fn=<MseLossBackward>)\n",
            "epoch: 55 loss: tensor(0.0515, grad_fn=<MseLossBackward>)\n",
            "epoch: 56 loss: tensor(0.0512, grad_fn=<MseLossBackward>)\n",
            "epoch: 57 loss: tensor(0.0509, grad_fn=<MseLossBackward>)\n",
            "epoch: 58 loss: tensor(0.0506, grad_fn=<MseLossBackward>)\n",
            "epoch: 59 loss: tensor(0.0503, grad_fn=<MseLossBackward>)\n",
            "epoch: 60 loss: tensor(0.0499, grad_fn=<MseLossBackward>)\n",
            "epoch: 61 loss: tensor(0.0496, grad_fn=<MseLossBackward>)\n",
            "epoch: 62 loss: tensor(0.0493, grad_fn=<MseLossBackward>)\n",
            "epoch: 63 loss: tensor(0.0490, grad_fn=<MseLossBackward>)\n",
            "epoch: 64 loss: tensor(0.0487, grad_fn=<MseLossBackward>)\n",
            "epoch: 65 loss: tensor(0.0484, grad_fn=<MseLossBackward>)\n",
            "epoch: 66 loss: tensor(0.0481, grad_fn=<MseLossBackward>)\n",
            "epoch: 67 loss: tensor(0.0478, grad_fn=<MseLossBackward>)\n",
            "epoch: 68 loss: tensor(0.0475, grad_fn=<MseLossBackward>)\n",
            "epoch: 69 loss: tensor(0.0472, grad_fn=<MseLossBackward>)\n",
            "epoch: 70 loss: tensor(0.0469, grad_fn=<MseLossBackward>)\n",
            "epoch: 71 loss: tensor(0.0466, grad_fn=<MseLossBackward>)\n",
            "epoch: 72 loss: tensor(0.0463, grad_fn=<MseLossBackward>)\n",
            "epoch: 73 loss: tensor(0.0461, grad_fn=<MseLossBackward>)\n",
            "epoch: 74 loss: tensor(0.0458, grad_fn=<MseLossBackward>)\n",
            "epoch: 75 loss: tensor(0.0455, grad_fn=<MseLossBackward>)\n",
            "epoch: 76 loss: tensor(0.0452, grad_fn=<MseLossBackward>)\n",
            "epoch: 77 loss: tensor(0.0449, grad_fn=<MseLossBackward>)\n",
            "epoch: 78 loss: tensor(0.0447, grad_fn=<MseLossBackward>)\n",
            "epoch: 79 loss: tensor(0.0444, grad_fn=<MseLossBackward>)\n",
            "epoch: 80 loss: tensor(0.0441, grad_fn=<MseLossBackward>)\n",
            "epoch: 81 loss: tensor(0.0439, grad_fn=<MseLossBackward>)\n",
            "epoch: 82 loss: tensor(0.0436, grad_fn=<MseLossBackward>)\n",
            "epoch: 83 loss: tensor(0.0433, grad_fn=<MseLossBackward>)\n",
            "epoch: 84 loss: tensor(0.0431, grad_fn=<MseLossBackward>)\n",
            "epoch: 85 loss: tensor(0.0428, grad_fn=<MseLossBackward>)\n",
            "epoch: 86 loss: tensor(0.0425, grad_fn=<MseLossBackward>)\n",
            "epoch: 87 loss: tensor(0.0423, grad_fn=<MseLossBackward>)\n",
            "epoch: 88 loss: tensor(0.0420, grad_fn=<MseLossBackward>)\n",
            "epoch: 89 loss: tensor(0.0418, grad_fn=<MseLossBackward>)\n",
            "epoch: 90 loss: tensor(0.0415, grad_fn=<MseLossBackward>)\n",
            "epoch: 91 loss: tensor(0.0413, grad_fn=<MseLossBackward>)\n",
            "epoch: 92 loss: tensor(0.0410, grad_fn=<MseLossBackward>)\n",
            "epoch: 93 loss: tensor(0.0408, grad_fn=<MseLossBackward>)\n",
            "epoch: 94 loss: tensor(0.0405, grad_fn=<MseLossBackward>)\n",
            "epoch: 95 loss: tensor(0.0403, grad_fn=<MseLossBackward>)\n",
            "epoch: 96 loss: tensor(0.0401, grad_fn=<MseLossBackward>)\n",
            "epoch: 97 loss: tensor(0.0398, grad_fn=<MseLossBackward>)\n",
            "epoch: 98 loss: tensor(0.0396, grad_fn=<MseLossBackward>)\n",
            "epoch: 99 loss: tensor(0.0394, grad_fn=<MseLossBackward>)\n",
            "epoch: 100 loss: tensor(0.0391, grad_fn=<MseLossBackward>)\n",
            "[Net(\n",
            "  (lin1): Linear(in_features=2, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (lin2): Linear(in_features=10, out_features=20, bias=True)\n",
            "), Net(\n",
            "  (lin1): Linear(in_features=2, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (lin2): Linear(in_features=10, out_features=20, bias=True)\n",
            ")]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAcfjP09EYmS",
        "outputId": "a7e9b71b-a2ce-45df-df3d-717d278424ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(2):\n",
        "  for p in save[i].parameters():\n",
        "    print(p)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2914, -0.1130],\n",
            "        [-0.2879,  0.4710],\n",
            "        [-0.5957,  0.1354],\n",
            "        [ 0.0294, -0.1331],\n",
            "        [ 0.5959, -0.1033],\n",
            "        [ 0.5983,  0.2800],\n",
            "        [-0.4025, -0.0251],\n",
            "        [ 0.0883, -0.3096],\n",
            "        [ 0.0556,  0.1948],\n",
            "        [-0.0578, -0.5518]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.5731,  0.1377,  0.2841, -0.5811, -0.5374, -0.3452,  0.3281, -0.2634,\n",
            "        -0.1337,  0.1244], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.1177, -0.1863, -0.2026, -0.2303, -0.2994, -0.1909,  0.2157, -0.0466,\n",
            "          0.1999,  0.1740],\n",
            "        [-0.1457, -0.1335, -0.0319,  0.1815, -0.2844,  0.2966,  0.3079,  0.0892,\n",
            "         -0.1505,  0.0631],\n",
            "        [ 0.1753, -0.1600,  0.1418, -0.1915, -0.1387,  0.3213, -0.2342, -0.2125,\n",
            "         -0.0708, -0.1614],\n",
            "        [ 0.3205,  0.0806, -0.1084, -0.0817, -0.1881, -0.2159, -0.2609,  0.1877,\n",
            "         -0.1884, -0.1653],\n",
            "        [-0.1631,  0.1972,  0.0872, -0.0069,  0.0598, -0.0216,  0.1231, -0.2115,\n",
            "          0.1490, -0.1744],\n",
            "        [-0.1063,  0.0027, -0.0760,  0.1325, -0.2930,  0.2271,  0.2411, -0.1264,\n",
            "         -0.1070,  0.1921],\n",
            "        [-0.0628, -0.1233,  0.3048,  0.0280,  0.0311, -0.2782,  0.3261,  0.0744,\n",
            "         -0.0126,  0.0517],\n",
            "        [ 0.2919, -0.0464, -0.2841,  0.2686, -0.2130, -0.2775,  0.3110,  0.1536,\n",
            "          0.2781,  0.1706],\n",
            "        [ 0.2599, -0.0241, -0.1152,  0.1074, -0.1921,  0.0796,  0.2216,  0.3034,\n",
            "         -0.1337,  0.0317],\n",
            "        [ 0.1188,  0.1076, -0.2334,  0.2680, -0.1571, -0.3012,  0.1832,  0.0606,\n",
            "          0.2978,  0.0454],\n",
            "        [-0.1629,  0.3017,  0.0600, -0.0761,  0.0894,  0.1377, -0.3035,  0.2509,\n",
            "         -0.2659,  0.1818],\n",
            "        [ 0.2700,  0.2386, -0.2516, -0.2878, -0.1549, -0.2401, -0.2773,  0.1438,\n",
            "         -0.1950, -0.2564],\n",
            "        [-0.0202, -0.1729,  0.1318, -0.1937, -0.0049,  0.2841,  0.0163,  0.0100,\n",
            "          0.1324, -0.0083],\n",
            "        [-0.0954, -0.1822, -0.2662,  0.1485, -0.0413, -0.2367,  0.1568, -0.2705,\n",
            "         -0.2617, -0.3038],\n",
            "        [ 0.0904,  0.0484, -0.1226,  0.0306, -0.1154,  0.0633,  0.1832, -0.2849,\n",
            "          0.1639, -0.0615],\n",
            "        [ 0.1496, -0.2747, -0.1507, -0.1072,  0.0054,  0.2997, -0.2578, -0.0128,\n",
            "         -0.1918, -0.0130],\n",
            "        [ 0.2202,  0.0123, -0.1710, -0.0400, -0.0368, -0.0846,  0.2270,  0.1701,\n",
            "         -0.0079, -0.0138],\n",
            "        [ 0.2197, -0.2117,  0.1341, -0.2326,  0.0949, -0.2249,  0.1365, -0.1866,\n",
            "          0.0220, -0.2847],\n",
            "        [ 0.1216, -0.0225, -0.0477, -0.2492,  0.1938,  0.3124,  0.2775,  0.2439,\n",
            "          0.0664,  0.0276],\n",
            "        [ 0.0660,  0.2507,  0.3018, -0.1225,  0.0118,  0.2916, -0.2483, -0.1154,\n",
            "          0.0340, -0.0715]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-1.2339e-01, -2.8201e-02,  1.3924e-01,  1.1818e-01,  3.0265e-01,\n",
            "         2.9576e-01, -1.7740e-01,  2.2078e-01, -1.4245e-01,  7.5325e-02,\n",
            "         1.6547e-02,  2.0793e-02, -5.0215e-02,  2.0393e-01,  1.5358e-01,\n",
            "         3.5480e-01,  3.4970e-04,  1.9179e-01,  3.5949e-01, -1.9958e-01],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.2483, -0.1634],\n",
            "        [-0.2707,  0.4957],\n",
            "        [-0.5677,  0.1790],\n",
            "        [ 0.0294, -0.1331],\n",
            "        [ 0.6032, -0.1003],\n",
            "        [ 0.5868,  0.2703],\n",
            "        [-0.4323, -0.0735],\n",
            "        [ 0.0883, -0.3096],\n",
            "        [ 0.0599,  0.2035],\n",
            "        [-0.0535, -0.5512]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.4713,  0.1859,  0.3655, -0.5811, -0.5300, -0.3608,  0.2330, -0.2634,\n",
            "        -0.1216,  0.1329], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0656, -0.2125, -0.2198, -0.2303, -0.2995, -0.1944,  0.2047, -0.0466,\n",
            "          0.1986,  0.1734],\n",
            "        [-0.1996, -0.1607, -0.0493,  0.1815, -0.2846,  0.2921,  0.2969,  0.0892,\n",
            "         -0.1520,  0.0624],\n",
            "        [ 0.1213, -0.1879,  0.1248, -0.1915, -0.1388,  0.3135, -0.2435, -0.2125,\n",
            "         -0.0729, -0.1622],\n",
            "        [ 0.2689,  0.0539, -0.1250, -0.0817, -0.1882, -0.2232, -0.2699,  0.1877,\n",
            "         -0.1903, -0.1661],\n",
            "        [-0.2151,  0.1704,  0.0705, -0.0069,  0.0597, -0.0293,  0.1145, -0.2115,\n",
            "          0.1469, -0.1753],\n",
            "        [-0.1592, -0.0245, -0.0926,  0.1325, -0.2931,  0.2192,  0.2323, -0.1264,\n",
            "         -0.1090,  0.1913],\n",
            "        [-0.1137, -0.1488,  0.2871,  0.0280,  0.0309, -0.2802,  0.3149,  0.0744,\n",
            "         -0.0136,  0.0510],\n",
            "        [ 0.2418, -0.0723, -0.3001,  0.2686, -0.2131, -0.2852,  0.3028,  0.1536,\n",
            "          0.2761,  0.1698],\n",
            "        [ 0.2075, -0.0507, -0.1321,  0.1074, -0.1922,  0.0744,  0.2113,  0.3034,\n",
            "         -0.1353,  0.0310],\n",
            "        [ 0.0677,  0.0815, -0.2500,  0.2680, -0.1572, -0.3071,  0.1738,  0.0606,\n",
            "          0.2961,  0.0447],\n",
            "        [-0.2175,  0.2740,  0.0424, -0.0761,  0.0893,  0.1322, -0.3140,  0.2509,\n",
            "         -0.2676,  0.1810],\n",
            "        [ 0.2179,  0.2120, -0.2683, -0.2878, -0.1550, -0.2466, -0.2868,  0.1438,\n",
            "         -0.1968, -0.2572],\n",
            "        [-0.0746, -0.2004,  0.1143, -0.1937, -0.0050,  0.2790,  0.0054,  0.0100,\n",
            "          0.1308, -0.0090],\n",
            "        [-0.1468, -0.2083, -0.2831,  0.1485, -0.0414, -0.2417,  0.1468, -0.2705,\n",
            "         -0.2632, -0.3045],\n",
            "        [ 0.0380,  0.0215, -0.1391,  0.0306, -0.1155,  0.0560,  0.1742, -0.2849,\n",
            "          0.1619, -0.0623],\n",
            "        [ 0.0956, -0.3027, -0.1672, -0.1072,  0.0054,  0.2905, -0.2665, -0.0128,\n",
            "         -0.1941, -0.0138],\n",
            "        [ 0.1687, -0.0140, -0.1876, -0.0400, -0.0369, -0.0906,  0.2175,  0.1701,\n",
            "         -0.0097, -0.0146],\n",
            "        [ 0.1695, -0.2377,  0.1176, -0.2326,  0.0948, -0.2316,  0.1279, -0.1866,\n",
            "          0.0202, -0.2855],\n",
            "        [ 0.0693, -0.0498, -0.0636, -0.2492,  0.1937,  0.3025,  0.2699,  0.2439,\n",
            "          0.0640,  0.0267],\n",
            "        [ 0.0112,  0.2229,  0.2842, -0.1225,  0.0117,  0.2863, -0.2591, -0.1154,\n",
            "          0.0323, -0.0722]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2144, -0.1225,  0.0431,  0.0262,  0.2097,  0.2016, -0.2664,  0.1312,\n",
            "        -0.2346, -0.0152, -0.0795, -0.0715, -0.1457,  0.1132,  0.0606,  0.2583,\n",
            "        -0.0908,  0.1019,  0.2654, -0.2959], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wFrhMixVx5B",
        "outputId": "133ca748-2cce-4722-a87e-1ae97d625a6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.head = torch.nn.ModuleList()\n",
        "    for _ in range(2):\n",
        "      self.head.append(\n",
        "        torch.nn.Sequential(\n",
        "          nn.Linear(2, 10),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(10, 20)\n",
        "          ))\n",
        "  def forward(self, x, i):\n",
        "    return self.head[i].forward(x)\n",
        "\n",
        "xx = np.random.random((40, 2))\n",
        "x = [[]for i in range(2)]\n",
        "y = [[]for i in range(2)]\n",
        "\n",
        "for j in range(40):\n",
        "  if j % 2 == 0:\n",
        "    x[0].append(xx[j])\n",
        "    y[0].append(1)\n",
        "  else:\n",
        "    x[1].append(xx[j])\n",
        "    y[1].append(0)\n",
        "\n",
        "x[0] = torch.from_numpy(np.array(x[0]).astype(np.float32))\n",
        "x[1] = torch.from_numpy(np.array(x[1]).astype(np.float32))\n",
        "y[0] = torch.from_numpy(np.array(y[0]).astype(np.float32))\n",
        "y[1] = torch.from_numpy(np.array(y[1]).astype(np.float32))\n",
        "model = Net()\n",
        "print(x[0].shape, x[1].shape, y[0].shape, y[1].shape, 'shape')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "criterion = nn.MSELoss()\n",
        "for i in range(2):\n",
        "  for e in range(100):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(x[i], i)\n",
        "    loss = criterion(out, y[i])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('epoch:', e + 1, 'loss:', loss)\n",
        "\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 2]) torch.Size([20, 2]) torch.Size([20]) torch.Size([20]) shape\n",
            "epoch: 1 loss: tensor(1.1015, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(1.0960, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(1.0906, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(1.0852, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(1.0799, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(1.0746, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(1.0693, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(1.0641, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(1.0589, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(1.0538, grad_fn=<MseLossBackward>)\n",
            "epoch: 11 loss: tensor(1.0487, grad_fn=<MseLossBackward>)\n",
            "epoch: 12 loss: tensor(1.0437, grad_fn=<MseLossBackward>)\n",
            "epoch: 13 loss: tensor(1.0387, grad_fn=<MseLossBackward>)\n",
            "epoch: 14 loss: tensor(1.0337, grad_fn=<MseLossBackward>)\n",
            "epoch: 15 loss: tensor(1.0288, grad_fn=<MseLossBackward>)\n",
            "epoch: 16 loss: tensor(1.0239, grad_fn=<MseLossBackward>)\n",
            "epoch: 17 loss: tensor(1.0191, grad_fn=<MseLossBackward>)\n",
            "epoch: 18 loss: tensor(1.0143, grad_fn=<MseLossBackward>)\n",
            "epoch: 19 loss: tensor(1.0095, grad_fn=<MseLossBackward>)\n",
            "epoch: 20 loss: tensor(1.0048, grad_fn=<MseLossBackward>)\n",
            "epoch: 21 loss: tensor(1.0001, grad_fn=<MseLossBackward>)\n",
            "epoch: 22 loss: tensor(0.9955, grad_fn=<MseLossBackward>)\n",
            "epoch: 23 loss: tensor(0.9909, grad_fn=<MseLossBackward>)\n",
            "epoch: 24 loss: tensor(0.9863, grad_fn=<MseLossBackward>)\n",
            "epoch: 25 loss: tensor(0.9818, grad_fn=<MseLossBackward>)\n",
            "epoch: 26 loss: tensor(0.9773, grad_fn=<MseLossBackward>)\n",
            "epoch: 27 loss: tensor(0.9728, grad_fn=<MseLossBackward>)\n",
            "epoch: 28 loss: tensor(0.9684, grad_fn=<MseLossBackward>)\n",
            "epoch: 29 loss: tensor(0.9640, grad_fn=<MseLossBackward>)\n",
            "epoch: 30 loss: tensor(0.9597, grad_fn=<MseLossBackward>)\n",
            "epoch: 31 loss: tensor(0.9553, grad_fn=<MseLossBackward>)\n",
            "epoch: 32 loss: tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "epoch: 33 loss: tensor(0.9467, grad_fn=<MseLossBackward>)\n",
            "epoch: 34 loss: tensor(0.9425, grad_fn=<MseLossBackward>)\n",
            "epoch: 35 loss: tensor(0.9383, grad_fn=<MseLossBackward>)\n",
            "epoch: 36 loss: tensor(0.9341, grad_fn=<MseLossBackward>)\n",
            "epoch: 37 loss: tensor(0.9299, grad_fn=<MseLossBackward>)\n",
            "epoch: 38 loss: tensor(0.9258, grad_fn=<MseLossBackward>)\n",
            "epoch: 39 loss: tensor(0.9217, grad_fn=<MseLossBackward>)\n",
            "epoch: 40 loss: tensor(0.9176, grad_fn=<MseLossBackward>)\n",
            "epoch: 41 loss: tensor(0.9135, grad_fn=<MseLossBackward>)\n",
            "epoch: 42 loss: tensor(0.9095, grad_fn=<MseLossBackward>)\n",
            "epoch: 43 loss: tensor(0.9054, grad_fn=<MseLossBackward>)\n",
            "epoch: 44 loss: tensor(0.9014, grad_fn=<MseLossBackward>)\n",
            "epoch: 45 loss: tensor(0.8975, grad_fn=<MseLossBackward>)\n",
            "epoch: 46 loss: tensor(0.8935, grad_fn=<MseLossBackward>)\n",
            "epoch: 47 loss: tensor(0.8896, grad_fn=<MseLossBackward>)\n",
            "epoch: 48 loss: tensor(0.8857, grad_fn=<MseLossBackward>)\n",
            "epoch: 49 loss: tensor(0.8818, grad_fn=<MseLossBackward>)\n",
            "epoch: 50 loss: tensor(0.8780, grad_fn=<MseLossBackward>)\n",
            "epoch: 51 loss: tensor(0.8742, grad_fn=<MseLossBackward>)\n",
            "epoch: 52 loss: tensor(0.8704, grad_fn=<MseLossBackward>)\n",
            "epoch: 53 loss: tensor(0.8666, grad_fn=<MseLossBackward>)\n",
            "epoch: 54 loss: tensor(0.8629, grad_fn=<MseLossBackward>)\n",
            "epoch: 55 loss: tensor(0.8591, grad_fn=<MseLossBackward>)\n",
            "epoch: 56 loss: tensor(0.8554, grad_fn=<MseLossBackward>)\n",
            "epoch: 57 loss: tensor(0.8517, grad_fn=<MseLossBackward>)\n",
            "epoch: 58 loss: tensor(0.8480, grad_fn=<MseLossBackward>)\n",
            "epoch: 59 loss: tensor(0.8443, grad_fn=<MseLossBackward>)\n",
            "epoch: 60 loss: tensor(0.8407, grad_fn=<MseLossBackward>)\n",
            "epoch: 61 loss: tensor(0.8371, grad_fn=<MseLossBackward>)\n",
            "epoch: 62 loss: tensor(0.8334, grad_fn=<MseLossBackward>)\n",
            "epoch: 63 loss: tensor(0.8298, grad_fn=<MseLossBackward>)\n",
            "epoch: 64 loss: tensor(0.8262, grad_fn=<MseLossBackward>)\n",
            "epoch: 65 loss: tensor(0.8227, grad_fn=<MseLossBackward>)\n",
            "epoch: 66 loss: tensor(0.8191, grad_fn=<MseLossBackward>)\n",
            "epoch: 67 loss: tensor(0.8156, grad_fn=<MseLossBackward>)\n",
            "epoch: 68 loss: tensor(0.8121, grad_fn=<MseLossBackward>)\n",
            "epoch: 69 loss: tensor(0.8086, grad_fn=<MseLossBackward>)\n",
            "epoch: 70 loss: tensor(0.8051, grad_fn=<MseLossBackward>)\n",
            "epoch: 71 loss: tensor(0.8016, grad_fn=<MseLossBackward>)\n",
            "epoch: 72 loss: tensor(0.7981, grad_fn=<MseLossBackward>)\n",
            "epoch: 73 loss: tensor(0.7947, grad_fn=<MseLossBackward>)\n",
            "epoch: 74 loss: tensor(0.7913, grad_fn=<MseLossBackward>)\n",
            "epoch: 75 loss: tensor(0.7878, grad_fn=<MseLossBackward>)\n",
            "epoch: 76 loss: tensor(0.7844, grad_fn=<MseLossBackward>)\n",
            "epoch: 77 loss: tensor(0.7810, grad_fn=<MseLossBackward>)\n",
            "epoch: 78 loss: tensor(0.7776, grad_fn=<MseLossBackward>)\n",
            "epoch: 79 loss: tensor(0.7743, grad_fn=<MseLossBackward>)\n",
            "epoch: 80 loss: tensor(0.7709, grad_fn=<MseLossBackward>)\n",
            "epoch: 81 loss: tensor(0.7676, grad_fn=<MseLossBackward>)\n",
            "epoch: 82 loss: tensor(0.7642, grad_fn=<MseLossBackward>)\n",
            "epoch: 83 loss: tensor(0.7609, grad_fn=<MseLossBackward>)\n",
            "epoch: 84 loss: tensor(0.7576, grad_fn=<MseLossBackward>)\n",
            "epoch: 85 loss: tensor(0.7543, grad_fn=<MseLossBackward>)\n",
            "epoch: 86 loss: tensor(0.7510, grad_fn=<MseLossBackward>)\n",
            "epoch: 87 loss: tensor(0.7477, grad_fn=<MseLossBackward>)\n",
            "epoch: 88 loss: tensor(0.7445, grad_fn=<MseLossBackward>)\n",
            "epoch: 89 loss: tensor(0.7412, grad_fn=<MseLossBackward>)\n",
            "epoch: 90 loss: tensor(0.7380, grad_fn=<MseLossBackward>)\n",
            "epoch: 91 loss: tensor(0.7348, grad_fn=<MseLossBackward>)\n",
            "epoch: 92 loss: tensor(0.7316, grad_fn=<MseLossBackward>)\n",
            "epoch: 93 loss: tensor(0.7284, grad_fn=<MseLossBackward>)\n",
            "epoch: 94 loss: tensor(0.7252, grad_fn=<MseLossBackward>)\n",
            "epoch: 95 loss: tensor(0.7220, grad_fn=<MseLossBackward>)\n",
            "epoch: 96 loss: tensor(0.7188, grad_fn=<MseLossBackward>)\n",
            "epoch: 97 loss: tensor(0.7156, grad_fn=<MseLossBackward>)\n",
            "epoch: 98 loss: tensor(0.7125, grad_fn=<MseLossBackward>)\n",
            "epoch: 99 loss: tensor(0.7093, grad_fn=<MseLossBackward>)\n",
            "epoch: 100 loss: tensor(0.7062, grad_fn=<MseLossBackward>)\n",
            "epoch: 1 loss: tensor(0.0632, grad_fn=<MseLossBackward>)\n",
            "epoch: 2 loss: tensor(0.0629, grad_fn=<MseLossBackward>)\n",
            "epoch: 3 loss: tensor(0.0626, grad_fn=<MseLossBackward>)\n",
            "epoch: 4 loss: tensor(0.0623, grad_fn=<MseLossBackward>)\n",
            "epoch: 5 loss: tensor(0.0620, grad_fn=<MseLossBackward>)\n",
            "epoch: 6 loss: tensor(0.0617, grad_fn=<MseLossBackward>)\n",
            "epoch: 7 loss: tensor(0.0614, grad_fn=<MseLossBackward>)\n",
            "epoch: 8 loss: tensor(0.0611, grad_fn=<MseLossBackward>)\n",
            "epoch: 9 loss: tensor(0.0608, grad_fn=<MseLossBackward>)\n",
            "epoch: 10 loss: tensor(0.0605, grad_fn=<MseLossBackward>)\n",
            "epoch: 11 loss: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0602, grad_fn=<MseLossBackward>)\n",
            "epoch: 12 loss: tensor(0.0599, grad_fn=<MseLossBackward>)\n",
            "epoch: 13 loss: tensor(0.0596, grad_fn=<MseLossBackward>)\n",
            "epoch: 14 loss: tensor(0.0594, grad_fn=<MseLossBackward>)\n",
            "epoch: 15 loss: tensor(0.0591, grad_fn=<MseLossBackward>)\n",
            "epoch: 16 loss: tensor(0.0588, grad_fn=<MseLossBackward>)\n",
            "epoch: 17 loss: tensor(0.0585, grad_fn=<MseLossBackward>)\n",
            "epoch: 18 loss: tensor(0.0582, grad_fn=<MseLossBackward>)\n",
            "epoch: 19 loss: tensor(0.0579, grad_fn=<MseLossBackward>)\n",
            "epoch: 20 loss: tensor(0.0577, grad_fn=<MseLossBackward>)\n",
            "epoch: 21 loss: tensor(0.0574, grad_fn=<MseLossBackward>)\n",
            "epoch: 22 loss: tensor(0.0571, grad_fn=<MseLossBackward>)\n",
            "epoch: 23 loss: tensor(0.0569, grad_fn=<MseLossBackward>)\n",
            "epoch: 24 loss: tensor(0.0566, grad_fn=<MseLossBackward>)\n",
            "epoch: 25 loss: tensor(0.0563, grad_fn=<MseLossBackward>)\n",
            "epoch: 26 loss: tensor(0.0561, grad_fn=<MseLossBackward>)\n",
            "epoch: 27 loss: tensor(0.0558, grad_fn=<MseLossBackward>)\n",
            "epoch: 28 loss: tensor(0.0555, grad_fn=<MseLossBackward>)\n",
            "epoch: 29 loss: tensor(0.0553, grad_fn=<MseLossBackward>)\n",
            "epoch: 30 loss: tensor(0.0550, grad_fn=<MseLossBackward>)\n",
            "epoch: 31 loss: tensor(0.0548, grad_fn=<MseLossBackward>)\n",
            "epoch: 32 loss: tensor(0.0545, grad_fn=<MseLossBackward>)\n",
            "epoch: 33 loss: tensor(0.0542, grad_fn=<MseLossBackward>)\n",
            "epoch: 34 loss: tensor(0.0540, grad_fn=<MseLossBackward>)\n",
            "epoch: 35 loss: tensor(0.0537, grad_fn=<MseLossBackward>)\n",
            "epoch: 36 loss: tensor(0.0535, grad_fn=<MseLossBackward>)\n",
            "epoch: 37 loss: tensor(0.0532, grad_fn=<MseLossBackward>)\n",
            "epoch: 38 loss: tensor(0.0530, grad_fn=<MseLossBackward>)\n",
            "epoch: 39 loss: tensor(0.0528, grad_fn=<MseLossBackward>)\n",
            "epoch: 40 loss: tensor(0.0525, grad_fn=<MseLossBackward>)\n",
            "epoch: 41 loss: tensor(0.0523, grad_fn=<MseLossBackward>)\n",
            "epoch: 42 loss: tensor(0.0520, grad_fn=<MseLossBackward>)\n",
            "epoch: 43 loss: tensor(0.0518, grad_fn=<MseLossBackward>)\n",
            "epoch: 44 loss: tensor(0.0516, grad_fn=<MseLossBackward>)\n",
            "epoch: 45 loss: tensor(0.0513, grad_fn=<MseLossBackward>)\n",
            "epoch: 46 loss: tensor(0.0511, grad_fn=<MseLossBackward>)\n",
            "epoch: 47 loss: tensor(0.0509, grad_fn=<MseLossBackward>)\n",
            "epoch: 48 loss: tensor(0.0506, grad_fn=<MseLossBackward>)\n",
            "epoch: 49 loss: tensor(0.0504, grad_fn=<MseLossBackward>)\n",
            "epoch: 50 loss: tensor(0.0502, grad_fn=<MseLossBackward>)\n",
            "epoch: 51 loss: tensor(0.0499, grad_fn=<MseLossBackward>)\n",
            "epoch: 52 loss: tensor(0.0497, grad_fn=<MseLossBackward>)\n",
            "epoch: 53 loss: tensor(0.0495, grad_fn=<MseLossBackward>)\n",
            "epoch: 54 loss: tensor(0.0493, grad_fn=<MseLossBackward>)\n",
            "epoch: 55 loss: tensor(0.0490, grad_fn=<MseLossBackward>)\n",
            "epoch: 56 loss: tensor(0.0488, grad_fn=<MseLossBackward>)\n",
            "epoch: 57 loss: tensor(0.0486, grad_fn=<MseLossBackward>)\n",
            "epoch: 58 loss: tensor(0.0484, grad_fn=<MseLossBackward>)\n",
            "epoch: 59 loss: tensor(0.0482, grad_fn=<MseLossBackward>)\n",
            "epoch: 60 loss: tensor(0.0479, grad_fn=<MseLossBackward>)\n",
            "epoch: 61 loss: tensor(0.0477, grad_fn=<MseLossBackward>)\n",
            "epoch: 62 loss: tensor(0.0475, grad_fn=<MseLossBackward>)\n",
            "epoch: 63 loss: tensor(0.0473, grad_fn=<MseLossBackward>)\n",
            "epoch: 64 loss: tensor(0.0471, grad_fn=<MseLossBackward>)\n",
            "epoch: 65 loss: tensor(0.0469, grad_fn=<MseLossBackward>)\n",
            "epoch: 66 loss: tensor(0.0467, grad_fn=<MseLossBackward>)\n",
            "epoch: 67 loss: tensor(0.0465, grad_fn=<MseLossBackward>)\n",
            "epoch: 68 loss: tensor(0.0463, grad_fn=<MseLossBackward>)\n",
            "epoch: 69 loss: tensor(0.0461, grad_fn=<MseLossBackward>)\n",
            "epoch: 70 loss: tensor(0.0459, grad_fn=<MseLossBackward>)\n",
            "epoch: 71 loss: tensor(0.0457, grad_fn=<MseLossBackward>)\n",
            "epoch: 72 loss: tensor(0.0455, grad_fn=<MseLossBackward>)\n",
            "epoch: 73 loss: tensor(0.0453, grad_fn=<MseLossBackward>)\n",
            "epoch: 74 loss: tensor(0.0451, grad_fn=<MseLossBackward>)\n",
            "epoch: 75 loss: tensor(0.0449, grad_fn=<MseLossBackward>)\n",
            "epoch: 76 loss: tensor(0.0447, grad_fn=<MseLossBackward>)\n",
            "epoch: 77 loss: tensor(0.0445, grad_fn=<MseLossBackward>)\n",
            "epoch: 78 loss: tensor(0.0443, grad_fn=<MseLossBackward>)\n",
            "epoch: 79 loss: tensor(0.0441, grad_fn=<MseLossBackward>)\n",
            "epoch: 80 loss: tensor(0.0439, grad_fn=<MseLossBackward>)\n",
            "epoch: 81 loss: tensor(0.0437, grad_fn=<MseLossBackward>)\n",
            "epoch: 82 loss: tensor(0.0435, grad_fn=<MseLossBackward>)\n",
            "epoch: 83 loss: tensor(0.0433, grad_fn=<MseLossBackward>)\n",
            "epoch: 84 loss: tensor(0.0431, grad_fn=<MseLossBackward>)\n",
            "epoch: 85 loss: tensor(0.0429, grad_fn=<MseLossBackward>)\n",
            "epoch: 86 loss: tensor(0.0428, grad_fn=<MseLossBackward>)\n",
            "epoch: 87 loss: tensor(0.0426, grad_fn=<MseLossBackward>)\n",
            "epoch: 88 loss: tensor(0.0424, grad_fn=<MseLossBackward>)\n",
            "epoch: 89 loss: tensor(0.0422, grad_fn=<MseLossBackward>)\n",
            "epoch: 90 loss: tensor(0.0420, grad_fn=<MseLossBackward>)\n",
            "epoch: 91 loss: tensor(0.0418, grad_fn=<MseLossBackward>)\n",
            "epoch: 92 loss: tensor(0.0417, grad_fn=<MseLossBackward>)\n",
            "epoch: 93 loss: tensor(0.0415, grad_fn=<MseLossBackward>)\n",
            "epoch: 94 loss: tensor(0.0413, grad_fn=<MseLossBackward>)\n",
            "epoch: 95 loss: tensor(0.0411, grad_fn=<MseLossBackward>)\n",
            "epoch: 96 loss: tensor(0.0410, grad_fn=<MseLossBackward>)\n",
            "epoch: 97 loss: tensor(0.0408, grad_fn=<MseLossBackward>)\n",
            "epoch: 98 loss: tensor(0.0406, grad_fn=<MseLossBackward>)\n",
            "epoch: 99 loss: tensor(0.0404, grad_fn=<MseLossBackward>)\n",
            "epoch: 100 loss: tensor(0.0403, grad_fn=<MseLossBackward>)\n",
            "Net(\n",
            "  (head): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=2, out_features=10, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=10, out_features=20, bias=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=2, out_features=10, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=10, out_features=20, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC6esRmOlFl4",
        "outputId": "54727202-474e-40d3-db0b-ec64e6ebf394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(2):\n",
        "  for p in save[i].parameters():\n",
        "    print(p)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.6287,  0.5508],\n",
            "        [ 0.4756, -0.1229],\n",
            "        [-0.0266,  0.4165],\n",
            "        [ 0.7792,  0.0266],\n",
            "        [ 0.5805, -0.4286],\n",
            "        [ 0.5344, -0.6218],\n",
            "        [ 0.0021,  0.5801],\n",
            "        [-0.2372, -0.4829],\n",
            "        [ 0.4925,  0.0033],\n",
            "        [-0.6928,  0.6090]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.2154, -0.1954,  0.0571, -0.1987,  0.0665,  0.0586, -0.3737, -0.0659,\n",
            "         0.0250, -0.6591], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.3075,  0.1309, -0.1176,  0.1279, -0.1492,  0.1504,  0.0225,  0.0463,\n",
            "         -0.2329,  0.0400],\n",
            "        [-0.2673, -0.0536, -0.2192, -0.1399,  0.1230, -0.0732, -0.0473, -0.2437,\n",
            "         -0.0604,  0.0715],\n",
            "        [-0.1268, -0.0397,  0.2849,  0.2475,  0.2983,  0.3058, -0.1552, -0.2309,\n",
            "         -0.0852,  0.2704],\n",
            "        [-0.2189, -0.1722, -0.1644, -0.1820,  0.1493,  0.2105, -0.0689,  0.1194,\n",
            "          0.1019,  0.1048],\n",
            "        [-0.0004, -0.1172, -0.0338,  0.0727,  0.1329,  0.0718,  0.0513, -0.0026,\n",
            "          0.0589,  0.0657],\n",
            "        [ 0.1572,  0.2075, -0.0708,  0.1370, -0.1648, -0.1347, -0.1830,  0.0634,\n",
            "         -0.0105, -0.1226],\n",
            "        [ 0.2429,  0.0895,  0.3367,  0.2203, -0.2275, -0.1160,  0.2763,  0.2660,\n",
            "          0.2179,  0.1745],\n",
            "        [-0.0422,  0.2512, -0.2586,  0.2425, -0.2941, -0.1596, -0.1173, -0.3046,\n",
            "         -0.0979,  0.0622],\n",
            "        [ 0.2462, -0.2173,  0.0454,  0.2597, -0.2366, -0.1878, -0.2382, -0.1866,\n",
            "          0.2052,  0.2891],\n",
            "        [-0.1593,  0.1410,  0.0926,  0.1993,  0.1724,  0.1723, -0.2284, -0.1635,\n",
            "          0.2279,  0.2816],\n",
            "        [ 0.2499,  0.0746, -0.2122, -0.0039, -0.0644,  0.1180,  0.3224, -0.0938,\n",
            "          0.0700,  0.2040],\n",
            "        [ 0.1182, -0.1905, -0.2137, -0.0038,  0.1324,  0.0034,  0.3112,  0.2174,\n",
            "          0.2929,  0.0835],\n",
            "        [ 0.0378,  0.1978,  0.1041,  0.1238,  0.2762,  0.2155, -0.2372, -0.2786,\n",
            "          0.2063, -0.0841],\n",
            "        [ 0.0994, -0.0264, -0.2481,  0.0777,  0.0180,  0.0620, -0.0324,  0.0067,\n",
            "          0.0928, -0.2451],\n",
            "        [-0.1026,  0.1676, -0.1512,  0.1951,  0.1603, -0.1553, -0.1931, -0.2663,\n",
            "          0.2783,  0.0648],\n",
            "        [-0.1729,  0.0038, -0.2127,  0.0444,  0.0721, -0.1067, -0.2354,  0.1106,\n",
            "         -0.1297, -0.2968],\n",
            "        [ 0.2668, -0.0253, -0.2538,  0.2035, -0.0044,  0.2368,  0.2175, -0.2278,\n",
            "         -0.1167,  0.0946],\n",
            "        [-0.1265,  0.1398, -0.1625, -0.0933,  0.2514, -0.1752,  0.0279, -0.3160,\n",
            "          0.2894,  0.2102],\n",
            "        [ 0.1036, -0.2805, -0.0296, -0.1544, -0.0099, -0.2035,  0.2620,  0.0109,\n",
            "         -0.1443, -0.0652],\n",
            "        [ 0.0592,  0.2270, -0.0864,  0.3119, -0.0947, -0.0570, -0.0776, -0.2168,\n",
            "         -0.2274, -0.3049]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.3016,  0.1507, -0.1184,  0.1647,  0.1349,  0.1631,  0.0316,  0.3808,\n",
            "        -0.1001,  0.1274, -0.1349, -0.1302, -0.1533, -0.0674,  0.2025,  0.0803,\n",
            "         0.1595,  0.3186,  0.1840,  0.1165], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.6335,  0.2979],\n",
            "        [-0.6168, -0.6770],\n",
            "        [-0.0891,  0.6292],\n",
            "        [ 0.5317,  0.3202],\n",
            "        [-0.7020,  0.0424],\n",
            "        [ 0.1967,  0.6077],\n",
            "        [-0.6364, -0.6160],\n",
            "        [-0.2331,  0.5925],\n",
            "        [ 0.2791, -0.1950],\n",
            "        [ 0.4392,  0.2885]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.6819, -0.4796,  0.7008, -0.2068,  0.4836,  0.4171,  0.1872, -0.4454,\n",
            "        -0.3534, -0.2890], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0511, -0.1193, -0.1370,  0.0753, -0.2216,  0.1471,  0.1179, -0.0860,\n",
            "         -0.0572, -0.0238],\n",
            "        [-0.1198,  0.2167, -0.2019,  0.2414,  0.0293,  0.0426, -0.3121,  0.0236,\n",
            "          0.0154,  0.0806],\n",
            "        [ 0.0244, -0.2676, -0.1303,  0.0456,  0.2098, -0.1285,  0.2883,  0.0850,\n",
            "          0.0474, -0.1969],\n",
            "        [ 0.2432,  0.0178, -0.0360,  0.2112, -0.0940,  0.2646,  0.2242, -0.0964,\n",
            "          0.1670, -0.1683],\n",
            "        [-0.0628,  0.0720, -0.1156, -0.0789, -0.2382,  0.0199,  0.0715,  0.0259,\n",
            "          0.3043,  0.0582],\n",
            "        [-0.2560,  0.0546,  0.3008, -0.0481,  0.1639,  0.3131, -0.2112, -0.0416,\n",
            "          0.0063,  0.0979],\n",
            "        [-0.2283,  0.0777, -0.0196, -0.1571,  0.1628,  0.0255, -0.0519, -0.0150,\n",
            "          0.2079, -0.2403],\n",
            "        [-0.1664,  0.2594, -0.1148,  0.0066, -0.1886,  0.0093,  0.0277,  0.1613,\n",
            "          0.0402, -0.2350],\n",
            "        [ 0.1343,  0.1731,  0.2302,  0.0843, -0.2082, -0.3074,  0.0078,  0.1983,\n",
            "          0.1550,  0.2307],\n",
            "        [ 0.0887, -0.0200, -0.1370, -0.0557,  0.1758, -0.1506, -0.2897,  0.3062,\n",
            "         -0.0731,  0.1845],\n",
            "        [ 0.1890,  0.1655,  0.0743,  0.1262, -0.3026, -0.1113, -0.2025, -0.1387,\n",
            "          0.0354,  0.2029],\n",
            "        [-0.1636,  0.3110,  0.2291,  0.0522,  0.2108, -0.0529, -0.0954,  0.0327,\n",
            "         -0.0036,  0.1699],\n",
            "        [ 0.2955,  0.0415,  0.1343, -0.1054, -0.3138, -0.2199,  0.2745,  0.1222,\n",
            "         -0.0900,  0.1757],\n",
            "        [ 0.0454, -0.1917,  0.1064,  0.2088, -0.1126,  0.3107, -0.2619, -0.1055,\n",
            "         -0.1801,  0.2694],\n",
            "        [-0.1662, -0.1807,  0.0484, -0.0042, -0.0521,  0.2395, -0.2704,  0.1049,\n",
            "         -0.1509,  0.1016],\n",
            "        [-0.0342, -0.2373, -0.2708,  0.0516, -0.2230,  0.2884, -0.0937, -0.0360,\n",
            "         -0.2450,  0.0953],\n",
            "        [ 0.1090,  0.0103, -0.0099,  0.0377,  0.1898, -0.2077,  0.0278, -0.2295,\n",
            "         -0.0955, -0.3074],\n",
            "        [ 0.1539,  0.2208, -0.2107,  0.1699,  0.1080,  0.1240,  0.0161, -0.0019,\n",
            "         -0.2052, -0.0364],\n",
            "        [-0.1875,  0.2174,  0.2146, -0.1385, -0.1047,  0.0134,  0.0880,  0.1752,\n",
            "         -0.0049, -0.0037],\n",
            "        [-0.0652, -0.1673,  0.0573,  0.0051, -0.2875,  0.1560,  0.1155,  0.0270,\n",
            "         -0.0054,  0.0854]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0665, -0.0954, -0.2493,  0.1549,  0.1899,  0.1586,  0.2407, -0.2849,\n",
            "        -0.0622,  0.2312, -0.0111, -0.2260,  0.2532, -0.0786,  0.1797,  0.2559,\n",
            "         0.0661, -0.1439,  0.0354,  0.1031], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.6417,  0.5347],\n",
            "        [ 0.4688, -0.1268],\n",
            "        [ 0.0920,  0.5188],\n",
            "        [ 0.6987, -0.0336],\n",
            "        [ 0.5694, -0.4389],\n",
            "        [ 0.5360, -0.6219],\n",
            "        [ 0.0103,  0.5934],\n",
            "        [-0.2372, -0.4829],\n",
            "        [ 0.4602, -0.0264],\n",
            "        [-0.6928,  0.6090]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1885, -0.2046,  0.2559, -0.3180,  0.0485,  0.0599, -0.3583, -0.0659,\n",
            "        -0.0328, -0.6591], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.2959,  0.1257, -0.1487,  0.1123, -0.1655,  0.1387,  0.0186,  0.0463,\n",
            "         -0.2543,  0.0400],\n",
            "        [-0.2854, -0.0603, -0.2635, -0.1607,  0.1021, -0.0880, -0.0534, -0.2437,\n",
            "         -0.0893,  0.0715],\n",
            "        [-0.1439, -0.0450,  0.2454,  0.2302,  0.2808,  0.2935, -0.1606, -0.2309,\n",
            "         -0.1101,  0.2704],\n",
            "        [-0.2355, -0.1778, -0.2042, -0.2000,  0.1316,  0.1981, -0.0745,  0.1194,\n",
            "          0.0767,  0.1048],\n",
            "        [-0.0147, -0.1225, -0.0691,  0.0563,  0.1161,  0.0599,  0.0466, -0.0026,\n",
            "          0.0360,  0.0657],\n",
            "        [ 0.1432,  0.2015, -0.1074,  0.1189, -0.1839, -0.1486, -0.1877,  0.0634,\n",
            "         -0.0355, -0.1226],\n",
            "        [ 0.2320,  0.0841,  0.3073,  0.2048, -0.2450, -0.1291,  0.2730,  0.2660,\n",
            "          0.1966,  0.1745],\n",
            "        [-0.0553,  0.2460, -0.2920,  0.2265, -0.3109, -0.1717, -0.1217, -0.3046,\n",
            "         -0.1202,  0.0622],\n",
            "        [ 0.2299, -0.2247,  0.0021,  0.2379, -0.2601, -0.2050, -0.2435, -0.1866,\n",
            "          0.1752,  0.2891],\n",
            "        [-0.1732,  0.1372,  0.0614,  0.1865,  0.1596,  0.1634, -0.2327, -0.1635,\n",
            "          0.2091,  0.2816],\n",
            "        [ 0.2312,  0.0669, -0.2606, -0.0275, -0.0890,  0.1004,  0.3161, -0.0938,\n",
            "          0.0374,  0.2040],\n",
            "        [ 0.0995, -0.1977, -0.2605, -0.0258,  0.1096, -0.0129,  0.3051,  0.2174,\n",
            "          0.2622,  0.0835],\n",
            "        [ 0.0202,  0.1923,  0.0630,  0.1059,  0.2582,  0.2028, -0.2428, -0.2786,\n",
            "          0.1806, -0.0841],\n",
            "        [ 0.0803, -0.0337, -0.2958,  0.0553, -0.0049,  0.0458, -0.0387,  0.0067,\n",
            "          0.0616, -0.2451],\n",
            "        [-0.1169,  0.1632, -0.1843,  0.1811,  0.1460, -0.1654, -0.1977, -0.2663,\n",
            "          0.2580,  0.0648],\n",
            "        [-0.1918, -0.0032, -0.2594,  0.0225,  0.0500, -0.1224, -0.2418,  0.1106,\n",
            "         -0.1601, -0.2968],\n",
            "        [ 0.2524, -0.0309, -0.2905,  0.1863, -0.0222,  0.2242,  0.2127, -0.2278,\n",
            "         -0.1407,  0.0946],\n",
            "        [-0.1391,  0.1358, -0.1920, -0.1062,  0.2384, -0.1844,  0.0238, -0.3160,\n",
            "          0.2710,  0.2102],\n",
            "        [ 0.0898, -0.2875, -0.0676, -0.1747, -0.0314, -0.2192,  0.2574,  0.0109,\n",
            "         -0.1716, -0.0652],\n",
            "        [ 0.0435,  0.2209, -0.1260,  0.2931, -0.1144, -0.0712, -0.0828, -0.2168,\n",
            "         -0.2537, -0.3049]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.2232,  0.0422, -0.2157,  0.0686,  0.0474,  0.0705, -0.0475,  0.2974,\n",
            "        -0.2113,  0.0519, -0.2565, -0.2466, -0.2541, -0.1852,  0.1223, -0.0343,\n",
            "         0.0688,  0.2468,  0.0858,  0.0174], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.6335,  0.2979],\n",
            "        [-0.6168, -0.6770],\n",
            "        [-0.1025,  0.6122],\n",
            "        [ 0.5349,  0.3241],\n",
            "        [-0.6966,  0.0468],\n",
            "        [ 0.1733,  0.5813],\n",
            "        [-0.6364, -0.6160],\n",
            "        [-0.2331,  0.5929],\n",
            "        [ 0.2791, -0.1950],\n",
            "        [ 0.4276,  0.2772]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.6819, -0.4796,  0.6692, -0.1971,  0.4914,  0.3666,  0.1872, -0.4450,\n",
            "        -0.3534, -0.3081], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 5.1076e-02, -1.1927e-01, -1.3809e-01,  7.4424e-02, -2.2105e-01,\n",
            "          1.4587e-01,  1.1794e-01, -8.5990e-02, -5.7172e-02, -2.4184e-02],\n",
            "        [-1.1981e-01,  2.1669e-01, -1.8554e-01,  2.4428e-01,  3.3347e-02,\n",
            "          5.5553e-02, -3.1206e-01,  2.3720e-02,  1.5370e-02,  8.1464e-02],\n",
            "        [ 2.4392e-02, -2.6763e-01, -9.4466e-02,  5.3380e-02,  2.1701e-01,\n",
            "         -9.9303e-02,  2.8829e-01,  8.5137e-02,  4.7376e-02, -1.9416e-01],\n",
            "        [ 2.4318e-01,  1.7801e-02, -6.3745e-02,  2.0502e-01, -9.9438e-02,\n",
            "          2.4199e-01,  2.2420e-01, -9.6531e-02,  1.6698e-01, -1.7054e-01],\n",
            "        [-6.2823e-02,  7.2009e-02, -1.1809e-01, -7.9787e-02, -2.3820e-01,\n",
            "          1.7742e-02,  7.1465e-02,  2.5916e-02,  3.0433e-01,  5.7947e-02],\n",
            "        [-2.5596e-01,  5.4620e-02,  2.4083e-01, -6.0184e-02,  1.5053e-01,\n",
            "          2.6466e-01, -2.1124e-01, -4.1936e-02,  6.3285e-03,  9.3726e-02],\n",
            "        [-2.2827e-01,  7.7680e-02, -3.8515e-02, -1.6012e-01,  1.5789e-01,\n",
            "          1.0600e-02, -5.1904e-02, -1.5097e-02,  2.0788e-01, -2.4109e-01],\n",
            "        [-1.6645e-01,  2.5941e-01, -7.8294e-02,  1.3843e-02, -1.8036e-01,\n",
            "          3.8765e-02,  2.7720e-02,  1.6148e-01,  4.0172e-02, -2.3253e-01],\n",
            "        [ 1.3430e-01,  1.7313e-01,  2.3768e-01,  8.5341e-02, -2.0607e-01,\n",
            "         -3.0158e-01,  7.7710e-03,  1.9834e-01,  1.5498e-01,  2.3089e-01],\n",
            "        [ 8.8684e-02, -1.9987e-02, -1.3875e-01, -5.5686e-02,  1.7499e-01,\n",
            "         -1.5183e-01, -2.8973e-01,  3.0615e-01, -7.3128e-02,  1.8465e-01],\n",
            "        [ 1.8897e-01,  1.6550e-01,  7.8716e-02,  1.2621e-01, -3.0067e-01,\n",
            "         -1.0817e-01, -2.0255e-01, -1.3861e-01,  3.5438e-02,  2.0275e-01],\n",
            "        [-1.6359e-01,  3.1099e-01,  2.2721e-01,  5.1992e-02,  2.1009e-01,\n",
            "         -5.4382e-02, -9.5385e-02,  3.2658e-02, -3.5504e-03,  1.6976e-01],\n",
            "        [ 2.9553e-01,  4.1461e-02,  1.2313e-01, -1.0798e-01, -3.1571e-01,\n",
            "         -2.2907e-01,  2.7452e-01,  1.2213e-01, -8.9970e-02,  1.7478e-01],\n",
            "        [ 4.5366e-02, -1.9173e-01,  8.1539e-02,  2.0267e-01, -1.1699e-01,\n",
            "          2.9002e-01, -2.6187e-01, -1.0561e-01, -1.8005e-01,  2.6702e-01],\n",
            "        [-1.6621e-01, -1.8067e-01,  1.4628e-02, -1.1395e-02, -5.9085e-02,\n",
            "          2.1209e-01, -2.7041e-01,  1.0474e-01, -1.5088e-01,  9.9099e-02],\n",
            "        [-3.4239e-02, -2.3733e-01, -2.8622e-01,  4.7706e-02, -2.2541e-01,\n",
            "          2.7563e-01, -9.3656e-02, -3.6033e-02, -2.4504e-01,  9.3909e-02],\n",
            "        [ 1.0902e-01,  1.0276e-02, -3.3590e-03,  3.9748e-02,  1.9045e-01,\n",
            "         -2.0199e-01,  2.7780e-02, -2.2945e-01, -9.5494e-02, -3.0652e-01],\n",
            "        [ 1.5387e-01,  2.2081e-01, -1.9449e-01,  1.7302e-01,  1.1172e-01,\n",
            "          1.3704e-01,  1.6111e-02, -1.8255e-03, -2.0522e-01, -3.5426e-02],\n",
            "        [-1.8751e-01,  2.1737e-01,  1.9770e-01, -1.4182e-01, -1.0850e-01,\n",
            "         -1.8922e-04,  8.7962e-02,  1.7509e-01, -4.8631e-03, -4.8805e-03],\n",
            "        [-6.5218e-02, -1.6733e-01,  3.8713e-02,  5.9882e-04, -2.9078e-01,\n",
            "          1.4068e-01,  1.1552e-01,  2.6899e-02, -5.4252e-03,  8.3718e-02]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0651, -0.0781, -0.2115,  0.1258,  0.1866,  0.0964,  0.2203, -0.2464,\n",
            "        -0.0542,  0.2286, -0.0065, -0.2269,  0.2407, -0.1038,  0.1444,  0.2391,\n",
            "         0.0725, -0.1266,  0.0179,  0.0835], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}