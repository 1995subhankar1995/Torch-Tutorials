{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12oM3aYdXo2U1RnmAm5tlIaerqb_bxvfI",
      "authorship_tag": "ABX9TyNRovTz2ZcVlNcwm0/U5SSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1995subhankar1995/Torch-Tutorials/blob/main/Torch_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dQQ3kSqoC1A",
        "outputId": "6851c617-fd61-4d80-f7ac-085e94dda952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Calculation():\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "   \n",
        "\n",
        "  def add(self, z):\n",
        "    return self.x + self.y + z\n",
        "\n",
        "ha = Calculation(10, 3)\n",
        "print(ha.add(-8))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iIPtT_7qZBV"
      },
      "source": [
        "class Matrix():\n",
        "  def __init__(self, X, y, lr, W):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.lr = lr\n",
        "    self.W = W\n",
        "\n",
        "  def Cost(self):\n",
        "    self.W = self.W - self.lr * (np.matmul(np.matmul(np.transpose(self.X), self.X), self.W) - np.matmul(np.transpose(X), y))\n",
        "    cost = np.linalg.norm(np.matmul(self.X, self.W) - y)\n",
        "    return self.W, cost\n",
        "\n",
        "  def predict(self, X1):\n",
        "    print(self.W, 'W2')\n",
        "    prediction = np.matmul(X1, self.W)\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9r0vDRDWyFg"
      },
      "source": [
        "# Tensor Basics\n",
        "# Tutorial 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlcxx9719iNi",
        "outputId": "5cccde17-1be1-4a93-cfea-ff9d0f4a9e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y = np.array([[1], [0], [1], [1], [0]])\n",
        "X = np.random.normal(size = (5, 3))\n",
        "W = np.random.randn(3, 1)\n",
        "print(W, 'W1', np.transpose(X))\n",
        "b = np.array([[1, 1, 2, 4, 3]])\n",
        "\n",
        "ha = Matrix(X, y, 0.01, W)\n",
        "for j in range(10):\n",
        "  W, cost  = ha.Cost()\n",
        "  print(W, 'j11', j, cost)\n",
        "\n",
        "x1 = np.random.randn(7, 3)\n",
        "prediction = ha.predict(x1)\n",
        "\n",
        "\n",
        "for j in range(10):\n",
        "  print(W, 'W3')\n",
        "  W, cost  = ha.Cost()\n",
        "  print(W, 'j22', j, cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.61982586]\n",
            " [ 0.26774549]\n",
            " [ 0.32291553]] W1 [[-0.84530505 -0.55352138  0.28509478  0.51716962 -2.18263075]\n",
            " [ 0.66467791  1.85608159  1.19311702 -0.57472103  0.18889198]\n",
            " [ 1.53701543 -0.3695089   0.84152021  0.64319889 -1.55757081]]\n",
            "[[-0.58628661]\n",
            " [ 0.25105546]\n",
            " [ 0.34963202]] j11 0 1.706787013363352\n",
            "[[-0.55590015]\n",
            " [ 0.23578914]\n",
            " [ 0.37388074]] j11 1 1.6063440331477408\n",
            "[[-0.52837421]\n",
            " [ 0.22182062]\n",
            " [ 0.39589202]] j11 2 1.5185994264655784\n",
            "[[-0.50344353]\n",
            " [ 0.20903534]\n",
            " [ 0.41587469]] j11 3 1.4422923572107897\n",
            "[[-0.48086739]\n",
            " [ 0.19732902]\n",
            " [ 0.43401809]] j11 4 1.376236361341479\n",
            "[[-0.46042732]\n",
            " [ 0.18660673]\n",
            " [ 0.45049391]] j11 5 1.319317062911159\n",
            "[[-0.44192503]\n",
            " [ 0.17678204]\n",
            " [ 0.46545778]] j11 6 1.2704922147058129\n",
            "[[-0.42518059]\n",
            " [ 0.16777627]\n",
            " [ 0.47905083]] j11 7 1.2287931858850478\n",
            "[[-0.4100307 ]\n",
            " [ 0.15951778]\n",
            " [ 0.49140099]] j11 8 1.193326950584961\n",
            "[[-0.39632715]\n",
            " [ 0.15194131]\n",
            " [ 0.50262424]] j11 9 1.163277716432885\n",
            "[[-0.39632715]\n",
            " [ 0.15194131]\n",
            " [ 0.50262424]] W2\n",
            "[[-0.39632715]\n",
            " [ 0.15194131]\n",
            " [ 0.50262424]] W3\n",
            "[[-0.38393545]\n",
            " [ 0.14498745]\n",
            " [ 0.51282575]] j22 0 1.1379075403996177\n",
            "[[-0.38393545]\n",
            " [ 0.14498745]\n",
            " [ 0.51282575]] W3\n",
            "[[-0.37273354]\n",
            " [ 0.13860206]\n",
            " [ 0.52210083]] j22 1 1.1165555530445475\n",
            "[[-0.37273354]\n",
            " [ 0.13860206]\n",
            " [ 0.52210083]] W3\n",
            "[[-0.36261066]\n",
            " [ 0.13273583]\n",
            " [ 0.53053589]] j22 2 1.0986356867458016\n",
            "[[-0.36261066]\n",
            " [ 0.13273583]\n",
            " [ 0.53053589]] W3\n",
            "[[-0.35346628]\n",
            " [ 0.12734383]\n",
            " [ 0.53820927]] j22 3 1.0836330286312845\n",
            "[[-0.35346628]\n",
            " [ 0.12734383]\n",
            " [ 0.53820927]] W3\n",
            "[[-0.34520917]\n",
            " [ 0.12238515]\n",
            " [ 0.54519197]] j22 4 1.0710990683400947\n",
            "[[-0.34520917]\n",
            " [ 0.12238515]\n",
            " [ 0.54519197]] W3\n",
            "[[-0.33775657]\n",
            " [ 0.11782245]\n",
            " [ 0.55154837]] j22 5 1.0606461816992003\n",
            "[[-0.33775657]\n",
            " [ 0.11782245]\n",
            " [ 0.55154837]] W3\n",
            "[[-0.33103334]\n",
            " [ 0.11362175]\n",
            " [ 0.5573368 ]] j22 6 1.0519416974563978\n",
            "[[-0.33103334]\n",
            " [ 0.11362175]\n",
            " [ 0.5573368 ]] W3\n",
            "[[-0.32497131]\n",
            " [ 0.10975206]\n",
            " [ 0.56261015]] j22 7 1.0447018561893684\n",
            "[[-0.32497131]\n",
            " [ 0.10975206]\n",
            " [ 0.56261015]] W3\n",
            "[[-0.31950863]\n",
            " [ 0.1061851 ]\n",
            " [ 0.56741635]] j22 8 1.0386859091999763\n",
            "[[-0.31950863]\n",
            " [ 0.1061851 ]\n",
            " [ 0.56741635]] W3\n",
            "[[-0.31458917]\n",
            " [ 0.10289513]\n",
            " [ 0.57179886]] j22 9 1.0336905371685612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8zH-GUr_i6W",
        "outputId": "1339f84d-bcd8-41aa-97ed-3317a4fea311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "a = torch.randn(3, requires_grad = True)\n",
        "print(a, 'a')\n",
        "b = a.mean()\n",
        "print(b, 'b')\n",
        "b.backward() # db/da\n",
        "print(a.grad, 'grad')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0076, 0.9647, 3.3311], requires_grad=True) a\n",
            "tensor(2.1011, grad_fn=<MeanBackward0>) b\n",
            "tensor([0.3333, 0.3333, 0.3333]) grad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypRnbbpGGng5"
      },
      "source": [
        "# Detach from gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBX764PMBCkL",
        "outputId": "26a8c68f-ffef-4930-a561-2c7f8f984d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "x = torch.randn(5, requires_grad = True)\n",
        "\n",
        "print(x)\n",
        "#x.requires_grad_(False)\n",
        "#x.detach()\n",
        "#with torch.no_grad():\n",
        "#x.requires_grad_(False)\n",
        "with torch.no_grad():\n",
        "  y = x + 11\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.1019,  1.0680,  0.1440, -1.3743, -0.0349], requires_grad=True)\n",
            "tensor([ 9.8981, 12.0680, 11.1440,  9.6257, 10.9651])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmSp4KitGhKr"
      },
      "source": [
        "# Gradient Calculation with autograd\n",
        "# Tutorial 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XByQp2keEuVt",
        "outputId": "34108fd5-f00c-44ea-c4ae-2a02527eee1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "x = torch.ones(4, requires_grad = True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (x * 3).sum()\n",
        "  model_output.backward()\n",
        "  print(x.grad) # gradients are accumulating each time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([9., 9., 9., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9bpcmgWGWx_",
        "outputId": "41b49062-269f-497b-8b6e-38ae967928a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "x = torch.ones(4, requires_grad = True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (x * 3).sum()\n",
        "  model_output.backward()\n",
        "  print(x.grad) # gradients are accumulating each time\n",
        "\n",
        "  x.grad.zero_() #gradients will not accumulate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPWm3zKsG4Bt",
        "outputId": "319304ef-7f09-47e1-9f01-44c723174226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "x = torch.tensor([1.0, 2.0])\n",
        "y = torch.tensor(1)\n",
        "w = torch.tensor([-1.0, 1.2], requires_grad = True)\n",
        "\n",
        "loss = (torch.matmul(x, w) - y)**2\n",
        "print(loss, 'loss')\n",
        "loss.backward()\n",
        "\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1600, grad_fn=<PowBackward0>) loss\n",
            "tensor([0.8000, 1.6000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5EerAzLXeim"
      },
      "source": [
        "# Gradient Descent with autograd\n",
        "# Tutorial 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohls42MxUDcR",
        "outputId": "1adfafb6-c35a-4312-bdb7-b895a8efa021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "class MODEL():\n",
        "  def __init__(self, X, y, w):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.w = w\n",
        "    print(self.w, 'init_w')\n",
        "\n",
        "  def forward(self):\n",
        "    print(self.w, 'forward_w')\n",
        "    return np.matmul(self.X, self.w)\n",
        "\n",
        "  def loss(self, y_pred, y_act):\n",
        "    print(self.w, 'loss_w')\n",
        "    return np.mean((y_act - y_pred)**2)\n",
        "\n",
        "  def Gradient(self, y_pred):\n",
        "    print(self.w, 'grad_w')\n",
        "    return np.matmul(np.transpose(self.X), np.array(np.matmul(self.X, self.w) - y_pred))\n",
        "\n",
        "\n",
        "X = np.array([[2, 3, 4], [1, 2, 4]])\n",
        "y = np.array([[1], [-1]])\n",
        "w = np.array([[2], [3], [1]])\n",
        "lr = 0.01\n",
        "predictions = np.matmul(X, w)\n",
        "print(predictions, 'predictions0')\n",
        "print(w, 'w')\n",
        "for i in range(4):\n",
        "  cal = MODEL(X, y, w)\n",
        "  y_pred = cal.forward() \n",
        "  #print(y_pred, 'y_pred', i)\n",
        "  Loss = cal.loss(y_pred, y)\n",
        "  print(Loss, 'Loss', i)\n",
        "  grad = cal.Gradient(y)\n",
        "  #print(grad, 'grad')\n",
        "\n",
        "  w = w - lr * grad\n",
        "  #print(w, 'w', i)\n",
        "\n",
        "X = np.array([[4, 3, 1], [1, 0, -1]])\n",
        "y = np.array([[1], [-1]])\n",
        "#w = np.array([0])\n",
        "predictions = np.matmul(X, w)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "print('next')\n",
        "for i in range(500):\n",
        "  calcu = MODEL(X, y, w)\n",
        "  y_pred = calcu.forward()\n",
        "  Loss = calcu.loss(y_pred, y)\n",
        "  print(Loss, 'Loss', i)\n",
        "  grad = calcu.Gradient(y)\n",
        "  w = w - lr * grad\n",
        "predictions = np.matmul(X, w)\n",
        "print(predictions, 'predictions2')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " [-0.02732329]] loss_w\n",
            "0.06756454989105852 Loss 115\n",
            "[[-0.6624784 ]\n",
            " [ 1.21077414]\n",
            " [-0.02732329]] grad_w\n",
            "[[-0.66433027]\n",
            " [ 1.21212158]\n",
            " [-0.02322569]] init_w\n",
            "[[-0.66433027]\n",
            " [ 1.21212158]\n",
            " [-0.02322569]] forward_w\n",
            "[[-0.66433027]\n",
            " [ 1.21212158]\n",
            " [-0.02322569]] loss_w\n",
            "0.06537898874136457 Loss 116\n",
            "[[-0.66433027]\n",
            " [ 1.21212158]\n",
            " [-0.02322569]] grad_w\n",
            "[[-0.66615194]\n",
            " [ 1.21344704]\n",
            " [-0.01919492]] init_w\n",
            "[[-0.66615194]\n",
            " [ 1.21344704]\n",
            " [-0.01919492]] forward_w\n",
            "[[-0.66615194]\n",
            " [ 1.21344704]\n",
            " [-0.01919492]] loss_w\n",
            "0.06326412557673455 Loss 117\n",
            "[[-0.66615194]\n",
            " [ 1.21344704]\n",
            " [-0.01919492]] grad_w\n",
            "[[-0.66794391]\n",
            " [ 1.21475089]\n",
            " [-0.01522987]] init_w\n",
            "[[-0.66794391]\n",
            " [ 1.21475089]\n",
            " [-0.01522987]] forward_w\n",
            "[[-0.66794391]\n",
            " [ 1.21475089]\n",
            " [-0.01522987]] loss_w\n",
            "0.061217673476442074 Loss 118\n",
            "[[-0.66794391]\n",
            " [ 1.21475089]\n",
            " [-0.01522987]] grad_w\n",
            "[[-0.66970666]\n",
            " [ 1.21603347]\n",
            " [-0.01132949]] init_w\n",
            "[[-0.66970666]\n",
            " [ 1.21603347]\n",
            " [-0.01132949]] forward_w\n",
            "[[-0.66970666]\n",
            " [ 1.21603347]\n",
            " [-0.01132949]] loss_w\n",
            "0.0592374194965 Loss 119\n",
            "[[-0.66970666]\n",
            " [ 1.21603347]\n",
            " [-0.01132949]] grad_w\n",
            "[[-0.67144066]\n",
            " [ 1.21729514]\n",
            " [-0.0074927 ]] init_w\n",
            "[[-0.67144066]\n",
            " [ 1.21729514]\n",
            " [-0.0074927 ]] forward_w\n",
            "[[-0.67144066]\n",
            " [ 1.21729514]\n",
            " [-0.0074927 ]] loss_w\n",
            "0.057321222276679414 Loss 120\n",
            "[[-0.67144066]\n",
            " [ 1.21729514]\n",
            " [-0.0074927 ]] grad_w\n",
            "[[-0.67314638]\n",
            " [ 1.21853624]\n",
            " [-0.00371848]] init_w\n",
            "[[-0.67314638]\n",
            " [ 1.21853624]\n",
            " [-0.00371848]] forward_w\n",
            "[[-0.67314638]\n",
            " [ 1.21853624]\n",
            " [-0.00371848]] loss_w\n",
            "0.05546700972493615 Loss 121\n",
            "[[-0.67314638]\n",
            " [ 1.21853624]\n",
            " [-0.00371848]] grad_w\n",
            "[[-6.74824291e-01]\n",
            " [ 1.21975710e+00]\n",
            " [-5.80736304e-06]] init_w\n",
            "[[-6.74824291e-01]\n",
            " [ 1.21975710e+00]\n",
            " [-5.80736304e-06]] forward_w\n",
            "[[-6.74824291e-01]\n",
            " [ 1.21975710e+00]\n",
            " [-5.80736304e-06]] loss_w\n",
            "0.05367277677674091 Loss 122\n",
            "[[-6.74824291e-01]\n",
            " [ 1.21975710e+00]\n",
            " [-5.80736304e-06]] grad_w\n",
            "[[-0.67647484]\n",
            " [ 1.22095805]\n",
            " [ 0.00364632]] init_w\n",
            "[[-0.67647484]\n",
            " [ 1.22095805]\n",
            " [ 0.00364632]] forward_w\n",
            "[[-0.67647484]\n",
            " [ 1.22095805]\n",
            " [ 0.00364632]] loss_w\n",
            "0.05193658322688991 Loss 123\n",
            "[[-0.67647484]\n",
            " [ 1.22095805]\n",
            " [ 0.00364632]] grad_w\n",
            "[[-0.67809847]\n",
            " [ 1.22213942]\n",
            " [ 0.0072389 ]] init_w\n",
            "[[-0.67809847]\n",
            " [ 1.22213942]\n",
            " [ 0.0072389 ]] forward_w\n",
            "[[-0.67809847]\n",
            " [ 1.22213942]\n",
            " [ 0.0072389 ]] loss_w\n",
            "0.05025655163145171 Loss 124\n",
            "[[-0.67809847]\n",
            " [ 1.22213942]\n",
            " [ 0.0072389 ]] grad_w\n",
            "[[-0.67969563]\n",
            " [ 1.22330152]\n",
            " [ 0.0107729 ]] init_w\n",
            "[[-0.67969563]\n",
            " [ 1.22330152]\n",
            " [ 0.0107729 ]] forward_w\n",
            "[[-0.67969563]\n",
            " [ 1.22330152]\n",
            " [ 0.0107729 ]] loss_w\n",
            "0.0486308652775813 Loss 125\n",
            "[[-0.67969563]\n",
            " [ 1.22330152]\n",
            " [ 0.0107729 ]] grad_w\n",
            "[[-0.68126674]\n",
            " [ 1.22444467]\n",
            " [ 0.01424926]] init_w\n",
            "[[-0.68126674]\n",
            " [ 1.22444467]\n",
            " [ 0.01424926]] forward_w\n",
            "[[-0.68126674]\n",
            " [ 1.22444467]\n",
            " [ 0.01424926]] loss_w\n",
            "0.047057766219006036 Loss 126\n",
            "[[-0.68126674]\n",
            " [ 1.22444467]\n",
            " [ 0.01424926]] grad_w\n",
            "[[-0.68281223]\n",
            " [ 1.22556918]\n",
            " [ 0.01766894]] init_w\n",
            "[[-0.68281223]\n",
            " [ 1.22556918]\n",
            " [ 0.01766894]] forward_w\n",
            "[[-0.68281223]\n",
            " [ 1.22556918]\n",
            " [ 0.01766894]] loss_w\n",
            "0.045535553375059455 Loss 127\n",
            "[[-0.68281223]\n",
            " [ 1.22556918]\n",
            " [ 0.01766894]] grad_w\n",
            "[[-0.68433252]\n",
            " [ 1.22667535]\n",
            " [ 0.02103285]] init_w\n",
            "[[-0.68433252]\n",
            " [ 1.22667535]\n",
            " [ 0.02103285]] forward_w\n",
            "[[-0.68433252]\n",
            " [ 1.22667535]\n",
            " [ 0.02103285]] loss_w\n",
            "0.04406258069120653 Loss 128\n",
            "[[-0.68433252]\n",
            " [ 1.22667535]\n",
            " [ 0.02103285]] grad_w\n",
            "[[-0.68582802]\n",
            " [ 1.22776349]\n",
            " [ 0.02434191]] init_w\n",
            "[[-0.68582802]\n",
            " [ 1.22776349]\n",
            " [ 0.02434191]] forward_w\n",
            "[[-0.68582802]\n",
            " [ 1.22776349]\n",
            " [ 0.02434191]] loss_w\n",
            "0.04263725535907254 Loss 129\n",
            "[[-0.68582802]\n",
            " [ 1.22776349]\n",
            " [ 0.02434191]] grad_w\n",
            "[[-0.68729913]\n",
            " [ 1.22883388]\n",
            " [ 0.02759701]] init_w\n",
            "[[-0.68729913]\n",
            " [ 1.22883388]\n",
            " [ 0.02759701]] forward_w\n",
            "[[-0.68729913]\n",
            " [ 1.22883388]\n",
            " [ 0.02759701]] loss_w\n",
            "0.041258036094049336 Loss 130\n",
            "[[-0.68729913]\n",
            " [ 1.22883388]\n",
            " [ 0.02759701]] grad_w\n",
            "[[-0.68874626]\n",
            " [ 1.22988682]\n",
            " [ 0.03079902]] init_w\n",
            "[[-0.68874626]\n",
            " [ 1.22988682]\n",
            " [ 0.03079902]] forward_w\n",
            "[[-0.68874626]\n",
            " [ 1.22988682]\n",
            " [ 0.03079902]] loss_w\n",
            "0.039923431468617494 Loss 131\n",
            "[[-0.68874626]\n",
            " [ 1.22988682]\n",
            " [ 0.03079902]] grad_w\n",
            "[[-0.69016978]\n",
            " [ 1.23092258]\n",
            " [ 0.03394883]] init_w\n",
            "[[-0.69016978]\n",
            " [ 1.23092258]\n",
            " [ 0.03394883]] forward_w\n",
            "[[-0.69016978]\n",
            " [ 1.23092258]\n",
            " [ 0.03394883]] loss_w\n",
            "0.0386319982995818 Loss 132\n",
            "[[-0.69016978]\n",
            " [ 1.23092258]\n",
            " [ 0.03394883]] grad_w\n",
            "[[-0.69157009]\n",
            " [ 1.23194146]\n",
            " [ 0.03704727]] init_w\n",
            "[[-0.69157009]\n",
            " [ 1.23194146]\n",
            " [ 0.03704727]] forward_w\n",
            "[[-0.69157009]\n",
            " [ 1.23194146]\n",
            " [ 0.03704727]] loss_w\n",
            "0.037382340087475735 Loss 133\n",
            "[[-0.69157009]\n",
            " [ 1.23194146]\n",
            " [ 0.03704727]] grad_w\n",
            "[[-0.69294757]\n",
            " [ 1.23294372]\n",
            " [ 0.04009518]] init_w\n",
            "[[-0.69294757]\n",
            " [ 1.23294372]\n",
            " [ 0.04009518]] forward_w\n",
            "[[-0.69294757]\n",
            " [ 1.23294372]\n",
            " [ 0.04009518]] loss_w\n",
            "0.036173105506448096 Loss 134\n",
            "[[-0.69294757]\n",
            " [ 1.23294372]\n",
            " [ 0.04009518]] grad_w\n",
            "[[-0.69430259]\n",
            " [ 1.23392964]\n",
            " [ 0.04309339]] init_w\n",
            "[[-0.69430259]\n",
            " [ 1.23392964]\n",
            " [ 0.04309339]] forward_w\n",
            "[[-0.69430259]\n",
            " [ 1.23392964]\n",
            " [ 0.04309339]] loss_w\n",
            "0.03500298694299808 Loss 135\n",
            "[[-0.69430259]\n",
            " [ 1.23392964]\n",
            " [ 0.04309339]] grad_w\n",
            "[[-0.69563551]\n",
            " [ 1.23489948]\n",
            " [ 0.04604271]] init_w\n",
            "[[-0.69563551]\n",
            " [ 1.23489948]\n",
            " [ 0.04604271]] forward_w\n",
            "[[-0.69563551]\n",
            " [ 1.23489948]\n",
            " [ 0.04604271]] loss_w\n",
            "0.033870719081979056 Loss 136\n",
            "[[-0.69563551]\n",
            " [ 1.23489948]\n",
            " [ 0.04604271]] grad_w\n",
            "[[-0.69694669]\n",
            " [ 1.23585351]\n",
            " [ 0.04894394]] init_w\n",
            "[[-0.69694669]\n",
            " [ 1.23585351]\n",
            " [ 0.04894394]] forward_w\n",
            "[[-0.69694669]\n",
            " [ 1.23585351]\n",
            " [ 0.04894394]] loss_w\n",
            "0.03277507753834214 Loss 137\n",
            "[[-0.69694669]\n",
            " [ 1.23585351]\n",
            " [ 0.04894394]] grad_w\n",
            "[[-0.69823649]\n",
            " [ 1.23679198]\n",
            " [ 0.05179786]] init_w\n",
            "[[-0.69823649]\n",
            " [ 1.23679198]\n",
            " [ 0.05179786]] forward_w\n",
            "[[-0.69823649]\n",
            " [ 1.23679198]\n",
            " [ 0.05179786]] loss_w\n",
            "0.031714877533139584 Loss 138\n",
            "[[-0.69823649]\n",
            " [ 1.23679198]\n",
            " [ 0.05179786]] grad_w\n",
            "[[-0.69950526]\n",
            " [ 1.23771514]\n",
            " [ 0.05460523]] init_w\n",
            "[[-0.69950526]\n",
            " [ 1.23771514]\n",
            " [ 0.05460523]] forward_w\n",
            "[[-0.69950526]\n",
            " [ 1.23771514]\n",
            " [ 0.05460523]] loss_w\n",
            "0.030688972612356487 Loss 139\n",
            "[[-0.69950526]\n",
            " [ 1.23771514]\n",
            " [ 0.05460523]] grad_w\n",
            "[[-0.70075334]\n",
            " [ 1.23862325]\n",
            " [ 0.05736683]] init_w\n",
            "[[-0.70075334]\n",
            " [ 1.23862325]\n",
            " [ 0.05736683]] forward_w\n",
            "[[-0.70075334]\n",
            " [ 1.23862325]\n",
            " [ 0.05736683]] loss_w\n",
            "0.029696253407185465 Loss 140\n",
            "[[-0.70075334]\n",
            " [ 1.23862325]\n",
            " [ 0.05736683]] grad_w\n",
            "[[-0.70198107]\n",
            " [ 1.23951656]\n",
            " [ 0.0600834 ]] init_w\n",
            "[[-0.70198107]\n",
            " [ 1.23951656]\n",
            " [ 0.0600834 ]] forward_w\n",
            "[[-0.70198107]\n",
            " [ 1.23951656]\n",
            " [ 0.0600834 ]] loss_w\n",
            "0.02873564643440373 Loss 141\n",
            "[[-0.70198107]\n",
            " [ 1.23951656]\n",
            " [ 0.0600834 ]] grad_w\n",
            "[[-0.70318877]\n",
            " [ 1.24039529]\n",
            " [ 0.06275567]] init_w\n",
            "[[-0.70318877]\n",
            " [ 1.24039529]\n",
            " [ 0.06275567]] forward_w\n",
            "[[-0.70318877]\n",
            " [ 1.24039529]\n",
            " [ 0.06275567]] loss_w\n",
            "0.02780611293555499 Loss 142\n",
            "[[-0.70318877]\n",
            " [ 1.24039529]\n",
            " [ 0.06275567]] grad_w\n",
            "[[-0.70437679]\n",
            " [ 1.2412597 ]\n",
            " [ 0.06538436]] init_w\n",
            "[[-0.70437679]\n",
            " [ 1.2412597 ]\n",
            " [ 0.06538436]] forward_w\n",
            "[[-0.70437679]\n",
            " [ 1.2412597 ]\n",
            " [ 0.06538436]] loss_w\n",
            "0.02690664775368163 Loss 143\n",
            "[[-0.70437679]\n",
            " [ 1.2412597 ]\n",
            " [ 0.06538436]] grad_w\n",
            "[[-0.70554543]\n",
            " [ 1.24211001]\n",
            " [ 0.06797018]] init_w\n",
            "[[-0.70554543]\n",
            " [ 1.24211001]\n",
            " [ 0.06797018]] forward_w\n",
            "[[-0.70554543]\n",
            " [ 1.24211001]\n",
            " [ 0.06797018]] loss_w\n",
            "0.026036278246391712 Loss 144\n",
            "[[-0.70554543]\n",
            " [ 1.24211001]\n",
            " [ 0.06797018]] grad_w\n",
            "[[-0.70669501]\n",
            " [ 1.24294646]\n",
            " [ 0.07051384]] init_w\n",
            "[[-0.70669501]\n",
            " [ 1.24294646]\n",
            " [ 0.07051384]] forward_w\n",
            "[[-0.70669501]\n",
            " [ 1.24294646]\n",
            " [ 0.07051384]] loss_w\n",
            "0.025194063234085875 Loss 145\n",
            "[[-0.70669501]\n",
            " [ 1.24294646]\n",
            " [ 0.07051384]] grad_w\n",
            "[[-0.70782585]\n",
            " [ 1.24376926]\n",
            " [ 0.07301602]] init_w\n",
            "[[-0.70782585]\n",
            " [ 1.24376926]\n",
            " [ 0.07301602]] forward_w\n",
            "[[-0.70782585]\n",
            " [ 1.24376926]\n",
            " [ 0.07301602]] loss_w\n",
            "0.024379091982206946 Loss 146\n",
            "[[-0.70782585]\n",
            " [ 1.24376926]\n",
            " [ 0.07301602]] grad_w\n",
            "[[-0.70893825]\n",
            " [ 1.24457865]\n",
            " [ 0.0754774 ]] init_w\n",
            "[[-0.70893825]\n",
            " [ 1.24457865]\n",
            " [ 0.0754774 ]] forward_w\n",
            "[[-0.70893825]\n",
            " [ 1.24457865]\n",
            " [ 0.0754774 ]] loss_w\n",
            "0.023590483216411263 Loss 147\n",
            "[[-0.70893825]\n",
            " [ 1.24457865]\n",
            " [ 0.0754774 ]] grad_w\n",
            "[[-0.71003251]\n",
            " [ 1.24537484]\n",
            " [ 0.07789864]] init_w\n",
            "[[-0.71003251]\n",
            " [ 1.24537484]\n",
            " [ 0.07789864]] forward_w\n",
            "[[-0.71003251]\n",
            " [ 1.24537484]\n",
            " [ 0.07789864]] loss_w\n",
            "0.022827384169597086 Loss 148\n",
            "[[-0.71003251]\n",
            " [ 1.24537484]\n",
            " [ 0.07789864]] grad_w\n",
            "[[-0.71110892]\n",
            " [ 1.24615804]\n",
            " [ 0.0802804 ]] init_w\n",
            "[[-0.71110892]\n",
            " [ 1.24615804]\n",
            " [ 0.0802804 ]] forward_w\n",
            "[[-0.71110892]\n",
            " [ 1.24615804]\n",
            " [ 0.0802804 ]] loss_w\n",
            "0.022088969659759386 Loss 149\n",
            "[[-0.71110892]\n",
            " [ 1.24615804]\n",
            " [ 0.0802804 ]] grad_w\n",
            "[[-0.71216778]\n",
            " [ 1.24692848]\n",
            " [ 0.08262331]] init_w\n",
            "[[-0.71216778]\n",
            " [ 1.24692848]\n",
            " [ 0.08262331]] forward_w\n",
            "[[-0.71216778]\n",
            " [ 1.24692848]\n",
            " [ 0.08262331]] loss_w\n",
            "0.02137444119767415 Loss 150\n",
            "[[-0.71216778]\n",
            " [ 1.24692848]\n",
            " [ 0.08262331]] grad_w\n",
            "[[-0.71320937]\n",
            " [ 1.24768635]\n",
            " [ 0.08492803]] init_w\n",
            "[[-0.71320937]\n",
            " [ 1.24768635]\n",
            " [ 0.08492803]] forward_w\n",
            "[[-0.71320937]\n",
            " [ 1.24768635]\n",
            " [ 0.08492803]] loss_w\n",
            "0.02068302612344694 Loss 151\n",
            "[[-0.71320937]\n",
            " [ 1.24768635]\n",
            " [ 0.08492803]] grad_w\n",
            "[[-0.71423398]\n",
            " [ 1.24843186]\n",
            " [ 0.08719516]] init_w\n",
            "[[-0.71423398]\n",
            " [ 1.24843186]\n",
            " [ 0.08719516]] forward_w\n",
            "[[-0.71423398]\n",
            " [ 1.24843186]\n",
            " [ 0.08719516]] loss_w\n",
            "0.020013976770992162 Loss 152\n",
            "[[-0.71423398]\n",
            " [ 1.24843186]\n",
            " [ 0.08719516]] grad_w\n",
            "[[-0.71524188]\n",
            " [ 1.24916522]\n",
            " [ 0.08942532]] init_w\n",
            "[[-0.71524188]\n",
            " [ 1.24916522]\n",
            " [ 0.08942532]] forward_w\n",
            "[[-0.71524188]\n",
            " [ 1.24916522]\n",
            " [ 0.08942532]] loss_w\n",
            "0.01936656965953967 Loss 153\n",
            "[[-0.71524188]\n",
            " [ 1.24916522]\n",
            " [ 0.08942532]] grad_w\n",
            "[[-0.71623335]\n",
            " [ 1.24988661]\n",
            " [ 0.09161911]] init_w\n",
            "[[-0.71623335]\n",
            " [ 1.24988661]\n",
            " [ 0.09161911]] forward_w\n",
            "[[-0.71623335]\n",
            " [ 1.24988661]\n",
            " [ 0.09161911]] loss_w\n",
            "0.018740104711294173 Loss 154\n",
            "[[-0.71623335]\n",
            " [ 1.24988661]\n",
            " [ 0.09161911]] grad_w\n",
            "[[-0.71720865]\n",
            " [ 1.25059625]\n",
            " [ 0.09377713]] init_w\n",
            "[[-0.71720865]\n",
            " [ 1.25059625]\n",
            " [ 0.09377713]] forward_w\n",
            "[[-0.71720865]\n",
            " [ 1.25059625]\n",
            " [ 0.09377713]] loss_w\n",
            "0.018133904494401706 Loss 155\n",
            "[[-0.71720865]\n",
            " [ 1.25059625]\n",
            " [ 0.09377713]] grad_w\n",
            "[[-0.71816804]\n",
            " [ 1.25129431]\n",
            " [ 0.09589996]] init_w\n",
            "[[-0.71816804]\n",
            " [ 1.25129431]\n",
            " [ 0.09589996]] forward_w\n",
            "[[-0.71816804]\n",
            " [ 1.25129431]\n",
            " [ 0.09589996]] loss_w\n",
            "0.01754731349040436 Loss 156\n",
            "[[-0.71816804]\n",
            " [ 1.25129431]\n",
            " [ 0.09589996]] grad_w\n",
            "[[-0.71911179]\n",
            " [ 1.25198099]\n",
            " [ 0.09798817]] init_w\n",
            "[[-0.71911179]\n",
            " [ 1.25198099]\n",
            " [ 0.09798817]] forward_w\n",
            "[[-0.71911179]\n",
            " [ 1.25198099]\n",
            " [ 0.09798817]] loss_w\n",
            "0.016979697385391217 Loss 157\n",
            "[[-0.71911179]\n",
            " [ 1.25198099]\n",
            " [ 0.09798817]] grad_w\n",
            "[[-0.72004015]\n",
            " [ 1.25265647]\n",
            " [ 0.10004233]] init_w\n",
            "[[-0.72004015]\n",
            " [ 1.25265647]\n",
            " [ 0.10004233]] forward_w\n",
            "[[-0.72004015]\n",
            " [ 1.25265647]\n",
            " [ 0.10004233]] loss_w\n",
            "0.016430442384078994 Loss 158\n",
            "[[-0.72004015]\n",
            " [ 1.25265647]\n",
            " [ 0.10004233]] grad_w\n",
            "[[-0.72095337]\n",
            " [ 1.25332093]\n",
            " [ 0.102063  ]] init_w\n",
            "[[-0.72095337]\n",
            " [ 1.25332093]\n",
            " [ 0.102063  ]] forward_w\n",
            "[[-0.72095337]\n",
            " [ 1.25332093]\n",
            " [ 0.102063  ]] loss_w\n",
            "0.015898954546080674 Loss 159\n",
            "[[-0.72095337]\n",
            " [ 1.25332093]\n",
            " [ 0.102063  ]] grad_w\n",
            "[[-0.7218517 ]\n",
            " [ 1.25397456]\n",
            " [ 0.10405071]] init_w\n",
            "[[-0.7218517 ]\n",
            " [ 1.25397456]\n",
            " [ 0.10405071]] forward_w\n",
            "[[-0.7218517 ]\n",
            " [ 1.25397456]\n",
            " [ 0.10405071]] loss_w\n",
            "0.015384659143644152 Loss 160\n",
            "[[-0.7218517 ]\n",
            " [ 1.25397456]\n",
            " [ 0.10405071]] grad_w\n",
            "[[-0.72273538]\n",
            " [ 1.25461754]\n",
            " [ 0.10600601]] init_w\n",
            "[[-0.72273538]\n",
            " [ 1.25461754]\n",
            " [ 0.10600601]] forward_w\n",
            "[[-0.72273538]\n",
            " [ 1.25461754]\n",
            " [ 0.10600601]] loss_w\n",
            "0.01488700004016684 Loss 161\n",
            "[[-0.72273538]\n",
            " [ 1.25461754]\n",
            " [ 0.10600601]] grad_w\n",
            "[[-0.72360465]\n",
            " [ 1.25525002]\n",
            " [ 0.10792943]] init_w\n",
            "[[-0.72360465]\n",
            " [ 1.25525002]\n",
            " [ 0.10792943]] forward_w\n",
            "[[-0.72360465]\n",
            " [ 1.25525002]\n",
            " [ 0.10792943]] loss_w\n",
            "0.014405439088813747 Loss 162\n",
            "[[-0.72360465]\n",
            " [ 1.25525002]\n",
            " [ 0.10792943]] grad_w\n",
            "[[-0.72445974]\n",
            " [ 1.2558722 ]\n",
            " [ 0.10982148]] init_w\n",
            "[[-0.72445974]\n",
            " [ 1.2558722 ]\n",
            " [ 0.10982148]] forward_w\n",
            "[[-0.72445974]\n",
            " [ 1.2558722 ]\n",
            " [ 0.10982148]] loss_w\n",
            "0.013939455550589046 Loss 163\n",
            "[[-0.72445974]\n",
            " [ 1.2558722 ]\n",
            " [ 0.10982148]] grad_w\n",
            "[[-0.7253009 ]\n",
            " [ 1.25648422]\n",
            " [ 0.11168267]] init_w\n",
            "[[-0.7253009 ]\n",
            " [ 1.25648422]\n",
            " [ 0.11168267]] forward_w\n",
            "[[-0.7253009 ]\n",
            " [ 1.25648422]\n",
            " [ 0.11168267]] loss_w\n",
            "0.013488545531231624 Loss 164\n",
            "[[-0.7253009 ]\n",
            " [ 1.25648422]\n",
            " [ 0.11168267]] grad_w\n",
            "[[-0.72612833]\n",
            " [ 1.25708627]\n",
            " [ 0.11351352]] init_w\n",
            "[[-0.72612833]\n",
            " [ 1.25708627]\n",
            " [ 0.11351352]] forward_w\n",
            "[[-0.72612833]\n",
            " [ 1.25708627]\n",
            " [ 0.11351352]] loss_w\n",
            "0.013052221436325758 Loss 165\n",
            "[[-0.72612833]\n",
            " [ 1.25708627]\n",
            " [ 0.11351352]] grad_w\n",
            "[[-0.72694227]\n",
            " [ 1.2576785 ]\n",
            " [ 0.11531451]] init_w\n",
            "[[-0.72694227]\n",
            " [ 1.2576785 ]\n",
            " [ 0.11531451]] forward_w\n",
            "[[-0.72694227]\n",
            " [ 1.2576785 ]\n",
            " [ 0.11531451]] loss_w\n",
            "0.01263001144403788 Loss 166\n",
            "[[-0.72694227]\n",
            " [ 1.2576785 ]\n",
            " [ 0.11531451]] grad_w\n",
            "[[-0.72774294]\n",
            " [ 1.25826107]\n",
            " [ 0.11708613]] init_w\n",
            "[[-0.72774294]\n",
            " [ 1.25826107]\n",
            " [ 0.11708613]] forward_w\n",
            "[[-0.72774294]\n",
            " [ 1.25826107]\n",
            " [ 0.11708613]] loss_w\n",
            "0.012221458994909045 Loss 167\n",
            "[[-0.72774294]\n",
            " [ 1.25826107]\n",
            " [ 0.11708613]] grad_w\n",
            "[[-0.72853055]\n",
            " [ 1.25883415]\n",
            " [ 0.11882887]] init_w\n",
            "[[-0.72853055]\n",
            " [ 1.25883415]\n",
            " [ 0.11882887]] forward_w\n",
            "[[-0.72853055]\n",
            " [ 1.25883415]\n",
            " [ 0.11882887]] loss_w\n",
            "0.011826122298151356 Loss 168\n",
            "[[-0.72853055]\n",
            " [ 1.25883415]\n",
            " [ 0.11882887]] grad_w\n",
            "[[-0.72930532]\n",
            " [ 1.25939787]\n",
            " [ 0.12054318]] init_w\n",
            "[[-0.72930532]\n",
            " [ 1.25939787]\n",
            " [ 0.12054318]] forward_w\n",
            "[[-0.72930532]\n",
            " [ 1.25939787]\n",
            " [ 0.12054318]] loss_w\n",
            "0.011443573853914757 Loss 169\n",
            "[[-0.72930532]\n",
            " [ 1.25939787]\n",
            " [ 0.12054318]] grad_w\n",
            "[[-0.73006746]\n",
            " [ 1.25995241]\n",
            " [ 0.12222954]] init_w\n",
            "[[-0.73006746]\n",
            " [ 1.25995241]\n",
            " [ 0.12222954]] forward_w\n",
            "[[-0.73006746]\n",
            " [ 1.25995241]\n",
            " [ 0.12222954]] loss_w\n",
            "0.011073399991007414 Loss 170\n",
            "[[-0.73006746]\n",
            " [ 1.25995241]\n",
            " [ 0.12222954]] grad_w\n",
            "[[-0.73081717]\n",
            " [ 1.2604979 ]\n",
            " [ 0.1238884 ]] init_w\n",
            "[[-0.73081717]\n",
            " [ 1.2604979 ]\n",
            " [ 0.1238884 ]] forward_w\n",
            "[[-0.73081717]\n",
            " [ 1.2604979 ]\n",
            " [ 0.1238884 ]] loss_w\n",
            "0.010715200419569573 Loss 171\n",
            "[[-0.73081717]\n",
            " [ 1.2604979 ]\n",
            " [ 0.1238884 ]] grad_w\n",
            "[[-0.73155465]\n",
            " [ 1.2610345 ]\n",
            " [ 0.12552021]] init_w\n",
            "[[-0.73155465]\n",
            " [ 1.2610345 ]\n",
            " [ 0.12552021]] forward_w\n",
            "[[-0.73155465]\n",
            " [ 1.2610345 ]\n",
            " [ 0.12552021]] loss_w\n",
            "0.010368587798217753 Loss 172\n",
            "[[-0.73155465]\n",
            " [ 1.2610345 ]\n",
            " [ 0.12552021]] grad_w\n",
            "[[-0.7322801 ]\n",
            " [ 1.26156234]\n",
            " [ 0.12712541]] init_w\n",
            "[[-0.7322801 ]\n",
            " [ 1.26156234]\n",
            " [ 0.12712541]] forward_w\n",
            "[[-0.7322801 ]\n",
            " [ 1.26156234]\n",
            " [ 0.12712541]] loss_w\n",
            "0.010033187315190567 Loss 173\n",
            "[[-0.7322801 ]\n",
            " [ 1.26156234]\n",
            " [ 0.12712541]] grad_w\n",
            "[[-0.73299373]\n",
            " [ 1.26208158]\n",
            " [ 0.12870444]] init_w\n",
            "[[-0.73299373]\n",
            " [ 1.26208158]\n",
            " [ 0.12870444]] forward_w\n",
            "[[-0.73299373]\n",
            " [ 1.26208158]\n",
            " [ 0.12870444]] loss_w\n",
            "0.00970863628304367 Loss 174\n",
            "[[-0.73299373]\n",
            " [ 1.26208158]\n",
            " [ 0.12870444]] grad_w\n",
            "[[-0.73369572]\n",
            " [ 1.26259235]\n",
            " [ 0.13025771]] init_w\n",
            "[[-0.73369572]\n",
            " [ 1.26259235]\n",
            " [ 0.13025771]] forward_w\n",
            "[[-0.73369572]\n",
            " [ 1.26259235]\n",
            " [ 0.13025771]] loss_w\n",
            "0.00939458374645543 Loss 175\n",
            "[[-0.73369572]\n",
            " [ 1.26259235]\n",
            " [ 0.13025771]] grad_w\n",
            "[[-0.73438626]\n",
            " [ 1.2630948 ]\n",
            " [ 0.13178566]] init_w\n",
            "[[-0.73438626]\n",
            " [ 1.2630948 ]\n",
            " [ 0.13178566]] forward_w\n",
            "[[-0.73438626]\n",
            " [ 1.2630948 ]\n",
            " [ 0.13178566]] loss_w\n",
            "0.009090690102719085 Loss 176\n",
            "[[-0.73438626]\n",
            " [ 1.2630948 ]\n",
            " [ 0.13178566]] grad_w\n",
            "[[-0.73506554]\n",
            " [ 1.26358905]\n",
            " [ 0.13328869]] init_w\n",
            "[[-0.73506554]\n",
            " [ 1.26358905]\n",
            " [ 0.13328869]] forward_w\n",
            "[[-0.73506554]\n",
            " [ 1.26358905]\n",
            " [ 0.13328869]] loss_w\n",
            "0.008796626734511257 Loss 177\n",
            "[[-0.73506554]\n",
            " [ 1.26358905]\n",
            " [ 0.13328869]] grad_w\n",
            "[[-0.73573375]\n",
            " [ 1.26407524]\n",
            " [ 0.13476721]] init_w\n",
            "[[-0.73573375]\n",
            " [ 1.26407524]\n",
            " [ 0.13476721]] forward_w\n",
            "[[-0.73573375]\n",
            " [ 1.26407524]\n",
            " [ 0.13476721]] loss_w\n",
            "0.008512075654539477 Loss 178\n",
            "[[-0.73573375]\n",
            " [ 1.26407524]\n",
            " [ 0.13476721]] grad_w\n",
            "[[-0.73639105]\n",
            " [ 1.2645535 ]\n",
            " [ 0.13622162]] init_w\n",
            "[[-0.73639105]\n",
            " [ 1.2645535 ]\n",
            " [ 0.13622162]] forward_w\n",
            "[[-0.73639105]\n",
            " [ 1.2645535 ]\n",
            " [ 0.13622162]] loss_w\n",
            "0.008236729161684666 Loss 179\n",
            "[[-0.73639105]\n",
            " [ 1.2645535 ]\n",
            " [ 0.13622162]] grad_w\n",
            "[[-0.73703764]\n",
            " [ 1.26502396]\n",
            " [ 0.13765232]] init_w\n",
            "[[-0.73703764]\n",
            " [ 1.26502396]\n",
            " [ 0.13765232]] forward_w\n",
            "[[-0.73703764]\n",
            " [ 1.26502396]\n",
            " [ 0.13765232]] loss_w\n",
            "0.007970289508266473 Loss 180\n",
            "[[-0.73703764]\n",
            " [ 1.26502396]\n",
            " [ 0.13765232]] grad_w\n",
            "[[-0.73767369]\n",
            " [ 1.26548675]\n",
            " [ 0.13905968]] init_w\n",
            "[[-0.73767369]\n",
            " [ 1.26548675]\n",
            " [ 0.13905968]] forward_w\n",
            "[[-0.73767369]\n",
            " [ 1.26548675]\n",
            " [ 0.13905968]] loss_w\n",
            "0.007712468578072037 Loss 181\n",
            "[[-0.73767369]\n",
            " [ 1.26548675]\n",
            " [ 0.13905968]] grad_w\n",
            "[[-0.73829936]\n",
            " [ 1.265942  ]\n",
            " [ 0.1404441 ]] init_w\n",
            "[[-0.73829936]\n",
            " [ 1.265942  ]\n",
            " [ 0.1404441 ]] forward_w\n",
            "[[-0.73829936]\n",
            " [ 1.265942  ]\n",
            " [ 0.1404441 ]] loss_w\n",
            "0.007462987574799633 Loss 182\n",
            "[[-0.73829936]\n",
            " [ 1.265942  ]\n",
            " [ 0.1404441 ]] grad_w\n",
            "[[-0.73891483]\n",
            " [ 1.26638982]\n",
            " [ 0.14180593]] init_w\n",
            "[[-0.73891483]\n",
            " [ 1.26638982]\n",
            " [ 0.14180593]] forward_w\n",
            "[[-0.73891483]\n",
            " [ 1.26638982]\n",
            " [ 0.14180593]] loss_w\n",
            "0.007221576720580514 Loss 183\n",
            "[[-0.73891483]\n",
            " [ 1.26638982]\n",
            " [ 0.14180593]] grad_w\n",
            "[[-0.73952027]\n",
            " [ 1.26683034]\n",
            " [ 0.14314557]] init_w\n",
            "[[-0.73952027]\n",
            " [ 1.26683034]\n",
            " [ 0.14314557]] forward_w\n",
            "[[-0.73952027]\n",
            " [ 1.26683034]\n",
            " [ 0.14314557]] loss_w\n",
            "0.0069879749642529085 Loss 184\n",
            "[[-0.73952027]\n",
            " [ 1.26683034]\n",
            " [ 0.14314557]] grad_w\n",
            "[[-0.74011583]\n",
            " [ 1.26726367]\n",
            " [ 0.14446335]] init_w\n",
            "[[-0.74011583]\n",
            " [ 1.26726367]\n",
            " [ 0.14446335]] forward_w\n",
            "[[-0.74011583]\n",
            " [ 1.26726367]\n",
            " [ 0.14446335]] loss_w\n",
            "0.006761929699072688 Loss 185\n",
            "[[-0.74011583]\n",
            " [ 1.26726367]\n",
            " [ 0.14446335]] grad_w\n",
            "[[-0.74070168]\n",
            " [ 1.26768994]\n",
            " [ 0.14575965]] init_w\n",
            "[[-0.74070168]\n",
            " [ 1.26768994]\n",
            " [ 0.14575965]] forward_w\n",
            "[[-0.74070168]\n",
            " [ 1.26768994]\n",
            " [ 0.14575965]] loss_w\n",
            "0.006543196489555518 Loss 186\n",
            "[[-0.74070168]\n",
            " [ 1.26768994]\n",
            " [ 0.14575965]] grad_w\n",
            "[[-0.74127798]\n",
            " [ 1.26810926]\n",
            " [ 0.14703481]] init_w\n",
            "[[-0.74127798]\n",
            " [ 1.26810926]\n",
            " [ 0.14703481]] forward_w\n",
            "[[-0.74127798]\n",
            " [ 1.26810926]\n",
            " [ 0.14703481]] loss_w\n",
            "0.0063315388071548346 Loss 187\n",
            "[[-0.74127798]\n",
            " [ 1.26810926]\n",
            " [ 0.14703481]] grad_w\n",
            "[[-0.74184488]\n",
            " [ 1.26852174]\n",
            " [ 0.14828917]] init_w\n",
            "[[-0.74184488]\n",
            " [ 1.26852174]\n",
            " [ 0.14828917]] forward_w\n",
            "[[-0.74184488]\n",
            " [ 1.26852174]\n",
            " [ 0.14828917]] loss_w\n",
            "0.006126727774490374 Loss 188\n",
            "[[-0.74184488]\n",
            " [ 1.26852174]\n",
            " [ 0.14828917]] grad_w\n",
            "[[-0.74240253]\n",
            " [ 1.26892749]\n",
            " [ 0.14952309]] init_w\n",
            "[[-0.74240253]\n",
            " [ 1.26892749]\n",
            " [ 0.14952309]] forward_w\n",
            "[[-0.74240253]\n",
            " [ 1.26892749]\n",
            " [ 0.14952309]] loss_w\n",
            "0.005928541917850045 Loss 189\n",
            "[[-0.74240253]\n",
            " [ 1.26892749]\n",
            " [ 0.14952309]] grad_w\n",
            "[[-0.74295109]\n",
            " [ 1.26932663]\n",
            " [ 0.15073688]] init_w\n",
            "[[-0.74295109]\n",
            " [ 1.26932663]\n",
            " [ 0.15073688]] forward_w\n",
            "[[-0.74295109]\n",
            " [ 1.26932663]\n",
            " [ 0.15073688]] loss_w\n",
            "0.005736766927698004 Loss 190\n",
            "[[-0.74295109]\n",
            " [ 1.26932663]\n",
            " [ 0.15073688]] grad_w\n",
            "[[-0.74349071]\n",
            " [ 1.26971926]\n",
            " [ 0.15193087]] init_w\n",
            "[[-0.74349071]\n",
            " [ 1.26971926]\n",
            " [ 0.15193087]] forward_w\n",
            "[[-0.74349071]\n",
            " [ 1.26971926]\n",
            " [ 0.15193087]] loss_w\n",
            "0.005551195426929594 Loss 191\n",
            "[[-0.74349071]\n",
            " [ 1.26971926]\n",
            " [ 0.15193087]] grad_w\n",
            "[[-0.74402152]\n",
            " [ 1.27010548]\n",
            " [ 0.1531054 ]] init_w\n",
            "[[-0.74402152]\n",
            " [ 1.27010548]\n",
            " [ 0.1531054 ]] forward_w\n",
            "[[-0.74402152]\n",
            " [ 1.27010548]\n",
            " [ 0.1531054 ]] loss_w\n",
            "0.005371626746622858 Loss 192\n",
            "[[-0.74402152]\n",
            " [ 1.27010548]\n",
            " [ 0.1531054 ]] grad_w\n",
            "[[-0.74454369]\n",
            " [ 1.27048541]\n",
            " [ 0.15426077]] init_w\n",
            "[[-0.74454369]\n",
            " [ 1.27048541]\n",
            " [ 0.15426077]] forward_w\n",
            "[[-0.74454369]\n",
            " [ 1.27048541]\n",
            " [ 0.15426077]] loss_w\n",
            "0.005197866709043885 Loss 193\n",
            "[[-0.74454369]\n",
            " [ 1.27048541]\n",
            " [ 0.15426077]] grad_w\n",
            "[[-0.74505733]\n",
            " [ 1.27085914]\n",
            " [ 0.1553973 ]] init_w\n",
            "[[-0.74505733]\n",
            " [ 1.27085914]\n",
            " [ 0.1553973 ]] forward_w\n",
            "[[-0.74505733]\n",
            " [ 1.27085914]\n",
            " [ 0.1553973 ]] loss_w\n",
            "0.005029727417671552 Loss 194\n",
            "[[-0.74505733]\n",
            " [ 1.27085914]\n",
            " [ 0.1553973 ]] grad_w\n",
            "[[-0.7455626 ]\n",
            " [ 1.27122678]\n",
            " [ 0.1565153 ]] init_w\n",
            "[[-0.7455626 ]\n",
            " [ 1.27122678]\n",
            " [ 0.1565153 ]] forward_w\n",
            "[[-0.7455626 ]\n",
            " [ 1.27122678]\n",
            " [ 0.1565153 ]] loss_w\n",
            "0.0048670270540143 Loss 195\n",
            "[[-0.7455626 ]\n",
            " [ 1.27122678]\n",
            " [ 0.1565153 ]] grad_w\n",
            "[[-0.74605963]\n",
            " [ 1.27158842]\n",
            " [ 0.15761507]] init_w\n",
            "[[-0.74605963]\n",
            " [ 1.27158842]\n",
            " [ 0.15761507]] forward_w\n",
            "[[-0.74605963]\n",
            " [ 1.27158842]\n",
            " [ 0.15761507]] loss_w\n",
            "0.00470958968099969 Loss 196\n",
            "[[-0.74605963]\n",
            " [ 1.27158842]\n",
            " [ 0.15761507]] grad_w\n",
            "[[-0.74654856]\n",
            " [ 1.27194417]\n",
            " [ 0.15869691]] init_w\n",
            "[[-0.74654856]\n",
            " [ 1.27194417]\n",
            " [ 0.15869691]] forward_w\n",
            "[[-0.74654856]\n",
            " [ 1.27194417]\n",
            " [ 0.15869691]] loss_w\n",
            "0.004557245052723632 Loss 197\n",
            "[[-0.74654856]\n",
            " [ 1.27194417]\n",
            " [ 0.15869691]] grad_w\n",
            "[[-0.74702951]\n",
            " [ 1.27229411]\n",
            " [ 0.1597611 ]] init_w\n",
            "[[-0.74702951]\n",
            " [ 1.27229411]\n",
            " [ 0.1597611 ]] forward_w\n",
            "[[-0.74702951]\n",
            " [ 1.27229411]\n",
            " [ 0.1597611 ]] loss_w\n",
            "0.004409828430353939 Loss 198\n",
            "[[-0.74702951]\n",
            " [ 1.27229411]\n",
            " [ 0.1597611 ]] grad_w\n",
            "[[-0.74750262]\n",
            " [ 1.27263835]\n",
            " [ 0.16080794]] init_w\n",
            "[[-0.74750262]\n",
            " [ 1.27263835]\n",
            " [ 0.16080794]] forward_w\n",
            "[[-0.74750262]\n",
            " [ 1.27263835]\n",
            " [ 0.16080794]] loss_w\n",
            "0.0042671804039889575 Loss 199\n",
            "[[-0.74750262]\n",
            " [ 1.27263835]\n",
            " [ 0.16080794]] grad_w\n",
            "[[-0.74796801]\n",
            " [ 1.27297698]\n",
            " [ 0.16183771]] init_w\n",
            "[[-0.74796801]\n",
            " [ 1.27297698]\n",
            " [ 0.16183771]] forward_w\n",
            "[[-0.74796801]\n",
            " [ 1.27297698]\n",
            " [ 0.16183771]] loss_w\n",
            "0.004129146720278622 Loss 200\n",
            "[[-0.74796801]\n",
            " [ 1.27297698]\n",
            " [ 0.16183771]] grad_w\n",
            "[[-0.74842582]\n",
            " [ 1.27331008]\n",
            " [ 0.16285069]] init_w\n",
            "[[-0.74842582]\n",
            " [ 1.27331008]\n",
            " [ 0.16285069]] forward_w\n",
            "[[-0.74842582]\n",
            " [ 1.27331008]\n",
            " [ 0.16285069]] loss_w\n",
            "0.003995578115621615 Loss 201\n",
            "[[-0.74842582]\n",
            " [ 1.27331008]\n",
            " [ 0.16285069]] grad_w\n",
            "[[-0.74887616]\n",
            " [ 1.27363775]\n",
            " [ 0.16384715]] init_w\n",
            "[[-0.74887616]\n",
            " [ 1.27363775]\n",
            " [ 0.16384715]] forward_w\n",
            "[[-0.74887616]\n",
            " [ 1.27363775]\n",
            " [ 0.16384715]] loss_w\n",
            "0.003866330154758263 Loss 202\n",
            "[[-0.74887616]\n",
            " [ 1.27363775]\n",
            " [ 0.16384715]] grad_w\n",
            "[[-0.74931916]\n",
            " [ 1.27396008]\n",
            " [ 0.16482735]] init_w\n",
            "[[-0.74931916]\n",
            " [ 1.27396008]\n",
            " [ 0.16482735]] forward_w\n",
            "[[-0.74931916]\n",
            " [ 1.27396008]\n",
            " [ 0.16482735]] loss_w\n",
            "0.003741263074584478 Loss 203\n",
            "[[-0.74931916]\n",
            " [ 1.27396008]\n",
            " [ 0.16482735]] grad_w\n",
            "[[-0.74975493]\n",
            " [ 1.27427715]\n",
            " [ 0.16579158]] init_w\n",
            "[[-0.74975493]\n",
            " [ 1.27427715]\n",
            " [ 0.16579158]] forward_w\n",
            "[[-0.74975493]\n",
            " [ 1.27427715]\n",
            " [ 0.16579158]] loss_w\n",
            "0.003620241633018142 Loss 204\n",
            "[[-0.74975493]\n",
            " [ 1.27427715]\n",
            " [ 0.16579158]] grad_w\n",
            "[[-0.7501836 ]\n",
            " [ 1.27458905]\n",
            " [ 0.16674008]] init_w\n",
            "[[-0.7501836 ]\n",
            " [ 1.27458905]\n",
            " [ 0.16674008]] forward_w\n",
            "[[-0.7501836 ]\n",
            " [ 1.27458905]\n",
            " [ 0.16674008]] loss_w\n",
            "0.003503134962754125 Loss 205\n",
            "[[-0.7501836 ]\n",
            " [ 1.27458905]\n",
            " [ 0.16674008]] grad_w\n",
            "[[-0.75060527]\n",
            " [ 1.27489586]\n",
            " [ 0.16767312]] init_w\n",
            "[[-0.75060527]\n",
            " [ 1.27489586]\n",
            " [ 0.16767312]] forward_w\n",
            "[[-0.75060527]\n",
            " [ 1.27489586]\n",
            " [ 0.16767312]] loss_w\n",
            "0.0033898164297501213 Loss 206\n",
            "[[-0.75060527]\n",
            " [ 1.27489586]\n",
            " [ 0.16767312]] grad_w\n",
            "[[-0.75102007]\n",
            " [ 1.27519768]\n",
            " [ 0.16859094]] init_w\n",
            "[[-0.75102007]\n",
            " [ 1.27519768]\n",
            " [ 0.16859094]] forward_w\n",
            "[[-0.75102007]\n",
            " [ 1.27519768]\n",
            " [ 0.16859094]] loss_w\n",
            "0.003280163496290168 Loss 207\n",
            "[[-0.75102007]\n",
            " [ 1.27519768]\n",
            " [ 0.16859094]] grad_w\n",
            "[[-0.75142811]\n",
            " [ 1.27549457]\n",
            " [ 0.16949379]] init_w\n",
            "[[-0.75142811]\n",
            " [ 1.27549457]\n",
            " [ 0.16949379]] forward_w\n",
            "[[-0.75142811]\n",
            " [ 1.27549457]\n",
            " [ 0.16949379]] loss_w\n",
            "0.0031740575884776287 Loss 208\n",
            "[[-0.75142811]\n",
            " [ 1.27549457]\n",
            " [ 0.16949379]] grad_w\n",
            "[[-0.75182949]\n",
            " [ 1.27578661]\n",
            " [ 0.17038192]] init_w\n",
            "[[-0.75182949]\n",
            " [ 1.27578661]\n",
            " [ 0.17038192]] forward_w\n",
            "[[-0.75182949]\n",
            " [ 1.27578661]\n",
            " [ 0.17038192]] loss_w\n",
            "0.003071383968014628 Loss 209\n",
            "[[-0.75182949]\n",
            " [ 1.27578661]\n",
            " [ 0.17038192]] grad_w\n",
            "[[-0.75222433]\n",
            " [ 1.2760739 ]\n",
            " [ 0.17125557]] init_w\n",
            "[[-0.75222433]\n",
            " [ 1.2760739 ]\n",
            " [ 0.17125557]] forward_w\n",
            "[[-0.75222433]\n",
            " [ 1.2760739 ]\n",
            " [ 0.17125557]] loss_w\n",
            "0.0029720316081290115 Loss 210\n",
            "[[-0.75222433]\n",
            " [ 1.2760739 ]\n",
            " [ 0.17125557]] grad_w\n",
            "[[-0.75261273]\n",
            " [ 1.2763565 ]\n",
            " [ 0.17211497]] init_w\n",
            "[[-0.75261273]\n",
            " [ 1.2763565 ]\n",
            " [ 0.17211497]] forward_w\n",
            "[[-0.75261273]\n",
            " [ 1.2763565 ]\n",
            " [ 0.17211497]] loss_w\n",
            "0.0028758930735148834 Loss 211\n",
            "[[-0.75261273]\n",
            " [ 1.2763565 ]\n",
            " [ 0.17211497]] grad_w\n",
            "[[-0.7529948 ]\n",
            " [ 1.2766345 ]\n",
            " [ 0.17296036]] init_w\n",
            "[[-0.7529948 ]\n",
            " [ 1.2766345 ]\n",
            " [ 0.17296036]] forward_w\n",
            "[[-0.7529948 ]\n",
            " [ 1.2766345 ]\n",
            " [ 0.17296036]] loss_w\n",
            "0.002782864404156731 Loss 212\n",
            "[[-0.7529948 ]\n",
            " [ 1.2766345 ]\n",
            " [ 0.17296036]] grad_w\n",
            "[[-0.75337063]\n",
            " [ 1.27690796]\n",
            " [ 0.17379196]] init_w\n",
            "[[-0.75337063]\n",
            " [ 1.27690796]\n",
            " [ 0.17379196]] forward_w\n",
            "[[-0.75337063]\n",
            " [ 1.27690796]\n",
            " [ 0.17379196]] loss_w\n",
            "0.0026928450029115817 Loss 213\n",
            "[[-0.75337063]\n",
            " [ 1.27690796]\n",
            " [ 0.17379196]] grad_w\n",
            "[[-0.75374034]\n",
            " [ 1.27717696]\n",
            " [ 0.17461   ]] init_w\n",
            "[[-0.75374034]\n",
            " [ 1.27717696]\n",
            " [ 0.17461   ]] forward_w\n",
            "[[-0.75374034]\n",
            " [ 1.27717696]\n",
            " [ 0.17461   ]] loss_w\n",
            "0.0026057375267276826 Loss 214\n",
            "[[-0.75374034]\n",
            " [ 1.27717696]\n",
            " [ 0.17461   ]] grad_w\n",
            "[[-0.75410401]\n",
            " [ 1.27744157]\n",
            " [ 0.1754147 ]] init_w\n",
            "[[-0.75410401]\n",
            " [ 1.27744157]\n",
            " [ 0.1754147 ]] forward_w\n",
            "[[-0.75410401]\n",
            " [ 1.27744157]\n",
            " [ 0.1754147 ]] loss_w\n",
            "0.0025214477813819644 Loss 215\n",
            "[[-0.75410401]\n",
            " [ 1.27744157]\n",
            " [ 0.1754147 ]] grad_w\n",
            "[[-0.75446176]\n",
            " [ 1.27770187]\n",
            " [ 0.17620628]] init_w\n",
            "[[-0.75446176]\n",
            " [ 1.27770187]\n",
            " [ 0.17620628]] forward_w\n",
            "[[-0.75446176]\n",
            " [ 1.27770187]\n",
            " [ 0.17620628]] loss_w\n",
            "0.0024398846196224846 Loss 216\n",
            "[[-0.75446176]\n",
            " [ 1.27770187]\n",
            " [ 0.17620628]] grad_w\n",
            "[[-0.75481367]\n",
            " [ 1.27795792]\n",
            " [ 0.17698495]] init_w\n",
            "[[-0.75481367]\n",
            " [ 1.27795792]\n",
            " [ 0.17698495]] forward_w\n",
            "[[-0.75481367]\n",
            " [ 1.27795792]\n",
            " [ 0.17698495]] loss_w\n",
            "0.00236095984260583 Loss 217\n",
            "[[-0.75481367]\n",
            " [ 1.27795792]\n",
            " [ 0.17698495]] grad_w\n",
            "[[-0.75515985]\n",
            " [ 1.2782098 ]\n",
            " [ 0.17775093]] init_w\n",
            "[[-0.75515985]\n",
            " [ 1.2782098 ]\n",
            " [ 0.17775093]] forward_w\n",
            "[[-0.75515985]\n",
            " [ 1.2782098 ]\n",
            " [ 0.17775093]] loss_w\n",
            "0.00228458810452266 Loss 218\n",
            "[[-0.75515985]\n",
            " [ 1.2782098 ]\n",
            " [ 0.17775093]] grad_w\n",
            "[[-0.75550038]\n",
            " [ 1.27845758]\n",
            " [ 0.17850441]] init_w\n",
            "[[-0.75550038]\n",
            " [ 1.27845758]\n",
            " [ 0.17850441]] forward_w\n",
            "[[-0.75550038]\n",
            " [ 1.27845758]\n",
            " [ 0.17850441]] loss_w\n",
            "0.002210686820308553 Loss 219\n",
            "[[-0.75550038]\n",
            " [ 1.27845758]\n",
            " [ 0.17850441]] grad_w\n",
            "[[-0.75583536]\n",
            " [ 1.27870131]\n",
            " [ 0.1792456 ]] init_w\n",
            "[[-0.75583536]\n",
            " [ 1.27870131]\n",
            " [ 0.1792456 ]] forward_w\n",
            "[[-0.75583536]\n",
            " [ 1.27870131]\n",
            " [ 0.1792456 ]] loss_w\n",
            "0.002139176076340041 Loss 220\n",
            "[[-0.75583536]\n",
            " [ 1.27870131]\n",
            " [ 0.1792456 ]] grad_w\n",
            "[[-0.75616487]\n",
            " [ 1.27894106]\n",
            " [ 0.17997471]] init_w\n",
            "[[-0.75616487]\n",
            " [ 1.27894106]\n",
            " [ 0.17997471]] forward_w\n",
            "[[-0.75616487]\n",
            " [ 1.27894106]\n",
            " [ 0.17997471]] loss_w\n",
            "0.002069978544019587 Loss 221\n",
            "[[-0.75616487]\n",
            " [ 1.27894106]\n",
            " [ 0.17997471]] grad_w\n",
            "[[-0.75648901]\n",
            " [ 1.27917691]\n",
            " [ 0.18069193]] init_w\n",
            "[[-0.75648901]\n",
            " [ 1.27917691]\n",
            " [ 0.18069193]] forward_w\n",
            "[[-0.75648901]\n",
            " [ 1.27917691]\n",
            " [ 0.18069193]] loss_w\n",
            "0.002003019396155745 Loss 222\n",
            "[[-0.75648901]\n",
            " [ 1.27917691]\n",
            " [ 0.18069193]] grad_w\n",
            "[[-0.75680787]\n",
            " [ 1.27940891]\n",
            " [ 0.18139746]] init_w\n",
            "[[-0.75680787]\n",
            " [ 1.27940891]\n",
            " [ 0.18139746]] forward_w\n",
            "[[-0.75680787]\n",
            " [ 1.27940891]\n",
            " [ 0.18139746]] loss_w\n",
            "0.001938226226048347 Loss 223\n",
            "[[-0.75680787]\n",
            " [ 1.27940891]\n",
            " [ 0.18139746]] grad_w\n",
            "[[-0.75712152]\n",
            " [ 1.27963713]\n",
            " [ 0.18209148]] init_w\n",
            "[[-0.75712152]\n",
            " [ 1.27963713]\n",
            " [ 0.18209148]] forward_w\n",
            "[[-0.75712152]\n",
            " [ 1.27963713]\n",
            " [ 0.18209148]] loss_w\n",
            "0.0018755289691910143 Loss 224\n",
            "[[-0.75712152]\n",
            " [ 1.27963713]\n",
            " [ 0.18209148]] grad_w\n",
            "[[-0.75743006]\n",
            " [ 1.27986163]\n",
            " [ 0.18277418]] init_w\n",
            "[[-0.75743006]\n",
            " [ 1.27986163]\n",
            " [ 0.18277418]] forward_w\n",
            "[[-0.75743006]\n",
            " [ 1.27986163]\n",
            " [ 0.18277418]] loss_w\n",
            "0.001814859827506526 Loss 225\n",
            "[[-0.75743006]\n",
            " [ 1.27986163]\n",
            " [ 0.18277418]] grad_w\n",
            "[[-0.75773357]\n",
            " [ 1.28008246]\n",
            " [ 0.18344575]] init_w\n",
            "[[-0.75773357]\n",
            " [ 1.28008246]\n",
            " [ 0.18344575]] forward_w\n",
            "[[-0.75773357]\n",
            " [ 1.28008246]\n",
            " [ 0.18344575]] loss_w\n",
            "0.0017561531960328626 Loss 226\n",
            "[[-0.75773357]\n",
            " [ 1.28008246]\n",
            " [ 0.18344575]] grad_w\n",
            "[[-0.75803213]\n",
            " [ 1.2802997 ]\n",
            " [ 0.18410637]] init_w\n",
            "[[-0.75803213]\n",
            " [ 1.2802997 ]\n",
            " [ 0.18410637]] forward_w\n",
            "[[-0.75803213]\n",
            " [ 1.2802997 ]\n",
            " [ 0.18410637]] loss_w\n",
            "0.0016993455919808935 Loss 227\n",
            "[[-0.75803213]\n",
            " [ 1.2802997 ]\n",
            " [ 0.18410637]] grad_w\n",
            "[[-0.75832583]\n",
            " [ 1.28051339]\n",
            " [ 0.18475621]] init_w\n",
            "[[-0.75832583]\n",
            " [ 1.28051339]\n",
            " [ 0.18475621]] forward_w\n",
            "[[-0.75832583]\n",
            " [ 1.28051339]\n",
            " [ 0.18475621]] loss_w\n",
            "0.0016443755860868971 Loss 228\n",
            "[[-0.75832583]\n",
            " [ 1.28051339]\n",
            " [ 0.18475621]] grad_w\n",
            "[[-0.75861473]\n",
            " [ 1.2807236 ]\n",
            " [ 0.18539546]] init_w\n",
            "[[-0.75861473]\n",
            " [ 1.2807236 ]\n",
            " [ 0.18539546]] forward_w\n",
            "[[-0.75861473]\n",
            " [ 1.2807236 ]\n",
            " [ 0.18539546]] loss_w\n",
            "0.0015911837361855614 Loss 229\n",
            "[[-0.75861473]\n",
            " [ 1.2807236 ]\n",
            " [ 0.18539546]] grad_w\n",
            "[[-0.75889892]\n",
            " [ 1.28093038]\n",
            " [ 0.18602429]] init_w\n",
            "[[-0.75889892]\n",
            " [ 1.28093038]\n",
            " [ 0.18602429]] forward_w\n",
            "[[-0.75889892]\n",
            " [ 1.28093038]\n",
            " [ 0.18602429]] loss_w\n",
            "0.0015397125229318768 Loss 230\n",
            "[[-0.75889892]\n",
            " [ 1.28093038]\n",
            " [ 0.18602429]] grad_w\n",
            "[[-0.75917848]\n",
            " [ 1.28113379]\n",
            " [ 0.18664286]] init_w\n",
            "[[-0.75917848]\n",
            " [ 1.28113379]\n",
            " [ 0.18664286]] forward_w\n",
            "[[-0.75917848]\n",
            " [ 1.28113379]\n",
            " [ 0.18664286]] loss_w\n",
            "0.0014899062876022086 Loss 231\n",
            "[[-0.75917848]\n",
            " [ 1.28113379]\n",
            " [ 0.18664286]] grad_w\n",
            "[[-0.75945348]\n",
            " [ 1.28133388]\n",
            " [ 0.18725134]] init_w\n",
            "[[-0.75945348]\n",
            " [ 1.28133388]\n",
            " [ 0.18725134]] forward_w\n",
            "[[-0.75945348]\n",
            " [ 1.28133388]\n",
            " [ 0.18725134]] loss_w\n",
            "0.001441711171907379 Loss 232\n",
            "[[-0.75945348]\n",
            " [ 1.28133388]\n",
            " [ 0.18725134]] grad_w\n",
            "[[-0.75972399]\n",
            " [ 1.2815307 ]\n",
            " [ 0.1878499 ]] init_w\n",
            "[[-0.75972399]\n",
            " [ 1.2815307 ]\n",
            " [ 0.1878499 ]] forward_w\n",
            "[[-0.75972399]\n",
            " [ 1.2815307 ]\n",
            " [ 0.1878499 ]] loss_w\n",
            "0.0013950750597526971 Loss 233\n",
            "[[-0.75972399]\n",
            " [ 1.2815307 ]\n",
            " [ 0.1878499 ]] grad_w\n",
            "[[-0.75999009]\n",
            " [ 1.28172432]\n",
            " [ 0.1884387 ]] init_w\n",
            "[[-0.75999009]\n",
            " [ 1.28172432]\n",
            " [ 0.1884387 ]] forward_w\n",
            "[[-0.75999009]\n",
            " [ 1.28172432]\n",
            " [ 0.1884387 ]] loss_w\n",
            "0.001349947520881813 Loss 234\n",
            "[[-0.75999009]\n",
            " [ 1.28172432]\n",
            " [ 0.1884387 ]] grad_w\n",
            "[[-0.76025186]\n",
            " [ 1.28191478]\n",
            " [ 0.1890179 ]] init_w\n",
            "[[-0.76025186]\n",
            " [ 1.28191478]\n",
            " [ 0.1890179 ]] forward_w\n",
            "[[-0.76025186]\n",
            " [ 1.28191478]\n",
            " [ 0.1890179 ]] loss_w\n",
            "0.0013062797563437223 Loss 235\n",
            "[[-0.76025186]\n",
            " [ 1.28191478]\n",
            " [ 0.1890179 ]] grad_w\n",
            "[[-0.76050935]\n",
            " [ 1.28210214]\n",
            " [ 0.18958766]] init_w\n",
            "[[-0.76050935]\n",
            " [ 1.28210214]\n",
            " [ 0.18958766]] forward_w\n",
            "[[-0.76050935]\n",
            " [ 1.28210214]\n",
            " [ 0.18958766]] loss_w\n",
            "0.0012640245457236554 Loss 236\n",
            "[[-0.76050935]\n",
            " [ 1.28210214]\n",
            " [ 0.18958766]] grad_w\n",
            "[[-0.76076265]\n",
            " [ 1.28228644]\n",
            " [ 0.19014812]] init_w\n",
            "[[-0.76076265]\n",
            " [ 1.28228644]\n",
            " [ 0.19014812]] forward_w\n",
            "[[-0.76076265]\n",
            " [ 1.28228644]\n",
            " [ 0.19014812]] loss_w\n",
            "0.0012231361960810183 Loss 237\n",
            "[[-0.76076265]\n",
            " [ 1.28228644]\n",
            " [ 0.19014812]] grad_w\n",
            "[[-0.76101182]\n",
            " [ 1.28246773]\n",
            " [ 0.19069944]] init_w\n",
            "[[-0.76101182]\n",
            " [ 1.28246773]\n",
            " [ 0.19069944]] forward_w\n",
            "[[-0.76101182]\n",
            " [ 1.28246773]\n",
            " [ 0.19069944]] loss_w\n",
            "0.0011835704925390023 Loss 238\n",
            "[[-0.76101182]\n",
            " [ 1.28246773]\n",
            " [ 0.19069944]] grad_w\n",
            "[[-0.76125692]\n",
            " [ 1.28264607]\n",
            " [ 0.19124178]] init_w\n",
            "[[-0.76125692]\n",
            " [ 1.28264607]\n",
            " [ 0.19124178]] forward_w\n",
            "[[-0.76125692]\n",
            " [ 1.28264607]\n",
            " [ 0.19124178]] loss_w\n",
            "0.001145284650472593 Loss 239\n",
            "[[-0.76125692]\n",
            " [ 1.28264607]\n",
            " [ 0.19124178]] grad_w\n",
            "[[-0.76149802]\n",
            " [ 1.2828215 ]\n",
            " [ 0.19177527]] init_w\n",
            "[[-0.76149802]\n",
            " [ 1.2828215 ]\n",
            " [ 0.19177527]] forward_w\n",
            "[[-0.76149802]\n",
            " [ 1.2828215 ]\n",
            " [ 0.19177527]] loss_w\n",
            "0.0011082372692430916 Loss 240\n",
            "[[-0.76149802]\n",
            " [ 1.2828215 ]\n",
            " [ 0.19177527]] grad_w\n",
            "[[-0.7617352 ]\n",
            " [ 1.28299407]\n",
            " [ 0.19230006]] init_w\n",
            "[[-0.7617352 ]\n",
            " [ 1.28299407]\n",
            " [ 0.19230006]] forward_w\n",
            "[[-0.7617352 ]\n",
            " [ 1.28299407]\n",
            " [ 0.19230006]] loss_w\n",
            "0.0010723882874293118 Loss 241\n",
            "[[-0.7617352 ]\n",
            " [ 1.28299407]\n",
            " [ 0.19230006]] grad_w\n",
            "[[-0.76196851]\n",
            " [ 1.28316383]\n",
            " [ 0.19281629]] init_w\n",
            "[[-0.76196851]\n",
            " [ 1.28316383]\n",
            " [ 0.19281629]] forward_w\n",
            "[[-0.76196851]\n",
            " [ 1.28316383]\n",
            " [ 0.19281629]] loss_w\n",
            "0.0010376989395068995 Loss 242\n",
            "[[-0.76196851]\n",
            " [ 1.28316383]\n",
            " [ 0.19281629]] grad_w\n",
            "[[-0.76219801]\n",
            " [ 1.28333082]\n",
            " [ 0.1933241 ]] init_w\n",
            "[[-0.76219801]\n",
            " [ 1.28333082]\n",
            " [ 0.1933241 ]] forward_w\n",
            "[[-0.76219801]\n",
            " [ 1.28333082]\n",
            " [ 0.1933241 ]] loss_w\n",
            "0.001004131713928961 Loss 243\n",
            "[[-0.76219801]\n",
            " [ 1.28333082]\n",
            " [ 0.1933241 ]] grad_w\n",
            "[[-0.76242377]\n",
            " [ 1.28349508]\n",
            " [ 0.19382364]] init_w\n",
            "[[-0.76242377]\n",
            " [ 1.28349508]\n",
            " [ 0.19382364]] forward_w\n",
            "[[-0.76242377]\n",
            " [ 1.28349508]\n",
            " [ 0.19382364]] loss_w\n",
            "0.0009716503125627537 Loss 244\n",
            "[[-0.76242377]\n",
            " [ 1.28349508]\n",
            " [ 0.19382364]] grad_w\n",
            "[[-0.76264585]\n",
            " [ 1.28365667]\n",
            " [ 0.19431503]] init_w\n",
            "[[-0.76264585]\n",
            " [ 1.28365667]\n",
            " [ 0.19431503]] forward_w\n",
            "[[-0.76264585]\n",
            " [ 1.28365667]\n",
            " [ 0.19431503]] loss_w\n",
            "0.0009402196114384341 Loss 245\n",
            "[[-0.76264585]\n",
            " [ 1.28365667]\n",
            " [ 0.19431503]] grad_w\n",
            "[[-0.7628643 ]\n",
            " [ 1.28381562]\n",
            " [ 0.1947984 ]] init_w\n",
            "[[-0.7628643 ]\n",
            " [ 1.28381562]\n",
            " [ 0.1947984 ]] forward_w\n",
            "[[-0.7628643 ]\n",
            " [ 1.28381562]\n",
            " [ 0.1947984 ]] loss_w\n",
            "0.0009098056227675455 Loss 246\n",
            "[[-0.7628643 ]\n",
            " [ 1.28381562]\n",
            " [ 0.1947984 ]] grad_w\n",
            "[[-0.7630792 ]\n",
            " [ 1.28397198]\n",
            " [ 0.19527389]] init_w\n",
            "[[-0.7630792 ]\n",
            " [ 1.28397198]\n",
            " [ 0.19527389]] forward_w\n",
            "[[-0.7630792 ]\n",
            " [ 1.28397198]\n",
            " [ 0.19527389]] loss_w\n",
            "0.0008803754581901147 Loss 247\n",
            "[[-0.7630792 ]\n",
            " [ 1.28397198]\n",
            " [ 0.19527389]] grad_w\n",
            "[[-0.76329059]\n",
            " [ 1.28412578]\n",
            " [ 0.19574163]] init_w\n",
            "[[-0.76329059]\n",
            " [ 1.28412578]\n",
            " [ 0.19574163]] forward_w\n",
            "[[-0.76329059]\n",
            " [ 1.28412578]\n",
            " [ 0.19574163]] loss_w\n",
            "0.0008518972932105977 Loss 248\n",
            "[[-0.76329059]\n",
            " [ 1.28412578]\n",
            " [ 0.19574163]] grad_w\n",
            "[[-0.76349853]\n",
            " [ 1.28427709]\n",
            " [ 0.19620174]] init_w\n",
            "[[-0.76349853]\n",
            " [ 1.28427709]\n",
            " [ 0.19620174]] forward_w\n",
            "[[-0.76349853]\n",
            " [ 1.28427709]\n",
            " [ 0.19620174]] loss_w\n",
            "0.0008243403327842689 Loss 249\n",
            "[[-0.76349853]\n",
            " [ 1.28427709]\n",
            " [ 0.19620174]] grad_w\n",
            "[[-0.76370308]\n",
            " [ 1.28442592]\n",
            " [ 0.19665435]] init_w\n",
            "[[-0.76370308]\n",
            " [ 1.28442592]\n",
            " [ 0.19665435]] forward_w\n",
            "[[-0.76370308]\n",
            " [ 1.28442592]\n",
            " [ 0.19665435]] loss_w\n",
            "0.000797674778016805 Loss 250\n",
            "[[-0.76370308]\n",
            " [ 1.28442592]\n",
            " [ 0.19665435]] grad_w\n",
            "[[-0.7639043 ]\n",
            " [ 1.28457233]\n",
            " [ 0.19709958]] init_w\n",
            "[[-0.7639043 ]\n",
            " [ 1.28457233]\n",
            " [ 0.19709958]] forward_w\n",
            "[[-0.7639043 ]\n",
            " [ 1.28457233]\n",
            " [ 0.19709958]] loss_w\n",
            "0.0007718717939410535 Loss 251\n",
            "[[-0.7639043 ]\n",
            " [ 1.28457233]\n",
            " [ 0.19709958]] grad_w\n",
            "[[-0.76410224]\n",
            " [ 1.28471635]\n",
            " [ 0.19753755]] init_w\n",
            "[[-0.76410224]\n",
            " [ 1.28471635]\n",
            " [ 0.19753755]] forward_w\n",
            "[[-0.76410224]\n",
            " [ 1.28471635]\n",
            " [ 0.19753755]] loss_w\n",
            "0.0007469034783361633 Loss 252\n",
            "[[-0.76410224]\n",
            " [ 1.28471635]\n",
            " [ 0.19753755]] grad_w\n",
            "[[-0.76429694]\n",
            " [ 1.28485802]\n",
            " [ 0.19796837]] init_w\n",
            "[[-0.76429694]\n",
            " [ 1.28485802]\n",
            " [ 0.19796837]] forward_w\n",
            "[[-0.76429694]\n",
            " [ 1.28485802]\n",
            " [ 0.19796837]] loss_w\n",
            "0.0007227428315553437 Loss 253\n",
            "[[-0.76429694]\n",
            " [ 1.28485802]\n",
            " [ 0.19796837]] grad_w\n",
            "[[-0.76448848]\n",
            " [ 1.28499738]\n",
            " [ 0.19839217]] init_w\n",
            "[[-0.76448848]\n",
            " [ 1.28499738]\n",
            " [ 0.19839217]] forward_w\n",
            "[[-0.76448848]\n",
            " [ 1.28499738]\n",
            " [ 0.19839217]] loss_w\n",
            "0.0006993637273296183 Loss 254\n",
            "[[-0.76448848]\n",
            " [ 1.28499738]\n",
            " [ 0.19839217]] grad_w\n",
            "[[-0.76467689]\n",
            " [ 1.28513446]\n",
            " [ 0.19880906]] init_w\n",
            "[[-0.76467689]\n",
            " [ 1.28513446]\n",
            " [ 0.19880906]] forward_w\n",
            "[[-0.76467689]\n",
            " [ 1.28513446]\n",
            " [ 0.19880906]] loss_w\n",
            "0.0006767408845160237 Loss 255\n",
            "[[-0.76467689]\n",
            " [ 1.28513446]\n",
            " [ 0.19880906]] grad_w\n",
            "[[-0.76486222]\n",
            " [ 1.28526932]\n",
            " [ 0.19921915]] init_w\n",
            "[[-0.76486222]\n",
            " [ 1.28526932]\n",
            " [ 0.19921915]] forward_w\n",
            "[[-0.76486222]\n",
            " [ 1.28526932]\n",
            " [ 0.19921915]] loss_w\n",
            "0.0006548498397596738 Loss 256\n",
            "[[-0.76486222]\n",
            " [ 1.28526932]\n",
            " [ 0.19921915]] grad_w\n",
            "[[-0.76504454]\n",
            " [ 1.28540197]\n",
            " [ 0.19962256]] init_w\n",
            "[[-0.76504454]\n",
            " [ 1.28540197]\n",
            " [ 0.19962256]] forward_w\n",
            "[[-0.76504454]\n",
            " [ 1.28540197]\n",
            " [ 0.19962256]] loss_w\n",
            "0.0006336669210401708 Loss 257\n",
            "[[-0.76504454]\n",
            " [ 1.28540197]\n",
            " [ 0.19962256]] grad_w\n",
            "[[-0.76522388]\n",
            " [ 1.28553246]\n",
            " [ 0.20001938]] init_w\n",
            "[[-0.76522388]\n",
            " [ 1.28553246]\n",
            " [ 0.20001938]] forward_w\n",
            "[[-0.76522388]\n",
            " [ 1.28553246]\n",
            " [ 0.20001938]] loss_w\n",
            "0.0006131692220737063 Loss 258\n",
            "[[-0.76522388]\n",
            " [ 1.28553246]\n",
            " [ 0.20001938]] grad_w\n",
            "[[-0.7654003 ]\n",
            " [ 1.28566082]\n",
            " [ 0.20040974]] init_w\n",
            "[[-0.7654003 ]\n",
            " [ 1.28566082]\n",
            " [ 0.20040974]] forward_w\n",
            "[[-0.7654003 ]\n",
            " [ 1.28566082]\n",
            " [ 0.20040974]] loss_w\n",
            "0.000593334577543206 Loss 259\n",
            "[[-0.7654003 ]\n",
            " [ 1.28566082]\n",
            " [ 0.20040974]] grad_w\n",
            "[[-0.76557384]\n",
            " [ 1.28578709]\n",
            " [ 0.20079373]] init_w\n",
            "[[-0.76557384]\n",
            " [ 1.28578709]\n",
            " [ 0.20079373]] forward_w\n",
            "[[-0.76557384]\n",
            " [ 1.28578709]\n",
            " [ 0.20079373]] loss_w\n",
            "0.0005741415391297267 Loss 260\n",
            "[[-0.76557384]\n",
            " [ 1.28578709]\n",
            " [ 0.20079373]] grad_w\n",
            "[[-0.76574455]\n",
            " [ 1.2859113 ]\n",
            " [ 0.20117146]] init_w\n",
            "[[-0.76574455]\n",
            " [ 1.2859113 ]\n",
            " [ 0.20117146]] forward_w\n",
            "[[-0.76574455]\n",
            " [ 1.2859113 ]\n",
            " [ 0.20117146]] loss_w\n",
            "0.0005555693523191792 Loss 261\n",
            "[[-0.76574455]\n",
            " [ 1.2859113 ]\n",
            " [ 0.20117146]] grad_w\n",
            "[[-0.76591247]\n",
            " [ 1.28603349]\n",
            " [ 0.20154302]] init_w\n",
            "[[-0.76591247]\n",
            " [ 1.28603349]\n",
            " [ 0.20154302]] forward_w\n",
            "[[-0.76591247]\n",
            " [ 1.28603349]\n",
            " [ 0.20154302]] loss_w\n",
            "0.0005375979339592942 Loss 262\n",
            "[[-0.76591247]\n",
            " [ 1.28603349]\n",
            " [ 0.20154302]] grad_w\n",
            "[[-0.76607766]\n",
            " [ 1.28615368]\n",
            " [ 0.20190853]] init_w\n",
            "[[-0.76607766]\n",
            " [ 1.28615368]\n",
            " [ 0.20190853]] forward_w\n",
            "[[-0.76607766]\n",
            " [ 1.28615368]\n",
            " [ 0.20190853]] loss_w\n",
            "0.000520207850542597 Loss 263\n",
            "[[-0.76607766]\n",
            " [ 1.28615368]\n",
            " [ 0.20190853]] grad_w\n",
            "[[-0.76624016]\n",
            " [ 1.28627191]\n",
            " [ 0.20226808]] init_w\n",
            "[[-0.76624016]\n",
            " [ 1.28627191]\n",
            " [ 0.20226808]] forward_w\n",
            "[[-0.76624016]\n",
            " [ 1.28627191]\n",
            " [ 0.20226808]] loss_w\n",
            "0.0005033802971918348 Loss 264\n",
            "[[-0.76624016]\n",
            " [ 1.28627191]\n",
            " [ 0.20226808]] grad_w\n",
            "[[-0.7664    ]\n",
            " [ 1.28638822]\n",
            " [ 0.20262177]] init_w\n",
            "[[-0.7664    ]\n",
            " [ 1.28638822]\n",
            " [ 0.20262177]] forward_w\n",
            "[[-0.7664    ]\n",
            " [ 1.28638822]\n",
            " [ 0.20262177]] loss_w\n",
            "0.0004870970773252389 Loss 265\n",
            "[[-0.7664    ]\n",
            " [ 1.28638822]\n",
            " [ 0.20262177]] grad_w\n",
            "[[-0.76655724]\n",
            " [ 1.28650262]\n",
            " [ 0.20296969]] init_w\n",
            "[[-0.76655724]\n",
            " [ 1.28650262]\n",
            " [ 0.20296969]] forward_w\n",
            "[[-0.76655724]\n",
            " [ 1.28650262]\n",
            " [ 0.20296969]] loss_w\n",
            "0.00047134058297948966 Loss 266\n",
            "[[-0.76655724]\n",
            " [ 1.28650262]\n",
            " [ 0.20296969]] grad_w\n",
            "[[-0.76671192]\n",
            " [ 1.28661517]\n",
            " [ 0.20331193]] init_w\n",
            "[[-0.76671192]\n",
            " [ 1.28661517]\n",
            " [ 0.20331193]] forward_w\n",
            "[[-0.76671192]\n",
            " [ 1.28661517]\n",
            " [ 0.20331193]] loss_w\n",
            "0.00045609377576926857 Loss 267\n",
            "[[-0.76671192]\n",
            " [ 1.28661517]\n",
            " [ 0.20331193]] grad_w\n",
            "[[-0.76686407]\n",
            " [ 1.28672587]\n",
            " [ 0.20364859]] init_w\n",
            "[[-0.76686407]\n",
            " [ 1.28672587]\n",
            " [ 0.20364859]] forward_w\n",
            "[[-0.76686407]\n",
            " [ 1.28672587]\n",
            " [ 0.20364859]] loss_w\n",
            "0.00044134016846268946 Loss 268\n",
            "[[-0.76686407]\n",
            " [ 1.28672587]\n",
            " [ 0.20364859]] grad_w\n",
            "[[-0.76701374]\n",
            " [ 1.28683478]\n",
            " [ 0.20397977]] init_w\n",
            "[[-0.76701374]\n",
            " [ 1.28683478]\n",
            " [ 0.20397977]] forward_w\n",
            "[[-0.76701374]\n",
            " [ 1.28683478]\n",
            " [ 0.20397977]] loss_w\n",
            "0.00042706380715269954 Loss 269\n",
            "[[-0.76701374]\n",
            " [ 1.28683478]\n",
            " [ 0.20397977]] grad_w\n",
            "[[-0.76716097]\n",
            " [ 1.2869419 ]\n",
            " [ 0.20430554]] init_w\n",
            "[[-0.76716097]\n",
            " [ 1.2869419 ]\n",
            " [ 0.20430554]] forward_w\n",
            "[[-0.76716097]\n",
            " [ 1.2869419 ]\n",
            " [ 0.20430554]] loss_w\n",
            "0.00041324925400525495 Loss 270\n",
            "[[-0.76716097]\n",
            " [ 1.2869419 ]\n",
            " [ 0.20430554]] grad_w\n",
            "[[-0.7673058 ]\n",
            " [ 1.28704728]\n",
            " [ 0.204626  ]] init_w\n",
            "[[-0.7673058 ]\n",
            " [ 1.28704728]\n",
            " [ 0.204626  ]] forward_w\n",
            "[[-0.7673058 ]\n",
            " [ 1.28704728]\n",
            " [ 0.204626  ]] loss_w\n",
            "0.0003998815705654882 Loss 271\n",
            "[[-0.7673058 ]\n",
            " [ 1.28704728]\n",
            " [ 0.204626  ]] grad_w\n",
            "[[-0.76744827]\n",
            " [ 1.28715094]\n",
            " [ 0.20494124]] init_w\n",
            "[[-0.76744827]\n",
            " [ 1.28715094]\n",
            " [ 0.20494124]] forward_w\n",
            "[[-0.76744827]\n",
            " [ 1.28715094]\n",
            " [ 0.20494124]] loss_w\n",
            "0.0003869463016039431 Loss 272\n",
            "[[-0.76744827]\n",
            " [ 1.28715094]\n",
            " [ 0.20494124]] grad_w\n",
            "[[-0.76758841]\n",
            " [ 1.28725291]\n",
            " [ 0.20525133]] init_w\n",
            "[[-0.76758841]\n",
            " [ 1.28725291]\n",
            " [ 0.20525133]] forward_w\n",
            "[[-0.76758841]\n",
            " [ 1.28725291]\n",
            " [ 0.20525133]] loss_w\n",
            "0.00037442945948530437 Loss 273\n",
            "[[-0.76758841]\n",
            " [ 1.28725291]\n",
            " [ 0.20525133]] grad_w\n",
            "[[-0.76772627]\n",
            " [ 1.28735322]\n",
            " [ 0.20555637]] init_w\n",
            "[[-0.76772627]\n",
            " [ 1.28735322]\n",
            " [ 0.20555637]] forward_w\n",
            "[[-0.76772627]\n",
            " [ 1.28735322]\n",
            " [ 0.20555637]] loss_w\n",
            "0.00036231750904278017 Loss 274\n",
            "[[-0.76772627]\n",
            " [ 1.28735322]\n",
            " [ 0.20555637]] grad_w\n",
            "[[-0.76786188]\n",
            " [ 1.28745189]\n",
            " [ 0.20585644]] init_w\n",
            "[[-0.76786188]\n",
            " [ 1.28745189]\n",
            " [ 0.20585644]] forward_w\n",
            "[[-0.76786188]\n",
            " [ 1.28745189]\n",
            " [ 0.20585644]] loss_w\n",
            "0.00035059735294176904 Loss 275\n",
            "[[-0.76786188]\n",
            " [ 1.28745189]\n",
            " [ 0.20585644]] grad_w\n",
            "[[-0.76799528]\n",
            " [ 1.28754895]\n",
            " [ 0.20615161]] init_w\n",
            "[[-0.76799528]\n",
            " [ 1.28754895]\n",
            " [ 0.20615161]] forward_w\n",
            "[[-0.76799528]\n",
            " [ 1.28754895]\n",
            " [ 0.20615161]] loss_w\n",
            "0.00033925631751697063 Loss 276\n",
            "[[-0.76799528]\n",
            " [ 1.28754895]\n",
            " [ 0.20615161]] grad_w\n",
            "[[-0.76812651]\n",
            " [ 1.28764443]\n",
            " [ 0.20644197]] init_w\n",
            "[[-0.76812651]\n",
            " [ 1.28764443]\n",
            " [ 0.20644197]] forward_w\n",
            "[[-0.76812651]\n",
            " [ 1.28764443]\n",
            " [ 0.20644197]] loss_w\n",
            "0.0003282821390676387 Loss 277\n",
            "[[-0.76812651]\n",
            " [ 1.28764443]\n",
            " [ 0.20644197]] grad_w\n",
            "[[-0.76825559]\n",
            " [ 1.28773836]\n",
            " [ 0.20672759]] init_w\n",
            "[[-0.76825559]\n",
            " [ 1.28773836]\n",
            " [ 0.20672759]] forward_w\n",
            "[[-0.76825559]\n",
            " [ 1.28773836]\n",
            " [ 0.20672759]] loss_w\n",
            "0.0003176629505961468 Loss 278\n",
            "[[-0.76825559]\n",
            " [ 1.28773836]\n",
            " [ 0.20672759]] grad_w\n",
            "[[-0.76838257]\n",
            " [ 1.28783075]\n",
            " [ 0.20700855]] init_w\n",
            "[[-0.76838257]\n",
            " [ 1.28783075]\n",
            " [ 0.20700855]] forward_w\n",
            "[[-0.76838257]\n",
            " [ 1.28783075]\n",
            " [ 0.20700855]] loss_w\n",
            "0.0003073872689755397 Loss 279\n",
            "[[-0.76838257]\n",
            " [ 1.28783075]\n",
            " [ 0.20700855]] grad_w\n",
            "[[-0.76850748]\n",
            " [ 1.28792163]\n",
            " [ 0.20728494]] init_w\n",
            "[[-0.76850748]\n",
            " [ 1.28792163]\n",
            " [ 0.20728494]] forward_w\n",
            "[[-0.76850748]\n",
            " [ 1.28792163]\n",
            " [ 0.20728494]] loss_w\n",
            "0.00029744398253217925 Loss 280\n",
            "[[-0.76850748]\n",
            " [ 1.28792163]\n",
            " [ 0.20728494]] grad_w\n",
            "[[-0.76863035]\n",
            " [ 1.28801103]\n",
            " [ 0.20755681]] init_w\n",
            "[[-0.76863035]\n",
            " [ 1.28801103]\n",
            " [ 0.20755681]] forward_w\n",
            "[[-0.76863035]\n",
            " [ 1.28801103]\n",
            " [ 0.20755681]] loss_w\n",
            "0.0002878223390300661 Loss 281\n",
            "[[-0.76863035]\n",
            " [ 1.28801103]\n",
            " [ 0.20755681]] grad_w\n",
            "[[-0.76875122]\n",
            " [ 1.28809898]\n",
            " [ 0.20782426]] init_w\n",
            "[[-0.76875122]\n",
            " [ 1.28809898]\n",
            " [ 0.20782426]] forward_w\n",
            "[[-0.76875122]\n",
            " [ 1.28809898]\n",
            " [ 0.20782426]] loss_w\n",
            "0.0002785119340438358 Loss 282\n",
            "[[-0.76875122]\n",
            " [ 1.28809898]\n",
            " [ 0.20782426]] grad_w\n",
            "[[-0.76887012]\n",
            " [ 1.28818549]\n",
            " [ 0.20808734]] init_w\n",
            "[[-0.76887012]\n",
            " [ 1.28818549]\n",
            " [ 0.20808734]] forward_w\n",
            "[[-0.76887012]\n",
            " [ 1.28818549]\n",
            " [ 0.20808734]] loss_w\n",
            "0.00026950269970787554 Loss 283\n",
            "[[-0.76887012]\n",
            " [ 1.28818549]\n",
            " [ 0.20808734]] grad_w\n",
            "[[-0.76898708]\n",
            " [ 1.28827059]\n",
            " [ 0.20834613]] init_w\n",
            "[[-0.76898708]\n",
            " [ 1.28827059]\n",
            " [ 0.20834613]] forward_w\n",
            "[[-0.76898708]\n",
            " [ 1.28827059]\n",
            " [ 0.20834613]] loss_w\n",
            "0.00026078489382936454 Loss 284\n",
            "[[-0.76898708]\n",
            " [ 1.28827059]\n",
            " [ 0.20834613]] grad_w\n",
            "[[-0.76910213]\n",
            " [ 1.2883543 ]\n",
            " [ 0.2086007 ]] init_w\n",
            "[[-0.76910213]\n",
            " [ 1.2883543 ]\n",
            " [ 0.2086007 ]] forward_w\n",
            "[[-0.76910213]\n",
            " [ 1.2883543 ]\n",
            " [ 0.2086007 ]] loss_w\n",
            "0.00025234908935350237 Loss 285\n",
            "[[-0.76910213]\n",
            " [ 1.2883543 ]\n",
            " [ 0.2086007 ]] grad_w\n",
            "[[-0.7692153 ]\n",
            " [ 1.28843665]\n",
            " [ 0.20885112]] init_w\n",
            "[[-0.7692153 ]\n",
            " [ 1.28843665]\n",
            " [ 0.20885112]] forward_w\n",
            "[[-0.7692153 ]\n",
            " [ 1.28843665]\n",
            " [ 0.20885112]] loss_w\n",
            "0.000244186164169498 Loss 286\n",
            "[[-0.7692153 ]\n",
            " [ 1.28843665]\n",
            " [ 0.20885112]] grad_w\n",
            "[[-0.76932663]\n",
            " [ 1.28851765]\n",
            " [ 0.20909746]] init_w\n",
            "[[-0.76932663]\n",
            " [ 1.28851765]\n",
            " [ 0.20909746]] forward_w\n",
            "[[-0.76932663]\n",
            " [ 1.28851765]\n",
            " [ 0.20909746]] loss_w\n",
            "0.0002362872912463136 Loss 287\n",
            "[[-0.76932663]\n",
            " [ 1.28851765]\n",
            " [ 0.20909746]] grad_w\n",
            "[[-0.76943615]\n",
            " [ 1.28859734]\n",
            " [ 0.20933978]] init_w\n",
            "[[-0.76943615]\n",
            " [ 1.28859734]\n",
            " [ 0.20933978]] forward_w\n",
            "[[-0.76943615]\n",
            " [ 1.28859734]\n",
            " [ 0.20933978]] loss_w\n",
            "0.00022864392908750363 Loss 288\n",
            "[[-0.76943615]\n",
            " [ 1.28859734]\n",
            " [ 0.20933978]] grad_w\n",
            "[[-0.76954388]\n",
            " [ 1.28867572]\n",
            " [ 0.20957815]] init_w\n",
            "[[-0.76954388]\n",
            " [ 1.28867572]\n",
            " [ 0.20957815]] forward_w\n",
            "[[-0.76954388]\n",
            " [ 1.28867572]\n",
            " [ 0.20957815]] loss_w\n",
            "0.00022124781249481204 Loss 289\n",
            "[[-0.76954388]\n",
            " [ 1.28867572]\n",
            " [ 0.20957815]] grad_w\n",
            "[[-0.76964985]\n",
            " [ 1.28875283]\n",
            " [ 0.20981263]] init_w\n",
            "[[-0.76964985]\n",
            " [ 1.28875283]\n",
            " [ 0.20981263]] forward_w\n",
            "[[-0.76964985]\n",
            " [ 1.28875283]\n",
            " [ 0.20981263]] loss_w\n",
            "0.00021409094363054586 Loss 290\n",
            "[[-0.76964985]\n",
            " [ 1.28875283]\n",
            " [ 0.20981263]] grad_w\n",
            "[[-0.76975409]\n",
            " [ 1.28882868]\n",
            " [ 0.21004329]] init_w\n",
            "[[-0.76975409]\n",
            " [ 1.28882868]\n",
            " [ 0.21004329]] forward_w\n",
            "[[-0.76975409]\n",
            " [ 1.28882868]\n",
            " [ 0.21004329]] loss_w\n",
            "0.0002071655833690663 Loss 291\n",
            "[[-0.76975409]\n",
            " [ 1.28882868]\n",
            " [ 0.21004329]] grad_w\n",
            "[[-0.76985664]\n",
            " [ 1.28890329]\n",
            " [ 0.21027019]] init_w\n",
            "[[-0.76985664]\n",
            " [ 1.28890329]\n",
            " [ 0.21027019]] forward_w\n",
            "[[-0.76985664]\n",
            " [ 1.28890329]\n",
            " [ 0.21027019]] loss_w\n",
            "0.00020046424292803318 Loss 292\n",
            "[[-0.76985664]\n",
            " [ 1.28890329]\n",
            " [ 0.21027019]] grad_w\n",
            "[[-0.76995751]\n",
            " [ 1.28897668]\n",
            " [ 0.21049338]] init_w\n",
            "[[-0.76995751]\n",
            " [ 1.28897668]\n",
            " [ 0.21049338]] forward_w\n",
            "[[-0.76995751]\n",
            " [ 1.28897668]\n",
            " [ 0.21049338]] loss_w\n",
            "0.00019397967577036495 Loss 293\n",
            "[[-0.76995751]\n",
            " [ 1.28897668]\n",
            " [ 0.21049338]] grad_w\n",
            "[[-0.77005674]\n",
            " [ 1.28904888]\n",
            " [ 0.21071294]] init_w\n",
            "[[-0.77005674]\n",
            " [ 1.28904888]\n",
            " [ 0.21071294]] forward_w\n",
            "[[-0.77005674]\n",
            " [ 1.28904888]\n",
            " [ 0.21071294]] loss_w\n",
            "0.00018770486976814067 Loss 294\n",
            "[[-0.77005674]\n",
            " [ 1.28904888]\n",
            " [ 0.21071294]] grad_w\n",
            "[[-0.77015434]\n",
            " [ 1.2891199 ]\n",
            " [ 0.21092892]] init_w\n",
            "[[-0.77015434]\n",
            " [ 1.2891199 ]\n",
            " [ 0.21092892]] forward_w\n",
            "[[-0.77015434]\n",
            " [ 1.2891199 ]\n",
            " [ 0.21092892]] loss_w\n",
            "0.00018163303962000593 Loss 295\n",
            "[[-0.77015434]\n",
            " [ 1.2891199 ]\n",
            " [ 0.21092892]] grad_w\n",
            "[[-0.77025036]\n",
            " [ 1.28918976]\n",
            " [ 0.21114137]] init_w\n",
            "[[-0.77025036]\n",
            " [ 1.28918976]\n",
            " [ 0.21114137]] forward_w\n",
            "[[-0.77025036]\n",
            " [ 1.28918976]\n",
            " [ 0.21114137]] loss_w\n",
            "0.0001757576195138354 Loss 296\n",
            "[[-0.77025036]\n",
            " [ 1.28918976]\n",
            " [ 0.21114137]] grad_w\n",
            "[[-0.77034481]\n",
            " [ 1.28925849]\n",
            " [ 0.21135036]] init_w\n",
            "[[-0.77034481]\n",
            " [ 1.28925849]\n",
            " [ 0.21135036]] forward_w\n",
            "[[-0.77034481]\n",
            " [ 1.28925849]\n",
            " [ 0.21135036]] loss_w\n",
            "0.00017007225602674647 Loss 297\n",
            "[[-0.77034481]\n",
            " [ 1.28925849]\n",
            " [ 0.21135036]] grad_w\n",
            "[[-0.77043772]\n",
            " [ 1.28932609]\n",
            " [ 0.21155595]] init_w\n",
            "[[-0.77043772]\n",
            " [ 1.28932609]\n",
            " [ 0.21155595]] forward_w\n",
            "[[-0.77043772]\n",
            " [ 1.28932609]\n",
            " [ 0.21155595]] loss_w\n",
            "0.00016457080125479486 Loss 298\n",
            "[[-0.77043772]\n",
            " [ 1.28932609]\n",
            " [ 0.21155595]] grad_w\n",
            "[[-0.77052912]\n",
            " [ 1.28939259]\n",
            " [ 0.21175818]] init_w\n",
            "[[-0.77052912]\n",
            " [ 1.28939259]\n",
            " [ 0.21175818]] forward_w\n",
            "[[-0.77052912]\n",
            " [ 1.28939259]\n",
            " [ 0.21175818]] loss_w\n",
            "0.00015924730616489343 Loss 299\n",
            "[[-0.77052912]\n",
            " [ 1.28939259]\n",
            " [ 0.21175818]] grad_w\n",
            "[[-0.77061903]\n",
            " [ 1.28945801]\n",
            " [ 0.21195711]] init_w\n",
            "[[-0.77061903]\n",
            " [ 1.28945801]\n",
            " [ 0.21195711]] forward_w\n",
            "[[-0.77061903]\n",
            " [ 1.28945801]\n",
            " [ 0.21195711]] loss_w\n",
            "0.00015409601416178478 Loss 300\n",
            "[[-0.77061903]\n",
            " [ 1.28945801]\n",
            " [ 0.21195711]] grad_w\n",
            "[[-0.77070747]\n",
            " [ 1.28952236]\n",
            " [ 0.2121528 ]] init_w\n",
            "[[-0.77070747]\n",
            " [ 1.28952236]\n",
            " [ 0.2121528 ]] forward_w\n",
            "[[-0.77070747]\n",
            " [ 1.28952236]\n",
            " [ 0.2121528 ]] loss_w\n",
            "0.00014911135486311928 Loss 301\n",
            "[[-0.77070747]\n",
            " [ 1.28952236]\n",
            " [ 0.2121528 ]] grad_w\n",
            "[[-0.77079446]\n",
            " [ 1.28958566]\n",
            " [ 0.21234529]] init_w\n",
            "[[-0.77079446]\n",
            " [ 1.28958566]\n",
            " [ 0.21234529]] forward_w\n",
            "[[-0.77079446]\n",
            " [ 1.28958566]\n",
            " [ 0.21234529]] loss_w\n",
            "0.000144287938075875 Loss 302\n",
            "[[-0.77079446]\n",
            " [ 1.28958566]\n",
            " [ 0.21234529]] grad_w\n",
            "[[-0.77088004]\n",
            " [ 1.28964792]\n",
            " [ 0.21253465]] init_w\n",
            "[[-0.77088004]\n",
            " [ 1.28964792]\n",
            " [ 0.21253465]] forward_w\n",
            "[[-0.77088004]\n",
            " [ 1.28964792]\n",
            " [ 0.21253465]] loss_w\n",
            "0.00013962054796765343 Loss 303\n",
            "[[-0.77088004]\n",
            " [ 1.28964792]\n",
            " [ 0.21253465]] grad_w\n",
            "[[-0.77096423]\n",
            " [ 1.28970918]\n",
            " [ 0.21272092]] init_w\n",
            "[[-0.77096423]\n",
            " [ 1.28970918]\n",
            " [ 0.21272092]] forward_w\n",
            "[[-0.77096423]\n",
            " [ 1.28970918]\n",
            " [ 0.21272092]] loss_w\n",
            "0.000135104137426487 Loss 304\n",
            "[[-0.77096423]\n",
            " [ 1.28970918]\n",
            " [ 0.21272092]] grad_w\n",
            "[[-0.77104704]\n",
            " [ 1.28976943]\n",
            " [ 0.21290416]] init_w\n",
            "[[-0.77104704]\n",
            " [ 1.28976943]\n",
            " [ 0.21290416]] forward_w\n",
            "[[-0.77104704]\n",
            " [ 1.28976943]\n",
            " [ 0.21290416]] loss_w\n",
            "0.00013073382260313037 Loss 305\n",
            "[[-0.77104704]\n",
            " [ 1.28976943]\n",
            " [ 0.21290416]] grad_w\n",
            "[[-0.7711285]\n",
            " [ 1.2898287]\n",
            " [ 0.2130844]] init_w\n",
            "[[-0.7711285]\n",
            " [ 1.2898287]\n",
            " [ 0.2130844]] forward_w\n",
            "[[-0.7711285]\n",
            " [ 1.2898287]\n",
            " [ 0.2130844]] loss_w\n",
            "0.0001265048776298699 Loss 306\n",
            "[[-0.7711285]\n",
            " [ 1.2898287]\n",
            " [ 0.2130844]] grad_w\n",
            "[[-0.77120863]\n",
            " [ 1.28988701]\n",
            " [ 0.21326171]] init_w\n",
            "[[-0.77120863]\n",
            " [ 1.28988701]\n",
            " [ 0.21326171]] forward_w\n",
            "[[-0.77120863]\n",
            " [ 1.28988701]\n",
            " [ 0.21326171]] loss_w\n",
            "0.0001224127295101773 Loss 307\n",
            "[[-0.77120863]\n",
            " [ 1.28988701]\n",
            " [ 0.21326171]] grad_w\n",
            "[[-0.77128745]\n",
            " [ 1.28994436]\n",
            " [ 0.21343612]] init_w\n",
            "[[-0.77128745]\n",
            " [ 1.28994436]\n",
            " [ 0.21343612]] forward_w\n",
            "[[-0.77128745]\n",
            " [ 1.28994436]\n",
            " [ 0.21343612]] loss_w\n",
            "0.00011845295317366962 Loss 308\n",
            "[[-0.77128745]\n",
            " [ 1.28994436]\n",
            " [ 0.21343612]] grad_w\n",
            "[[-0.77136499]\n",
            " [ 1.29000078]\n",
            " [ 0.21360769]] init_w\n",
            "[[-0.77136499]\n",
            " [ 1.29000078]\n",
            " [ 0.21360769]] forward_w\n",
            "[[-0.77136499]\n",
            " [ 1.29000078]\n",
            " [ 0.21360769]] loss_w\n",
            "0.00011462126669103507 Loss 309\n",
            "[[-0.77136499]\n",
            " [ 1.29000078]\n",
            " [ 0.21360769]] grad_w\n",
            "[[-0.77144127]\n",
            " [ 1.29005628]\n",
            " [ 0.21377646]] init_w\n",
            "[[-0.77144127]\n",
            " [ 1.29005628]\n",
            " [ 0.21377646]] forward_w\n",
            "[[-0.77144127]\n",
            " [ 1.29005628]\n",
            " [ 0.21377646]] loss_w\n",
            "0.00011091352664373814 Loss 310\n",
            "[[-0.77144127]\n",
            " [ 1.29005628]\n",
            " [ 0.21377646]] grad_w\n",
            "[[-0.7715163 ]\n",
            " [ 1.29011087]\n",
            " [ 0.21394248]] init_w\n",
            "[[-0.7715163 ]\n",
            " [ 1.29011087]\n",
            " [ 0.21394248]] forward_w\n",
            "[[-0.7715163 ]\n",
            " [ 1.29011087]\n",
            " [ 0.21394248]] loss_w\n",
            "0.00010732572364351134 Loss 311\n",
            "[[-0.7715163 ]\n",
            " [ 1.29011087]\n",
            " [ 0.21394248]] grad_w\n",
            "[[-0.77159011]\n",
            " [ 1.29016457]\n",
            " [ 0.2141058 ]] init_w\n",
            "[[-0.77159011]\n",
            " [ 1.29016457]\n",
            " [ 0.2141058 ]] forward_w\n",
            "[[-0.77159011]\n",
            " [ 1.29016457]\n",
            " [ 0.2141058 ]] loss_w\n",
            "0.00010385397799677402 Loss 312\n",
            "[[-0.77159011]\n",
            " [ 1.29016457]\n",
            " [ 0.2141058 ]] grad_w\n",
            "[[-0.77166271]\n",
            " [ 1.2902174 ]\n",
            " [ 0.21426645]] init_w\n",
            "[[-0.77166271]\n",
            " [ 1.2902174 ]\n",
            " [ 0.21426645]] forward_w\n",
            "[[-0.77166271]\n",
            " [ 1.2902174 ]\n",
            " [ 0.21426645]] loss_w\n",
            "0.00010049453550930243 Loss 313\n",
            "[[-0.77166271]\n",
            " [ 1.2902174 ]\n",
            " [ 0.21426645]] grad_w\n",
            "[[-0.77173413]\n",
            " [ 1.29026937]\n",
            " [ 0.21442448]] init_w\n",
            "[[-0.77173413]\n",
            " [ 1.29026937]\n",
            " [ 0.21442448]] forward_w\n",
            "[[-0.77173413]\n",
            " [ 1.29026937]\n",
            " [ 0.21442448]] loss_w\n",
            "9.724376342660792e-05 Loss 314\n",
            "[[-0.77173413]\n",
            " [ 1.29026937]\n",
            " [ 0.21442448]] grad_w\n",
            "[[-0.77180439]\n",
            " [ 1.29032048]\n",
            " [ 0.21457993]] init_w\n",
            "[[-0.77180439]\n",
            " [ 1.29032048]\n",
            " [ 0.21457993]] forward_w\n",
            "[[-0.77180439]\n",
            " [ 1.29032048]\n",
            " [ 0.21457993]] loss_w\n",
            "9.409814650563513e-05 Loss 315\n",
            "[[-0.77180439]\n",
            " [ 1.29032048]\n",
            " [ 0.21457993]] grad_w\n",
            "[[-0.7718735 ]\n",
            " [ 1.29037077]\n",
            " [ 0.21473285]] init_w\n",
            "[[-0.7718735 ]\n",
            " [ 1.29037077]\n",
            " [ 0.21473285]] forward_w\n",
            "[[-0.7718735 ]\n",
            " [ 1.29037077]\n",
            " [ 0.21473285]] loss_w\n",
            "9.105428321353173e-05 Loss 316\n",
            "[[-0.7718735 ]\n",
            " [ 1.29037077]\n",
            " [ 0.21473285]] grad_w\n",
            "[[-0.77194148]\n",
            " [ 1.29042023]\n",
            " [ 0.21488327]] init_w\n",
            "[[-0.77194148]\n",
            " [ 1.29042023]\n",
            " [ 0.21488327]] forward_w\n",
            "[[-0.77194148]\n",
            " [ 1.29042023]\n",
            " [ 0.21488327]] loss_w\n",
            "8.810888204937731e-05 Loss 317\n",
            "[[-0.77194148]\n",
            " [ 1.29042023]\n",
            " [ 0.21488327]] grad_w\n",
            "[[-0.77200836]\n",
            " [ 1.29046889]\n",
            " [ 0.21503125]] init_w\n",
            "[[-0.77200836]\n",
            " [ 1.29046889]\n",
            " [ 0.21503125]] forward_w\n",
            "[[-0.77200836]\n",
            " [ 1.29046889]\n",
            " [ 0.21503125]] loss_w\n",
            "8.525875798489875e-05 Loss 318\n",
            "[[-0.77200836]\n",
            " [ 1.29046889]\n",
            " [ 0.21503125]] grad_w\n",
            "[[-0.77207414]\n",
            " [ 1.29051676]\n",
            " [ 0.21517681]] init_w\n",
            "[[-0.77207414]\n",
            " [ 1.29051676]\n",
            " [ 0.21517681]] forward_w\n",
            "[[-0.77207414]\n",
            " [ 1.29051676]\n",
            " [ 0.21517681]] loss_w\n",
            "8.250082902032453e-05 Loss 319\n",
            "[[-0.77207414]\n",
            " [ 1.29051676]\n",
            " [ 0.21517681]] grad_w\n",
            "[[-0.77213885]\n",
            " [ 1.29056384]\n",
            " [ 0.21531999]] init_w\n",
            "[[-0.77213885]\n",
            " [ 1.29056384]\n",
            " [ 0.21531999]] forward_w\n",
            "[[-0.77213885]\n",
            " [ 1.29056384]\n",
            " [ 0.21531999]] loss_w\n",
            "7.983211285163668e-05 Loss 320\n",
            "[[-0.77213885]\n",
            " [ 1.29056384]\n",
            " [ 0.21531999]] grad_w\n",
            "[[-0.77220251]\n",
            " [ 1.29061016]\n",
            " [ 0.21546084]] init_w\n",
            "[[-0.77220251]\n",
            " [ 1.29061016]\n",
            " [ 0.21546084]] forward_w\n",
            "[[-0.77220251]\n",
            " [ 1.29061016]\n",
            " [ 0.21546084]] loss_w\n",
            "7.724972364564417e-05 Loss 321\n",
            "[[-0.77220251]\n",
            " [ 1.29061016]\n",
            " [ 0.21546084]] grad_w\n",
            "[[-0.77226513]\n",
            " [ 1.29065572]\n",
            " [ 0.2155994 ]] init_w\n",
            "[[-0.77226513]\n",
            " [ 1.29065572]\n",
            " [ 0.2155994 ]] forward_w\n",
            "[[-0.77226513]\n",
            " [ 1.29065572]\n",
            " [ 0.2155994 ]] loss_w\n",
            "7.475086891935985e-05 Loss 322\n",
            "[[-0.77226513]\n",
            " [ 1.29065572]\n",
            " [ 0.2155994 ]] grad_w\n",
            "[[-0.77232672]\n",
            " [ 1.29070054]\n",
            " [ 0.21573569]] init_w\n",
            "[[-0.77232672]\n",
            " [ 1.29070054]\n",
            " [ 0.21573569]] forward_w\n",
            "[[-0.77232672]\n",
            " [ 1.29070054]\n",
            " [ 0.21573569]] loss_w\n",
            "7.233284652034431e-05 Loss 323\n",
            "[[-0.77232672]\n",
            " [ 1.29070054]\n",
            " [ 0.21573569]] grad_w\n",
            "[[-0.77238731]\n",
            " [ 1.29074463]\n",
            " [ 0.21586976]] init_w\n",
            "[[-0.77238731]\n",
            " [ 1.29074463]\n",
            " [ 0.21586976]] forward_w\n",
            "[[-0.77238731]\n",
            " [ 1.29074463]\n",
            " [ 0.21586976]] loss_w\n",
            "6.999304170470239e-05 Loss 324\n",
            "[[-0.77238731]\n",
            " [ 1.29074463]\n",
            " [ 0.21586976]] grad_w\n",
            "[[-0.77244692]\n",
            " [ 1.29078799]\n",
            " [ 0.21600165]] init_w\n",
            "[[-0.77244692]\n",
            " [ 1.29078799]\n",
            " [ 0.21600165]] forward_w\n",
            "[[-0.77244692]\n",
            " [ 1.29078799]\n",
            " [ 0.21600165]] loss_w\n",
            "6.772892430962938e-05 Loss 325\n",
            "[[-0.77244692]\n",
            " [ 1.29078799]\n",
            " [ 0.21600165]] grad_w\n",
            "[[-0.77250555]\n",
            " [ 1.29083066]\n",
            " [ 0.21613138]] init_w\n",
            "[[-0.77250555]\n",
            " [ 1.29083066]\n",
            " [ 0.21613138]] forward_w\n",
            "[[-0.77250555]\n",
            " [ 1.29083066]\n",
            " [ 0.21613138]] loss_w\n",
            "6.55380460173827e-05 Loss 326\n",
            "[[-0.77250555]\n",
            " [ 1.29083066]\n",
            " [ 0.21613138]] grad_w\n",
            "[[-0.77256323]\n",
            " [ 1.29087262]\n",
            " [ 0.216259  ]] init_w\n",
            "[[-0.77256323]\n",
            " [ 1.29087262]\n",
            " [ 0.216259  ]] forward_w\n",
            "[[-0.77256323]\n",
            " [ 1.29087262]\n",
            " [ 0.216259  ]] loss_w\n",
            "6.341803770779837e-05 Loss 327\n",
            "[[-0.77256323]\n",
            " [ 1.29087262]\n",
            " [ 0.216259  ]] grad_w\n",
            "[[-0.77261996]\n",
            " [ 1.2909139 ]\n",
            " [ 0.21638454]] init_w\n",
            "[[-0.77261996]\n",
            " [ 1.2909139 ]\n",
            " [ 0.21638454]] forward_w\n",
            "[[-0.77261996]\n",
            " [ 1.2909139 ]\n",
            " [ 0.21638454]] loss_w\n",
            "6.136660689641218e-05 Loss 328\n",
            "[[-0.77261996]\n",
            " [ 1.2909139 ]\n",
            " [ 0.21638454]] grad_w\n",
            "[[-0.77267577]\n",
            " [ 1.29095451]\n",
            " [ 0.21650803]] init_w\n",
            "[[-0.77267577]\n",
            " [ 1.29095451]\n",
            " [ 0.21650803]] forward_w\n",
            "[[-0.77267577]\n",
            " [ 1.29095451]\n",
            " [ 0.21650803]] loss_w\n",
            "5.938153525547592e-05 Loss 329\n",
            "[[-0.77267577]\n",
            " [ 1.29095451]\n",
            " [ 0.21650803]] grad_w\n",
            "[[-0.77273068]\n",
            " [ 1.29099446]\n",
            " [ 0.21662951]] init_w\n",
            "[[-0.77273068]\n",
            " [ 1.29099446]\n",
            " [ 0.21662951]] forward_w\n",
            "[[-0.77273068]\n",
            " [ 1.29099446]\n",
            " [ 0.21662951]] loss_w\n",
            "5.74606762151536e-05 Loss 330\n",
            "[[-0.77273068]\n",
            " [ 1.29099446]\n",
            " [ 0.21662951]] grad_w\n",
            "[[-0.77278468]\n",
            " [ 1.29103375]\n",
            " [ 0.216749  ]] init_w\n",
            "[[-0.77278468]\n",
            " [ 1.29103375]\n",
            " [ 0.216749  ]] forward_w\n",
            "[[-0.77278468]\n",
            " [ 1.29103375]\n",
            " [ 0.216749  ]] loss_w\n",
            "5.560195264231093e-05 Loss 331\n",
            "[[-0.77278468]\n",
            " [ 1.29103375]\n",
            " [ 0.216749  ]] grad_w\n",
            "[[-0.77283781]\n",
            " [ 1.29107241]\n",
            " [ 0.21686655]] init_w\n",
            "[[-0.77283781]\n",
            " [ 1.29107241]\n",
            " [ 0.21686655]] forward_w\n",
            "[[-0.77283781]\n",
            " [ 1.29107241]\n",
            " [ 0.21686655]] loss_w\n",
            "5.380335459439905e-05 Loss 332\n",
            "[[-0.77283781]\n",
            " [ 1.29107241]\n",
            " [ 0.21686655]] grad_w\n",
            "[[-0.77289006]\n",
            " [ 1.29111043]\n",
            " [ 0.21698218]] init_w\n",
            "[[-0.77289006]\n",
            " [ 1.29111043]\n",
            " [ 0.21698218]] forward_w\n",
            "[[-0.77289006]\n",
            " [ 1.29111043]\n",
            " [ 0.21698218]] loss_w\n",
            "5.2062937145984455e-05 Loss 333\n",
            "[[-0.77289006]\n",
            " [ 1.29111043]\n",
            " [ 0.21698218]] grad_w\n",
            "[[-0.77294147]\n",
            " [ 1.29114783]\n",
            " [ 0.21709593]] init_w\n",
            "[[-0.77294147]\n",
            " [ 1.29114783]\n",
            " [ 0.21709593]] forward_w\n",
            "[[-0.77294147]\n",
            " [ 1.29114783]\n",
            " [ 0.21709593]] loss_w\n",
            "5.037881828559451e-05 Loss 334\n",
            "[[-0.77294147]\n",
            " [ 1.29114783]\n",
            " [ 0.21709593]] grad_w\n",
            "[[-0.77299204]\n",
            " [ 1.29118463]\n",
            " [ 0.21720782]] init_w\n",
            "[[-0.77299204]\n",
            " [ 1.29118463]\n",
            " [ 0.21720782]] forward_w\n",
            "[[-0.77299204]\n",
            " [ 1.29118463]\n",
            " [ 0.21720782]] loss_w\n",
            "4.874917688059647e-05 Loss 335\n",
            "[[-0.77299204]\n",
            " [ 1.29118463]\n",
            " [ 0.21720782]] grad_w\n",
            "[[-0.77304178]\n",
            " [ 1.29122082]\n",
            " [ 0.21731788]] init_w\n",
            "[[-0.77304178]\n",
            " [ 1.29122082]\n",
            " [ 0.21731788]] forward_w\n",
            "[[-0.77304178]\n",
            " [ 1.29122082]\n",
            " [ 0.21731788]] loss_w\n",
            "4.717225070789743e-05 Loss 336\n",
            "[[-0.77304178]\n",
            " [ 1.29122082]\n",
            " [ 0.21731788]] grad_w\n",
            "[[-0.77309071]\n",
            " [ 1.29125642]\n",
            " [ 0.21742616]] init_w\n",
            "[[-0.77309071]\n",
            " [ 1.29125642]\n",
            " [ 0.21742616]] forward_w\n",
            "[[-0.77309071]\n",
            " [ 1.29125642]\n",
            " [ 0.21742616]] loss_w\n",
            "4.564633454835837e-05 Loss 337\n",
            "[[-0.77309071]\n",
            " [ 1.29125642]\n",
            " [ 0.21742616]] grad_w\n",
            "[[-0.77313885]\n",
            " [ 1.29129145]\n",
            " [ 0.21753266]] init_w\n",
            "[[-0.77313885]\n",
            " [ 1.29129145]\n",
            " [ 0.21753266]] forward_w\n",
            "[[-0.77313885]\n",
            " [ 1.29129145]\n",
            " [ 0.21753266]] loss_w\n",
            "4.416977834283719e-05 Loss 338\n",
            "[[-0.77313885]\n",
            " [ 1.29129145]\n",
            " [ 0.21753266]] grad_w\n",
            "[[-0.7731862 ]\n",
            " [ 1.2913259 ]\n",
            " [ 0.21763743]] init_w\n",
            "[[-0.7731862 ]\n",
            " [ 1.2913259 ]\n",
            " [ 0.21763743]] forward_w\n",
            "[[-0.7731862 ]\n",
            " [ 1.2913259 ]\n",
            " [ 0.21763743]] loss_w\n",
            "4.274098540789872e-05 Loss 339\n",
            "[[-0.7731862 ]\n",
            " [ 1.2913259 ]\n",
            " [ 0.21763743]] grad_w\n",
            "[[-0.77323277]\n",
            " [ 1.29135979]\n",
            " [ 0.21774049]] init_w\n",
            "[[-0.77323277]\n",
            " [ 1.29135979]\n",
            " [ 0.21774049]] forward_w\n",
            "[[-0.77323277]\n",
            " [ 1.29135979]\n",
            " [ 0.21774049]] loss_w\n",
            "4.135841070921794e-05 Loss 340\n",
            "[[-0.77323277]\n",
            " [ 1.29135979]\n",
            " [ 0.21774049]] grad_w\n",
            "[[-0.77327859]\n",
            " [ 1.29139313]\n",
            " [ 0.21784187]] init_w\n",
            "[[-0.77327859]\n",
            " [ 1.29139313]\n",
            " [ 0.21784187]] forward_w\n",
            "[[-0.77327859]\n",
            " [ 1.29139313]\n",
            " [ 0.21784187]] loss_w\n",
            "4.002055919085644e-05 Loss 341\n",
            "[[-0.77327859]\n",
            " [ 1.29139313]\n",
            " [ 0.21784187]] grad_w\n",
            "[[-0.77332366]\n",
            " [ 1.29142592]\n",
            " [ 0.2179416 ]] init_w\n",
            "[[-0.77332366]\n",
            " [ 1.29142592]\n",
            " [ 0.2179416 ]] forward_w\n",
            "[[-0.77332366]\n",
            " [ 1.29142592]\n",
            " [ 0.2179416 ]] loss_w\n",
            "3.872598415857081e-05 Loss 342\n",
            "[[-0.77332366]\n",
            " [ 1.29142592]\n",
            " [ 0.2179416 ]] grad_w\n",
            "[[-0.773368  ]\n",
            " [ 1.29145818]\n",
            " [ 0.2180397 ]] init_w\n",
            "[[-0.773368  ]\n",
            " [ 1.29145818]\n",
            " [ 0.2180397 ]] forward_w\n",
            "[[-0.773368  ]\n",
            " [ 1.29145818]\n",
            " [ 0.2180397 ]] loss_w\n",
            "3.747328571542149e-05 Loss 343\n",
            "[[-0.773368  ]\n",
            " [ 1.29145818]\n",
            " [ 0.2180397 ]] grad_w\n",
            "[[-0.77341161]\n",
            " [ 1.29148991]\n",
            " [ 0.2181362 ]] init_w\n",
            "[[-0.77341161]\n",
            " [ 1.29148991]\n",
            " [ 0.2181362 ]] forward_w\n",
            "[[-0.77341161]\n",
            " [ 1.29148991]\n",
            " [ 0.2181362 ]] loss_w\n",
            "3.626110924798372e-05 Loss 344\n",
            "[[-0.77341161]\n",
            " [ 1.29148991]\n",
            " [ 0.2181362 ]] grad_w\n",
            "[[-0.77345451]\n",
            " [ 1.29152113]\n",
            " [ 0.21823112]] init_w\n",
            "[[-0.77345451]\n",
            " [ 1.29152113]\n",
            " [ 0.21823112]] forward_w\n",
            "[[-0.77345451]\n",
            " [ 1.29152113]\n",
            " [ 0.21823112]] loss_w\n",
            "3.508814396152878e-05 Loss 345\n",
            "[[-0.77345451]\n",
            " [ 1.29152113]\n",
            " [ 0.21823112]] grad_w\n",
            "[[-0.77349671]\n",
            " [ 1.29155183]\n",
            " [ 0.2183245 ]] init_w\n",
            "[[-0.77349671]\n",
            " [ 1.29155183]\n",
            " [ 0.2183245 ]] forward_w\n",
            "[[-0.77349671]\n",
            " [ 1.29155183]\n",
            " [ 0.2183245 ]] loss_w\n",
            "3.395312146258736e-05 Loss 346\n",
            "[[-0.77349671]\n",
            " [ 1.29155183]\n",
            " [ 0.2183245 ]] grad_w\n",
            "[[-0.77353823]\n",
            " [ 1.29158204]\n",
            " [ 0.21841636]] init_w\n",
            "[[-0.77353823]\n",
            " [ 1.29158204]\n",
            " [ 0.21841636]] forward_w\n",
            "[[-0.77353823]\n",
            " [ 1.29158204]\n",
            " [ 0.21841636]] loss_w\n",
            "3.2854814387365515e-05 Loss 347\n",
            "[[-0.77353823]\n",
            " [ 1.29158204]\n",
            " [ 0.21841636]] grad_w\n",
            "[[-0.77357906]\n",
            " [ 1.29161175]\n",
            " [ 0.21850672]] init_w\n",
            "[[-0.77357906]\n",
            " [ 1.29161175]\n",
            " [ 0.21850672]] forward_w\n",
            "[[-0.77357906]\n",
            " [ 1.29161175]\n",
            " [ 0.21850672]] loss_w\n",
            "3.1792035074527385e-05 Loss 348\n",
            "[[-0.77357906]\n",
            " [ 1.29161175]\n",
            " [ 0.21850672]] grad_w\n",
            "[[-0.77361923]\n",
            " [ 1.29164098]\n",
            " [ 0.2185956 ]] init_w\n",
            "[[-0.77361923]\n",
            " [ 1.29164098]\n",
            " [ 0.2185956 ]] forward_w\n",
            "[[-0.77361923]\n",
            " [ 1.29164098]\n",
            " [ 0.2185956 ]] loss_w\n",
            "3.076363428090683e-05 Loss 349\n",
            "[[-0.77361923]\n",
            " [ 1.29164098]\n",
            " [ 0.2185956 ]] grad_w\n",
            "[[-0.77365875]\n",
            " [ 1.29166973]\n",
            " [ 0.21868304]] init_w\n",
            "[[-0.77365875]\n",
            " [ 1.29166973]\n",
            " [ 0.21868304]] forward_w\n",
            "[[-0.77365875]\n",
            " [ 1.29166973]\n",
            " [ 0.21868304]] loss_w\n",
            "2.976849993876867e-05 Loss 350\n",
            "[[-0.77365875]\n",
            " [ 1.29166973]\n",
            " [ 0.21868304]] grad_w\n",
            "[[-0.77369762]\n",
            " [ 1.29169801]\n",
            " [ 0.21876905]] init_w\n",
            "[[-0.77369762]\n",
            " [ 1.29169801]\n",
            " [ 0.21876905]] forward_w\n",
            "[[-0.77369762]\n",
            " [ 1.29169801]\n",
            " [ 0.21876905]] loss_w\n",
            "2.8805555953265778e-05 Loss 351\n",
            "[[-0.77369762]\n",
            " [ 1.29169801]\n",
            " [ 0.21876905]] grad_w\n",
            "[[-0.77373586]\n",
            " [ 1.29172584]\n",
            " [ 0.21885366]] init_w\n",
            "[[-0.77373586]\n",
            " [ 1.29172584]\n",
            " [ 0.21885366]] forward_w\n",
            "[[-0.77373586]\n",
            " [ 1.29172584]\n",
            " [ 0.21885366]] loss_w\n",
            "2.7873761038797467e-05 Loss 352\n",
            "[[-0.77373586]\n",
            " [ 1.29172584]\n",
            " [ 0.21885366]] grad_w\n",
            "[[-0.77377347]\n",
            " [ 1.2917532 ]\n",
            " [ 0.21893688]] init_w\n",
            "[[-0.77377347]\n",
            " [ 1.2917532 ]\n",
            " [ 0.21893688]] forward_w\n",
            "[[-0.77377347]\n",
            " [ 1.2917532 ]\n",
            " [ 0.21893688]] loss_w\n",
            "2.6972107593011847e-05 Loss 353\n",
            "[[-0.77377347]\n",
            " [ 1.2917532 ]\n",
            " [ 0.21893688]] grad_w\n",
            "[[-0.77381047]\n",
            " [ 1.29178013]\n",
            " [ 0.21901875]] init_w\n",
            "[[-0.77381047]\n",
            " [ 1.29178013]\n",
            " [ 0.21901875]] forward_w\n",
            "[[-0.77381047]\n",
            " [ 1.29178013]\n",
            " [ 0.21901875]] loss_w\n",
            "2.6099620607223044e-05 Loss 354\n",
            "[[-0.77381047]\n",
            " [ 1.29178013]\n",
            " [ 0.21901875]] grad_w\n",
            "[[-0.77384687]\n",
            " [ 1.29180661]\n",
            " [ 0.21909929]] init_w\n",
            "[[-0.77384687]\n",
            " [ 1.29180661]\n",
            " [ 0.21909929]] forward_w\n",
            "[[-0.77384687]\n",
            " [ 1.29180661]\n",
            " [ 0.21909929]] loss_w\n",
            "2.5255356612081883e-05 Loss 355\n",
            "[[-0.77384687]\n",
            " [ 1.29180661]\n",
            " [ 0.21909929]] grad_w\n",
            "[[-0.77388267]\n",
            " [ 1.29183266]\n",
            " [ 0.21917851]] init_w\n",
            "[[-0.77388267]\n",
            " [ 1.29183266]\n",
            " [ 0.21917851]] forward_w\n",
            "[[-0.77388267]\n",
            " [ 1.29183266]\n",
            " [ 0.21917851]] loss_w\n",
            "2.4438402657352186e-05 Loss 356\n",
            "[[-0.77388267]\n",
            " [ 1.29183266]\n",
            " [ 0.21917851]] grad_w\n",
            "[[-0.77391789]\n",
            " [ 1.29185829]\n",
            " [ 0.21925644]] init_w\n",
            "[[-0.77391789]\n",
            " [ 1.29185829]\n",
            " [ 0.21925644]] forward_w\n",
            "[[-0.77391789]\n",
            " [ 1.29185829]\n",
            " [ 0.21925644]] loss_w\n",
            "2.3647875324681196e-05 Loss 357\n",
            "[[-0.77391789]\n",
            " [ 1.29185829]\n",
            " [ 0.21925644]] grad_w\n",
            "[[-0.77395254]\n",
            " [ 1.29188349]\n",
            " [ 0.2193331 ]] init_w\n",
            "[[-0.77395254]\n",
            " [ 1.29188349]\n",
            " [ 0.2193331 ]] forward_w\n",
            "[[-0.77395254]\n",
            " [ 1.29188349]\n",
            " [ 0.2193331 ]] loss_w\n",
            "2.288291977231239e-05 Loss 358\n",
            "[[-0.77395254]\n",
            " [ 1.29188349]\n",
            " [ 0.2193331 ]] grad_w\n",
            "[[-0.77398662]\n",
            " [ 1.29190829]\n",
            " [ 0.21940851]] init_w\n",
            "[[-0.77398662]\n",
            " [ 1.29190829]\n",
            " [ 0.21940851]] forward_w\n",
            "[[-0.77398662]\n",
            " [ 1.29190829]\n",
            " [ 0.21940851]] loss_w\n",
            "2.2142708810698137e-05 Loss 359\n",
            "[[-0.77398662]\n",
            " [ 1.29190829]\n",
            " [ 0.21940851]] grad_w\n",
            "[[-0.77402015]\n",
            " [ 1.29193268]\n",
            " [ 0.21948269]] init_w\n",
            "[[-0.77402015]\n",
            " [ 1.29193268]\n",
            " [ 0.21948269]] forward_w\n",
            "[[-0.77402015]\n",
            " [ 1.29193268]\n",
            " [ 0.21948269]] loss_w\n",
            "2.142644200800883e-05 Loss 360\n",
            "[[-0.77402015]\n",
            " [ 1.29193268]\n",
            " [ 0.21948269]] grad_w\n",
            "[[-0.77405312]\n",
            " [ 1.29195668]\n",
            " [ 0.21955566]] init_w\n",
            "[[-0.77405312]\n",
            " [ 1.29195668]\n",
            " [ 0.21955566]] forward_w\n",
            "[[-0.77405312]\n",
            " [ 1.29195668]\n",
            " [ 0.21955566]] loss_w\n",
            "2.0733344824583613e-05 Loss 361\n",
            "[[-0.77405312]\n",
            " [ 1.29195668]\n",
            " [ 0.21955566]] grad_w\n",
            "[[-0.77408556]\n",
            " [ 1.29198028]\n",
            " [ 0.21962744]] init_w\n",
            "[[-0.77408556]\n",
            " [ 1.29198028]\n",
            " [ 0.21962744]] forward_w\n",
            "[[-0.77408556]\n",
            " [ 1.29198028]\n",
            " [ 0.21962744]] loss_w\n",
            "2.0062667775378703e-05 Loss 362\n",
            "[[-0.77408556]\n",
            " [ 1.29198028]\n",
            " [ 0.21962744]] grad_w\n",
            "[[-0.77411748]\n",
            " [ 1.2920035 ]\n",
            " [ 0.21969805]] init_w\n",
            "[[-0.77411748]\n",
            " [ 1.2920035 ]\n",
            " [ 0.21969805]] forward_w\n",
            "[[-0.77411748]\n",
            " [ 1.2920035 ]\n",
            " [ 0.21969805]] loss_w\n",
            "1.9413685619502e-05 Loss 363\n",
            "[[-0.77411748]\n",
            " [ 1.2920035 ]\n",
            " [ 0.21969805]] grad_w\n",
            "[[-0.77414887]\n",
            " [ 1.29202634]\n",
            " [ 0.21976751]] init_w\n",
            "[[-0.77414887]\n",
            " [ 1.29202634]\n",
            " [ 0.21976751]] forward_w\n",
            "[[-0.77414887]\n",
            " [ 1.29202634]\n",
            " [ 0.21976751]] loss_w\n",
            "1.8785696575976395e-05 Loss 364\n",
            "[[-0.77414887]\n",
            " [ 1.29202634]\n",
            " [ 0.21976751]] grad_w\n",
            "[[-0.77417975]\n",
            " [ 1.29204881]\n",
            " [ 0.21983583]] init_w\n",
            "[[-0.77417975]\n",
            " [ 1.29204881]\n",
            " [ 0.21983583]] forward_w\n",
            "[[-0.77417975]\n",
            " [ 1.29204881]\n",
            " [ 0.21983583]] loss_w\n",
            "1.817802156485518e-05 Loss 365\n",
            "[[-0.77417975]\n",
            " [ 1.29204881]\n",
            " [ 0.21983583]] grad_w\n",
            "[[-0.77421012]\n",
            " [ 1.29207091]\n",
            " [ 0.21990304]] init_w\n",
            "[[-0.77421012]\n",
            " [ 1.29207091]\n",
            " [ 0.21990304]] forward_w\n",
            "[[-0.77421012]\n",
            " [ 1.29207091]\n",
            " [ 0.21990304]] loss_w\n",
            "1.759000347290545e-05 Loss 366\n",
            "[[-0.77421012]\n",
            " [ 1.29207091]\n",
            " [ 0.21990304]] grad_w\n",
            "[[-0.77424   ]\n",
            " [ 1.29209265]\n",
            " [ 0.21996916]] init_w\n",
            "[[-0.77424   ]\n",
            " [ 1.29209265]\n",
            " [ 0.21996916]] forward_w\n",
            "[[-0.77424   ]\n",
            " [ 1.29209265]\n",
            " [ 0.21996916]] loss_w\n",
            "1.7021006443022733e-05 Loss 367\n",
            "[[-0.77424   ]\n",
            " [ 1.29209265]\n",
            " [ 0.21996916]] grad_w\n",
            "[[-0.77426939]\n",
            " [ 1.29211404]\n",
            " [ 0.2200342 ]] init_w\n",
            "[[-0.77426939]\n",
            " [ 1.29211404]\n",
            " [ 0.2200342 ]] forward_w\n",
            "[[-0.77426939]\n",
            " [ 1.29211404]\n",
            " [ 0.2200342 ]] loss_w\n",
            "1.647041518665308e-05 Loss 368\n",
            "[[-0.77426939]\n",
            " [ 1.29211404]\n",
            " [ 0.2200342 ]] grad_w\n",
            "[[-0.77429831]\n",
            " [ 1.29213508]\n",
            " [ 0.22009817]] init_w\n",
            "[[-0.77429831]\n",
            " [ 1.29213508]\n",
            " [ 0.22009817]] forward_w\n",
            "[[-0.77429831]\n",
            " [ 1.29213508]\n",
            " [ 0.22009817]] loss_w\n",
            "1.5937634318441966e-05 Loss 369\n",
            "[[-0.77429831]\n",
            " [ 1.29213508]\n",
            " [ 0.22009817]] grad_w\n",
            "[[-0.77432675]\n",
            " [ 1.29215577]\n",
            " [ 0.22016111]] init_w\n",
            "[[-0.77432675]\n",
            " [ 1.29215577]\n",
            " [ 0.22016111]] forward_w\n",
            "[[-0.77432675]\n",
            " [ 1.29215577]\n",
            " [ 0.22016111]] loss_w\n",
            "1.5422087712409637e-05 Loss 370\n",
            "[[-0.77432675]\n",
            " [ 1.29215577]\n",
            " [ 0.22016111]] grad_w\n",
            "[[-0.77435473]\n",
            " [ 1.29217613]\n",
            " [ 0.22022301]] init_w\n",
            "[[-0.77435473]\n",
            " [ 1.29217613]\n",
            " [ 0.22022301]] forward_w\n",
            "[[-0.77435473]\n",
            " [ 1.29217613]\n",
            " [ 0.22022301]] loss_w\n",
            "1.4923217878957268e-05 Loss 371\n",
            "[[-0.77435473]\n",
            " [ 1.29217613]\n",
            " [ 0.22022301]] grad_w\n",
            "[[-0.77438225]\n",
            " [ 1.29219616]\n",
            " [ 0.22028391]] init_w\n",
            "[[-0.77438225]\n",
            " [ 1.29219616]\n",
            " [ 0.22028391]] forward_w\n",
            "[[-0.77438225]\n",
            " [ 1.29219616]\n",
            " [ 0.22028391]] loss_w\n",
            "1.444048536202021e-05 Loss 372\n",
            "[[-0.77438225]\n",
            " [ 1.29219616]\n",
            " [ 0.22028391]] grad_w\n",
            "[[-0.77440932]\n",
            " [ 1.29221585]\n",
            " [ 0.22034382]] init_w\n",
            "[[-0.77440932]\n",
            " [ 1.29221585]\n",
            " [ 0.22034382]] forward_w\n",
            "[[-0.77440932]\n",
            " [ 1.29221585]\n",
            " [ 0.22034382]] loss_w\n",
            "1.3973368155720724e-05 Loss 373\n",
            "[[-0.77440932]\n",
            " [ 1.29221585]\n",
            " [ 0.22034382]] grad_w\n",
            "[[-0.77443596]\n",
            " [ 1.29223523]\n",
            " [ 0.22040274]] init_w\n",
            "[[-0.77443596]\n",
            " [ 1.29223523]\n",
            " [ 0.22040274]] forward_w\n",
            "[[-0.77443596]\n",
            " [ 1.29223523]\n",
            " [ 0.22040274]] loss_w\n",
            "1.3521361139900189e-05 Loss 374\n",
            "[[-0.77443596]\n",
            " [ 1.29223523]\n",
            " [ 0.22040274]] grad_w\n",
            "[[-0.77446215]\n",
            " [ 1.29225429]\n",
            " [ 0.22046071]] init_w\n",
            "[[-0.77446215]\n",
            " [ 1.29225429]\n",
            " [ 0.22046071]] forward_w\n",
            "[[-0.77446215]\n",
            " [ 1.29225429]\n",
            " [ 0.22046071]] loss_w\n",
            "1.3083975533897903e-05 Loss 375\n",
            "[[-0.77446215]\n",
            " [ 1.29225429]\n",
            " [ 0.22046071]] grad_w\n",
            "[[-0.77448792]\n",
            " [ 1.29227304]\n",
            " [ 0.22051773]] init_w\n",
            "[[-0.77448792]\n",
            " [ 1.29227304]\n",
            " [ 0.22051773]] forward_w\n",
            "[[-0.77448792]\n",
            " [ 1.29227304]\n",
            " [ 0.22051773]] loss_w\n",
            "1.2660738368009266e-05 Loss 376\n",
            "[[-0.77448792]\n",
            " [ 1.29227304]\n",
            " [ 0.22051773]] grad_w\n",
            "[[-0.77451327]\n",
            " [ 1.29229149]\n",
            " [ 0.22057383]] init_w\n",
            "[[-0.77451327]\n",
            " [ 1.29229149]\n",
            " [ 0.22057383]] forward_w\n",
            "[[-0.77451327]\n",
            " [ 1.29229149]\n",
            " [ 0.22057383]] loss_w\n",
            "1.2251191972034164e-05 Loss 377\n",
            "[[-0.77451327]\n",
            " [ 1.29229149]\n",
            " [ 0.22057383]] grad_w\n",
            "[[-0.77453821]\n",
            " [ 1.29230963]\n",
            " [ 0.220629  ]] init_w\n",
            "[[-0.77453821]\n",
            " [ 1.29230963]\n",
            " [ 0.220629  ]] forward_w\n",
            "[[-0.77453821]\n",
            " [ 1.29230963]\n",
            " [ 0.220629  ]] loss_w\n",
            "1.185489348037342e-05 Loss 378\n",
            "[[-0.77453821]\n",
            " [ 1.29230963]\n",
            " [ 0.220629  ]] grad_w\n",
            "[[-0.77456274]\n",
            " [ 1.29232748]\n",
            " [ 0.22068328]] init_w\n",
            "[[-0.77456274]\n",
            " [ 1.29232748]\n",
            " [ 0.22068328]] forward_w\n",
            "[[-0.77456274]\n",
            " [ 1.29232748]\n",
            " [ 0.22068328]] loss_w\n",
            "1.1471414353135571e-05 Loss 379\n",
            "[[-0.77456274]\n",
            " [ 1.29232748]\n",
            " [ 0.22068328]] grad_w\n",
            "[[-0.77458687]\n",
            " [ 1.29234504]\n",
            " [ 0.22073667]] init_w\n",
            "[[-0.77458687]\n",
            " [ 1.29234504]\n",
            " [ 0.22073667]] forward_w\n",
            "[[-0.77458687]\n",
            " [ 1.29234504]\n",
            " [ 0.22073667]] loss_w\n",
            "1.1100339912727673e-05 Loss 380\n",
            "[[-0.77458687]\n",
            " [ 1.29234504]\n",
            " [ 0.22073667]] grad_w\n",
            "[[-0.77461061]\n",
            " [ 1.29236231]\n",
            " [ 0.22078919]] init_w\n",
            "[[-0.77461061]\n",
            " [ 1.29236231]\n",
            " [ 0.22078919]] forward_w\n",
            "[[-0.77461061]\n",
            " [ 1.29236231]\n",
            " [ 0.22078919]] loss_w\n",
            "1.0741268895444601e-05 Loss 381\n",
            "[[-0.77461061]\n",
            " [ 1.29236231]\n",
            " [ 0.22078919]] grad_w\n",
            "[[-0.77463396]\n",
            " [ 1.2923793 ]\n",
            " [ 0.22084086]] init_w\n",
            "[[-0.77463396]\n",
            " [ 1.2923793 ]\n",
            " [ 0.22084086]] forward_w\n",
            "[[-0.77463396]\n",
            " [ 1.2923793 ]\n",
            " [ 0.22084086]] loss_w\n",
            "1.0393813017560004e-05 Loss 382\n",
            "[[-0.77463396]\n",
            " [ 1.2923793 ]\n",
            " [ 0.22084086]] grad_w\n",
            "[[-0.77465693]\n",
            " [ 1.29239601]\n",
            " [ 0.22089168]] init_w\n",
            "[[-0.77465693]\n",
            " [ 1.29239601]\n",
            " [ 0.22089168]] forward_w\n",
            "[[-0.77465693]\n",
            " [ 1.29239601]\n",
            " [ 0.22089168]] loss_w\n",
            "1.0057596555450558e-05 Loss 383\n",
            "[[-0.77465693]\n",
            " [ 1.29239601]\n",
            " [ 0.22089168]] grad_w\n",
            "[[-0.77467952]\n",
            " [ 1.29241245]\n",
            " [ 0.22094167]] init_w\n",
            "[[-0.77467952]\n",
            " [ 1.29241245]\n",
            " [ 0.22094167]] forward_w\n",
            "[[-0.77467952]\n",
            " [ 1.29241245]\n",
            " [ 0.22094167]] loss_w\n",
            "9.732255939309093e-06 Loss 384\n",
            "[[-0.77467952]\n",
            " [ 1.29241245]\n",
            " [ 0.22094167]] grad_w\n",
            "[[-0.77470175]\n",
            " [ 1.29242862]\n",
            " [ 0.22099085]] init_w\n",
            "[[-0.77470175]\n",
            " [ 1.29242862]\n",
            " [ 0.22099085]] forward_w\n",
            "[[-0.77470175]\n",
            " [ 1.29242862]\n",
            " [ 0.22099085]] loss_w\n",
            "9.417439359991387e-06 Loss 385\n",
            "[[-0.77470175]\n",
            " [ 1.29242862]\n",
            " [ 0.22099085]] grad_w\n",
            "[[-0.77472361]\n",
            " [ 1.29244453]\n",
            " [ 0.22103923]] init_w\n",
            "[[-0.77472361]\n",
            " [ 1.29244453]\n",
            " [ 0.22103923]] forward_w\n",
            "[[-0.77472361]\n",
            " [ 1.29244453]\n",
            " [ 0.22103923]] loss_w\n",
            "9.112806388588835e-06 Loss 386\n",
            "[[-0.77472361]\n",
            " [ 1.29244453]\n",
            " [ 0.22103923]] grad_w\n",
            "[[-0.77474512]\n",
            " [ 1.29246018]\n",
            " [ 0.22108682]] init_w\n",
            "[[-0.77474512]\n",
            " [ 1.29246018]\n",
            " [ 0.22108682]] forward_w\n",
            "[[-0.77474512]\n",
            " [ 1.29246018]\n",
            " [ 0.22108682]] loss_w\n",
            "8.81802760830139e-06 Loss 387\n",
            "[[-0.77474512]\n",
            " [ 1.29246018]\n",
            " [ 0.22108682]] grad_w\n",
            "[[-0.77476627]\n",
            " [ 1.29247557]\n",
            " [ 0.22113363]] init_w\n",
            "[[-0.77476627]\n",
            " [ 1.29247557]\n",
            " [ 0.22113363]] forward_w\n",
            "[[-0.77476627]\n",
            " [ 1.29247557]\n",
            " [ 0.22113363]] loss_w\n",
            "8.53278425822043e-06 Loss 388\n",
            "[[-0.77476627]\n",
            " [ 1.29247557]\n",
            " [ 0.22113363]] grad_w\n",
            "[[-0.77478708]\n",
            " [ 1.29249071]\n",
            " [ 0.22117968]] init_w\n",
            "[[-0.77478708]\n",
            " [ 1.29249071]\n",
            " [ 0.22117968]] forward_w\n",
            "[[-0.77478708]\n",
            " [ 1.29249071]\n",
            " [ 0.22117968]] loss_w\n",
            "8.256767888636546e-06 Loss 389\n",
            "[[-0.77478708]\n",
            " [ 1.29249071]\n",
            " [ 0.22117968]] grad_w\n",
            "[[-0.77480756]\n",
            " [ 1.29250561]\n",
            " [ 0.22122498]] init_w\n",
            "[[-0.77480756]\n",
            " [ 1.29250561]\n",
            " [ 0.22122498]] forward_w\n",
            "[[-0.77480756]\n",
            " [ 1.29250561]\n",
            " [ 0.22122498]] loss_w\n",
            "7.989680027494669e-06 Loss 390\n",
            "[[-0.77480756]\n",
            " [ 1.29250561]\n",
            " [ 0.22122498]] grad_w\n",
            "[[-0.77482769]\n",
            " [ 1.29252026]\n",
            " [ 0.22126953]] init_w\n",
            "[[-0.77482769]\n",
            " [ 1.29252026]\n",
            " [ 0.22126953]] forward_w\n",
            "[[-0.77482769]\n",
            " [ 1.29252026]\n",
            " [ 0.22126953]] loss_w\n",
            "7.731231857636959e-06 Loss 391\n",
            "[[-0.77482769]\n",
            " [ 1.29252026]\n",
            " [ 0.22126953]] grad_w\n",
            "[[-0.7748475 ]\n",
            " [ 1.29253468]\n",
            " [ 0.22131337]] init_w\n",
            "[[-0.7748475 ]\n",
            " [ 1.29253468]\n",
            " [ 0.22131337]] forward_w\n",
            "[[-0.7748475 ]\n",
            " [ 1.29253468]\n",
            " [ 0.22131337]] loss_w\n",
            "7.4811439044933826e-06 Loss 392\n",
            "[[-0.7748475 ]\n",
            " [ 1.29253468]\n",
            " [ 0.22131337]] grad_w\n",
            "[[-0.77486699]\n",
            " [ 1.29254885]\n",
            " [ 0.22135648]] init_w\n",
            "[[-0.77486699]\n",
            " [ 1.29254885]\n",
            " [ 0.22135648]] forward_w\n",
            "[[-0.77486699]\n",
            " [ 1.29254885]\n",
            " [ 0.22135648]] loss_w\n",
            "7.2391457338657715e-06 Loss 393\n",
            "[[-0.77486699]\n",
            " [ 1.29254885]\n",
            " [ 0.22135648]] grad_w\n",
            "[[-0.77488616]\n",
            " [ 1.2925628 ]\n",
            " [ 0.2213989 ]] init_w\n",
            "[[-0.77488616]\n",
            " [ 1.2925628 ]\n",
            " [ 0.2213989 ]] forward_w\n",
            "[[-0.77488616]\n",
            " [ 1.2925628 ]\n",
            " [ 0.2213989 ]] loss_w\n",
            "7.0049756594932135e-06 Loss 394\n",
            "[[-0.77488616]\n",
            " [ 1.2925628 ]\n",
            " [ 0.2213989 ]] grad_w\n",
            "[[-0.77490501]\n",
            " [ 1.29257652]\n",
            " [ 0.22144062]] init_w\n",
            "[[-0.77490501]\n",
            " [ 1.29257652]\n",
            " [ 0.22144062]] forward_w\n",
            "[[-0.77490501]\n",
            " [ 1.29257652]\n",
            " [ 0.22144062]] loss_w\n",
            "6.77838046007854e-06 Loss 395\n",
            "[[-0.77490501]\n",
            " [ 1.29257652]\n",
            " [ 0.22144062]] grad_w\n",
            "[[-0.77492356]\n",
            " [ 1.29259002]\n",
            " [ 0.22148166]] init_w\n",
            "[[-0.77492356]\n",
            " [ 1.29259002]\n",
            " [ 0.22148166]] forward_w\n",
            "[[-0.77492356]\n",
            " [ 1.29259002]\n",
            " [ 0.22148166]] loss_w\n",
            "6.559115105461363e-06 Loss 396\n",
            "[[-0.77492356]\n",
            " [ 1.29259002]\n",
            " [ 0.22148166]] grad_w\n",
            "[[-0.77494181]\n",
            " [ 1.29260329]\n",
            " [ 0.22152204]] init_w\n",
            "[[-0.77494181]\n",
            " [ 1.29260329]\n",
            " [ 0.22152204]] forward_w\n",
            "[[-0.77494181]\n",
            " [ 1.29260329]\n",
            " [ 0.22152204]] loss_w\n",
            "6.34694249165778e-06 Loss 397\n",
            "[[-0.77494181]\n",
            " [ 1.29260329]\n",
            " [ 0.22152204]] grad_w\n",
            "[[-0.77495976]\n",
            " [ 1.29261635]\n",
            " [ 0.22156175]] init_w\n",
            "[[-0.77495976]\n",
            " [ 1.29261635]\n",
            " [ 0.22156175]] forward_w\n",
            "[[-0.77495976]\n",
            " [ 1.29261635]\n",
            " [ 0.22156175]] loss_w\n",
            "6.141633184462171e-06 Loss 398\n",
            "[[-0.77495976]\n",
            " [ 1.29261635]\n",
            " [ 0.22156175]] grad_w\n",
            "[[-0.77497741]\n",
            " [ 1.2926292 ]\n",
            " [ 0.22160082]] init_w\n",
            "[[-0.77497741]\n",
            " [ 1.2926292 ]\n",
            " [ 0.22160082]] forward_w\n",
            "[[-0.77497741]\n",
            " [ 1.2926292 ]\n",
            " [ 0.22160082]] loss_w\n",
            "5.94296517135091e-06 Loss 399\n",
            "[[-0.77497741]\n",
            " [ 1.2926292 ]\n",
            " [ 0.22160082]] grad_w\n",
            "[[-0.77499478]\n",
            " [ 1.29264184]\n",
            " [ 0.22163925]] init_w\n",
            "[[-0.77499478]\n",
            " [ 1.29264184]\n",
            " [ 0.22163925]] forward_w\n",
            "[[-0.77499478]\n",
            " [ 1.29264184]\n",
            " [ 0.22163925]] loss_w\n",
            "5.750723621404271e-06 Loss 400\n",
            "[[-0.77499478]\n",
            " [ 1.29264184]\n",
            " [ 0.22163925]] grad_w\n",
            "[[-0.77501187]\n",
            " [ 1.29265427]\n",
            " [ 0.22167705]] init_w\n",
            "[[-0.77501187]\n",
            " [ 1.29265427]\n",
            " [ 0.22167705]] forward_w\n",
            "[[-0.77501187]\n",
            " [ 1.29265427]\n",
            " [ 0.22167705]] loss_w\n",
            "5.564700653000983e-06 Loss 401\n",
            "[[-0.77501187]\n",
            " [ 1.29265427]\n",
            " [ 0.22167705]] grad_w\n",
            "[[-0.77502867]\n",
            " [ 1.2926665 ]\n",
            " [ 0.22171424]] init_w\n",
            "[[-0.77502867]\n",
            " [ 1.2926665 ]\n",
            " [ 0.22171424]] forward_w\n",
            "[[-0.77502867]\n",
            " [ 1.2926665 ]\n",
            " [ 0.22171424]] loss_w\n",
            "5.384695109021195e-06 Loss 402\n",
            "[[-0.77502867]\n",
            " [ 1.2926665 ]\n",
            " [ 0.22171424]] grad_w\n",
            "[[-0.77504521]\n",
            " [ 1.29267853]\n",
            " [ 0.22175082]] init_w\n",
            "[[-0.77504521]\n",
            " [ 1.29267853]\n",
            " [ 0.22175082]] forward_w\n",
            "[[-0.77504521]\n",
            " [ 1.29267853]\n",
            " [ 0.22175082]] loss_w\n",
            "5.210512339326247e-06 Loss 403\n",
            "[[-0.77504521]\n",
            " [ 1.29267853]\n",
            " [ 0.22175082]] grad_w\n",
            "[[-0.77506147]\n",
            " [ 1.29269036]\n",
            " [ 0.2217868 ]] init_w\n",
            "[[-0.77506147]\n",
            " [ 1.29269036]\n",
            " [ 0.2217868 ]] forward_w\n",
            "[[-0.77506147]\n",
            " [ 1.29269036]\n",
            " [ 0.2217868 ]] loss_w\n",
            "5.041963990270762e-06 Loss 404\n",
            "[[-0.77506147]\n",
            " [ 1.29269036]\n",
            " [ 0.2217868 ]] grad_w\n",
            "[[-0.77507747]\n",
            " [ 1.292702  ]\n",
            " [ 0.2218222 ]] init_w\n",
            "[[-0.77507747]\n",
            " [ 1.292702  ]\n",
            " [ 0.2218222 ]] forward_w\n",
            "[[-0.77507747]\n",
            " [ 1.292702  ]\n",
            " [ 0.2218222 ]] loss_w\n",
            "4.8788678010254326e-06 Loss 405\n",
            "[[-0.77507747]\n",
            " [ 1.292702  ]\n",
            " [ 0.2218222 ]] grad_w\n",
            "[[-0.7750932 ]\n",
            " [ 1.29271345]\n",
            " [ 0.22185702]] init_w\n",
            "[[-0.7750932 ]\n",
            " [ 1.29271345]\n",
            " [ 0.22185702]] forward_w\n",
            "[[-0.7750932 ]\n",
            " [ 1.29271345]\n",
            " [ 0.22185702]] loss_w\n",
            "4.7210474064902944e-06 Loss 406\n",
            "[[-0.7750932 ]\n",
            " [ 1.29271345]\n",
            " [ 0.22185702]] grad_w\n",
            "[[-0.77510868]\n",
            " [ 1.29272471]\n",
            " [ 0.22189127]] init_w\n",
            "[[-0.77510868]\n",
            " [ 1.29272471]\n",
            " [ 0.22189127]] forward_w\n",
            "[[-0.77510868]\n",
            " [ 1.29272471]\n",
            " [ 0.22189127]] loss_w\n",
            "4.568332146577925e-06 Loss 407\n",
            "[[-0.77510868]\n",
            " [ 1.29272471]\n",
            " [ 0.22189127]] grad_w\n",
            "[[-0.77512391]\n",
            " [ 1.29273579]\n",
            " [ 0.22192497]] init_w\n",
            "[[-0.77512391]\n",
            " [ 1.29273579]\n",
            " [ 0.22192497]] forward_w\n",
            "[[-0.77512391]\n",
            " [ 1.29273579]\n",
            " [ 0.22192497]] loss_w\n",
            "4.420556881671001e-06 Loss 408\n",
            "[[-0.77512391]\n",
            " [ 1.29273579]\n",
            " [ 0.22192497]] grad_w\n",
            "[[-0.77513889]\n",
            " [ 1.29274669]\n",
            " [ 0.22195811]] init_w\n",
            "[[-0.77513889]\n",
            " [ 1.29274669]\n",
            " [ 0.22195811]] forward_w\n",
            "[[-0.77513889]\n",
            " [ 1.29274669]\n",
            " [ 0.22195811]] loss_w\n",
            "4.27756181404813e-06 Loss 409\n",
            "[[-0.77513889]\n",
            " [ 1.29274669]\n",
            " [ 0.22195811]] grad_w\n",
            "[[-0.77515362]\n",
            " [ 1.29275741]\n",
            " [ 0.22199071]] init_w\n",
            "[[-0.77515362]\n",
            " [ 1.29275741]\n",
            " [ 0.22199071]] forward_w\n",
            "[[-0.77515362]\n",
            " [ 1.29275741]\n",
            " [ 0.22199071]] loss_w\n",
            "4.139192315083403e-06 Loss 410\n",
            "[[-0.77515362]\n",
            " [ 1.29275741]\n",
            " [ 0.22199071]] grad_w\n",
            "[[-0.77516812]\n",
            " [ 1.29276796]\n",
            " [ 0.22202279]] init_w\n",
            "[[-0.77516812]\n",
            " [ 1.29276796]\n",
            " [ 0.22202279]] forward_w\n",
            "[[-0.77516812]\n",
            " [ 1.29276796]\n",
            " [ 0.22202279]] loss_w\n",
            "4.00529875803998e-06 Loss 411\n",
            "[[-0.77516812]\n",
            " [ 1.29276796]\n",
            " [ 0.22202279]] grad_w\n",
            "[[-0.77518238]\n",
            " [ 1.29277833]\n",
            " [ 0.22205434]] init_w\n",
            "[[-0.77518238]\n",
            " [ 1.29277833]\n",
            " [ 0.22205434]] forward_w\n",
            "[[-0.77518238]\n",
            " [ 1.29277833]\n",
            " [ 0.22205434]] loss_w\n",
            "3.875736356268469e-06 Loss 412\n",
            "[[-0.77518238]\n",
            " [ 1.29277833]\n",
            " [ 0.22205434]] grad_w\n",
            "[[-0.7751964 ]\n",
            " [ 1.29278854]\n",
            " [ 0.22208537]] init_w\n",
            "[[-0.7751964 ]\n",
            " [ 1.29278854]\n",
            " [ 0.22208537]] forward_w\n",
            "[[-0.7751964 ]\n",
            " [ 1.29278854]\n",
            " [ 0.22208537]] loss_w\n",
            "3.7503650066428514e-06 Loss 413\n",
            "[[-0.7751964 ]\n",
            " [ 1.29278854]\n",
            " [ 0.22208537]] grad_w\n",
            "[[-0.7752102 ]\n",
            " [ 1.29279858]\n",
            " [ 0.2221159 ]] init_w\n",
            "[[-0.7752102 ]\n",
            " [ 1.29279858]\n",
            " [ 0.2221159 ]] forward_w\n",
            "[[-0.7752102 ]\n",
            " [ 1.29279858]\n",
            " [ 0.2221159 ]] loss_w\n",
            "3.629049138056231e-06 Loss 414\n",
            "[[-0.7752102 ]\n",
            " [ 1.29279858]\n",
            " [ 0.2221159 ]] grad_w\n",
            "[[-0.77522377]\n",
            " [ 1.29280845]\n",
            " [ 0.22214593]] init_w\n",
            "[[-0.77522377]\n",
            " [ 1.29280845]\n",
            " [ 0.22214593]] forward_w\n",
            "[[-0.77522377]\n",
            " [ 1.29280845]\n",
            " [ 0.22214593]] loss_w\n",
            "3.511657564823456e-06 Loss 415\n",
            "[[-0.77522377]\n",
            " [ 1.29280845]\n",
            " [ 0.22214593]] grad_w\n",
            "[[-0.77523712]\n",
            " [ 1.29281817]\n",
            " [ 0.22217547]] init_w\n",
            "[[-0.77523712]\n",
            " [ 1.29281817]\n",
            " [ 0.22217547]] forward_w\n",
            "[[-0.77523712]\n",
            " [ 1.29281817]\n",
            " [ 0.22217547]] loss_w\n",
            "3.3980633448208327e-06 Loss 416\n",
            "[[-0.77523712]\n",
            " [ 1.29281817]\n",
            " [ 0.22217547]] grad_w\n",
            "[[-0.77525026]\n",
            " [ 1.29282772]\n",
            " [ 0.22220453]] init_w\n",
            "[[-0.77525026]\n",
            " [ 1.29282772]\n",
            " [ 0.22220453]] forward_w\n",
            "[[-0.77525026]\n",
            " [ 1.29282772]\n",
            " [ 0.22220453]] loss_w\n",
            "3.2881436422165444e-06 Loss 417\n",
            "[[-0.77525026]\n",
            " [ 1.29282772]\n",
            " [ 0.22220453]] grad_w\n",
            "[[-0.77526318]\n",
            " [ 1.29283712]\n",
            " [ 0.22223312]] init_w\n",
            "[[-0.77526318]\n",
            " [ 1.29283712]\n",
            " [ 0.22223312]] forward_w\n",
            "[[-0.77526318]\n",
            " [ 1.29283712]\n",
            " [ 0.22223312]] loss_w\n",
            "3.181779594640672e-06 Loss 418\n",
            "[[-0.77526318]\n",
            " [ 1.29283712]\n",
            " [ 0.22223312]] grad_w\n",
            "[[-0.77527588]\n",
            " [ 1.29284637]\n",
            " [ 0.22226123]] init_w\n",
            "[[-0.77527588]\n",
            " [ 1.29284637]\n",
            " [ 0.22226123]] forward_w\n",
            "[[-0.77527588]\n",
            " [ 1.29284637]\n",
            " [ 0.22226123]] loss_w\n",
            "3.078856184655208e-06 Loss 419\n",
            "[[-0.77527588]\n",
            " [ 1.29284637]\n",
            " [ 0.22226123]] grad_w\n",
            "[[-0.77528838]\n",
            " [ 1.29285546]\n",
            " [ 0.2222889 ]] init_w\n",
            "[[-0.77528838]\n",
            " [ 1.29285546]\n",
            " [ 0.2222889 ]] forward_w\n",
            "[[-0.77528838]\n",
            " [ 1.29285546]\n",
            " [ 0.2222889 ]] loss_w\n",
            "2.979262115375747e-06 Loss 420\n",
            "[[-0.77528838]\n",
            " [ 1.29285546]\n",
            " [ 0.2222889 ]] grad_w\n",
            "[[-0.77530068]\n",
            " [ 1.29286441]\n",
            " [ 0.22231611]] init_w\n",
            "[[-0.77530068]\n",
            " [ 1.29286441]\n",
            " [ 0.22231611]] forward_w\n",
            "[[-0.77530068]\n",
            " [ 1.29286441]\n",
            " [ 0.22231611]] loss_w\n",
            "2.882889690122842e-06 Loss 421\n",
            "[[-0.77530068]\n",
            " [ 1.29286441]\n",
            " [ 0.22231611]] grad_w\n",
            "[[-0.77531278]\n",
            " [ 1.29287321]\n",
            " [ 0.22234287]] init_w\n",
            "[[-0.77531278]\n",
            " [ 1.29287321]\n",
            " [ 0.22234287]] forward_w\n",
            "[[-0.77531278]\n",
            " [ 1.29287321]\n",
            " [ 0.22234287]] loss_w\n",
            "2.789634695961866e-06 Loss 422\n",
            "[[-0.77531278]\n",
            " [ 1.29287321]\n",
            " [ 0.22234287]] grad_w\n",
            "[[-0.77532468]\n",
            " [ 1.29288187]\n",
            " [ 0.2223692 ]] init_w\n",
            "[[-0.77532468]\n",
            " [ 1.29288187]\n",
            " [ 0.2223692 ]] forward_w\n",
            "[[-0.77532468]\n",
            " [ 1.29288187]\n",
            " [ 0.2223692 ]] loss_w\n",
            "2.6993962910119594e-06 Loss 423\n",
            "[[-0.77532468]\n",
            " [ 1.29288187]\n",
            " [ 0.2223692 ]] grad_w\n",
            "[[-0.77533638]\n",
            " [ 1.29289039]\n",
            " [ 0.2223951 ]] init_w\n",
            "[[-0.77533638]\n",
            " [ 1.29289039]\n",
            " [ 0.2223951 ]] forward_w\n",
            "[[-0.77533638]\n",
            " [ 1.29289039]\n",
            " [ 0.2223951 ]] loss_w\n",
            "2.612076895400022e-06 Loss 424\n",
            "[[-0.77533638]\n",
            " [ 1.29289039]\n",
            " [ 0.2223951 ]] grad_w\n",
            "[[-0.7753479 ]\n",
            " [ 1.29289877]\n",
            " [ 0.22242058]] init_w\n",
            "[[-0.7753479 ]\n",
            " [ 1.29289877]\n",
            " [ 0.22242058]] forward_w\n",
            "[[-0.7753479 ]\n",
            " [ 1.29289877]\n",
            " [ 0.22242058]] loss_w\n",
            "2.527582085743378e-06 Loss 425\n",
            "[[-0.7753479 ]\n",
            " [ 1.29289877]\n",
            " [ 0.22242058]] grad_w\n",
            "[[-0.77535922]\n",
            " [ 1.29290701]\n",
            " [ 0.22244564]] init_w\n",
            "[[-0.77535922]\n",
            " [ 1.29290701]\n",
            " [ 0.22244564]] forward_w\n",
            "[[-0.77535922]\n",
            " [ 1.29290701]\n",
            " [ 0.22244564]] loss_w\n",
            "2.445820493041648e-06 Loss 426\n",
            "[[-0.77535922]\n",
            " [ 1.29290701]\n",
            " [ 0.22244564]] grad_w\n",
            "[[-0.77537037]\n",
            " [ 1.29291511]\n",
            " [ 0.22247029]] init_w\n",
            "[[-0.77537037]\n",
            " [ 1.29291511]\n",
            " [ 0.22247029]] forward_w\n",
            "[[-0.77537037]\n",
            " [ 1.29291511]\n",
            " [ 0.22247029]] loss_w\n",
            "2.3667037038774488e-06 Loss 427\n",
            "[[-0.77537037]\n",
            " [ 1.29291511]\n",
            " [ 0.22247029]] grad_w\n",
            "[[-0.77538133]\n",
            " [ 1.29292309]\n",
            " [ 0.22249455]] init_w\n",
            "[[-0.77538133]\n",
            " [ 1.29292309]\n",
            " [ 0.22249455]] forward_w\n",
            "[[-0.77538133]\n",
            " [ 1.29292309]\n",
            " [ 0.22249455]] loss_w\n",
            "2.2901461648075668e-06 Loss 428\n",
            "[[-0.77538133]\n",
            " [ 1.29292309]\n",
            " [ 0.22249455]] grad_w\n",
            "[[-0.77539211]\n",
            " [ 1.29293093]\n",
            " [ 0.2225184 ]] init_w\n",
            "[[-0.77539211]\n",
            " [ 1.29293093]\n",
            " [ 0.2225184 ]] forward_w\n",
            "[[-0.77539211]\n",
            " [ 1.29293093]\n",
            " [ 0.2225184 ]] loss_w\n",
            "2.216065089850324e-06 Loss 429\n",
            "[[-0.77539211]\n",
            " [ 1.29293093]\n",
            " [ 0.2225184 ]] grad_w\n",
            "[[-0.77540271]\n",
            " [ 1.29293865]\n",
            " [ 0.22254187]] init_w\n",
            "[[-0.77540271]\n",
            " [ 1.29293865]\n",
            " [ 0.22254187]] forward_w\n",
            "[[-0.77540271]\n",
            " [ 1.29293865]\n",
            " [ 0.22254187]] loss_w\n",
            "2.1443803709647653e-06 Loss 430\n",
            "[[-0.77540271]\n",
            " [ 1.29293865]\n",
            " [ 0.22254187]] grad_w\n",
            "[[-0.77541315]\n",
            " [ 1.29294624]\n",
            " [ 0.22256495]] init_w\n",
            "[[-0.77541315]\n",
            " [ 1.29294624]\n",
            " [ 0.22256495]] forward_w\n",
            "[[-0.77541315]\n",
            " [ 1.29294624]\n",
            " [ 0.22256495]] loss_w\n",
            "2.0750144914241994e-06 Loss 431\n",
            "[[-0.77541315]\n",
            " [ 1.29294624]\n",
            " [ 0.22256495]] grad_w\n",
            "[[-0.77542341]\n",
            " [ 1.29295371]\n",
            " [ 0.22258766]] init_w\n",
            "[[-0.77542341]\n",
            " [ 1.29295371]\n",
            " [ 0.22258766]] forward_w\n",
            "[[-0.77542341]\n",
            " [ 1.29295371]\n",
            " [ 0.22258766]] loss_w\n",
            "2.0078924419942086e-06 Loss 432\n",
            "[[-0.77542341]\n",
            " [ 1.29295371]\n",
            " [ 0.22258766]] grad_w\n",
            "[[-0.77543351]\n",
            " [ 1.29296105]\n",
            " [ 0.22261   ]] init_w\n",
            "[[-0.77543351]\n",
            " [ 1.29296105]\n",
            " [ 0.22261   ]] forward_w\n",
            "[[-0.77543351]\n",
            " [ 1.29296105]\n",
            " [ 0.22261   ]] loss_w\n",
            "1.9429416398196725e-06 Loss 433\n",
            "[[-0.77543351]\n",
            " [ 1.29296105]\n",
            " [ 0.22261   ]] grad_w\n",
            "[[-0.77544344]\n",
            " [ 1.29296828]\n",
            " [ 0.22263197]] init_w\n",
            "[[-0.77544344]\n",
            " [ 1.29296828]\n",
            " [ 0.22263197]] forward_w\n",
            "[[-0.77544344]\n",
            " [ 1.29296828]\n",
            " [ 0.22263197]] loss_w\n",
            "1.8800918499377878e-06 Loss 434\n",
            "[[-0.77544344]\n",
            " [ 1.29296828]\n",
            " [ 0.22263197]] grad_w\n",
            "[[-0.7754532 ]\n",
            " [ 1.29297539]\n",
            " [ 0.22265359]] init_w\n",
            "[[-0.7754532 ]\n",
            " [ 1.29297539]\n",
            " [ 0.22265359]] forward_w\n",
            "[[-0.7754532 ]\n",
            " [ 1.29297539]\n",
            " [ 0.22265359]] loss_w\n",
            "1.819275109329144e-06 Loss 435\n",
            "[[-0.7754532 ]\n",
            " [ 1.29297539]\n",
            " [ 0.22265359]] grad_w\n",
            "[[-0.77546281]\n",
            " [ 1.29298238]\n",
            " [ 0.22267485]] init_w\n",
            "[[-0.77546281]\n",
            " [ 1.29298238]\n",
            " [ 0.22267485]] forward_w\n",
            "[[-0.77546281]\n",
            " [ 1.29298238]\n",
            " [ 0.22267485]] loss_w\n",
            "1.7604256534241154e-06 Loss 436\n",
            "[[-0.77546281]\n",
            " [ 1.29298238]\n",
            " [ 0.22267485]] grad_w\n",
            "[[-0.77547227]\n",
            " [ 1.29298926]\n",
            " [ 0.22269577]] init_w\n",
            "[[-0.77547227]\n",
            " [ 1.29298926]\n",
            " [ 0.22269577]] forward_w\n",
            "[[-0.77547227]\n",
            " [ 1.29298926]\n",
            " [ 0.22269577]] loss_w\n",
            "1.703479844989827e-06 Loss 437\n",
            "[[-0.77547227]\n",
            " [ 1.29298926]\n",
            " [ 0.22269577]] grad_w\n",
            "[[-0.77548157]\n",
            " [ 1.29299602]\n",
            " [ 0.22271634]] init_w\n",
            "[[-0.77548157]\n",
            " [ 1.29299602]\n",
            " [ 0.22271634]] forward_w\n",
            "[[-0.77548157]\n",
            " [ 1.29299602]\n",
            " [ 0.22271634]] loss_w\n",
            "1.648376105314217e-06 Loss 438\n",
            "[[-0.77548157]\n",
            " [ 1.29299602]\n",
            " [ 0.22271634]] grad_w\n",
            "[[-0.77549071]\n",
            " [ 1.29300268]\n",
            " [ 0.22273658]] init_w\n",
            "[[-0.77549071]\n",
            " [ 1.29300268]\n",
            " [ 0.22273658]] forward_w\n",
            "[[-0.77549071]\n",
            " [ 1.29300268]\n",
            " [ 0.22273658]] loss_w\n",
            "1.5950548476181327e-06 Loss 439\n",
            "[[-0.77549071]\n",
            " [ 1.29300268]\n",
            " [ 0.22273658]] grad_w\n",
            "[[-0.77549971]\n",
            " [ 1.29300923]\n",
            " [ 0.22275649]] init_w\n",
            "[[-0.77549971]\n",
            " [ 1.29300923]\n",
            " [ 0.22275649]] forward_w\n",
            "[[-0.77549971]\n",
            " [ 1.29300923]\n",
            " [ 0.22275649]] loss_w\n",
            "1.5434584126206204e-06 Loss 440\n",
            "[[-0.77549971]\n",
            " [ 1.29300923]\n",
            " [ 0.22275649]] grad_w\n",
            "[[-0.77550856]\n",
            " [ 1.29301567]\n",
            " [ 0.22277608]] init_w\n",
            "[[-0.77550856]\n",
            " [ 1.29301567]\n",
            " [ 0.22277608]] forward_w\n",
            "[[-0.77550856]\n",
            " [ 1.29301567]\n",
            " [ 0.22277608]] loss_w\n",
            "1.493531006188944e-06 Loss 441\n",
            "[[-0.77550856]\n",
            " [ 1.29301567]\n",
            " [ 0.22277608]] grad_w\n",
            "[[-0.77551727]\n",
            " [ 1.293022  ]\n",
            " [ 0.22279534]] init_w\n",
            "[[-0.77551727]\n",
            " [ 1.293022  ]\n",
            " [ 0.22279534]] forward_w\n",
            "[[-0.77551727]\n",
            " [ 1.293022  ]\n",
            " [ 0.22279534]] loss_w\n",
            "1.4452186390044854e-06 Loss 442\n",
            "[[-0.77551727]\n",
            " [ 1.293022  ]\n",
            " [ 0.22279534]] grad_w\n",
            "[[-0.77552583]\n",
            " [ 1.29302823]\n",
            " [ 0.22281429]] init_w\n",
            "[[-0.77552583]\n",
            " [ 1.29302823]\n",
            " [ 0.22281429]] forward_w\n",
            "[[-0.77552583]\n",
            " [ 1.29302823]\n",
            " [ 0.22281429]] loss_w\n",
            "1.3984690681818917e-06 Loss 443\n",
            "[[-0.77552583]\n",
            " [ 1.29302823]\n",
            " [ 0.22281429]] grad_w\n",
            "[[-0.77553426]\n",
            " [ 1.29303436]\n",
            " [ 0.22283293]] init_w\n",
            "[[-0.77553426]\n",
            " [ 1.29303436]\n",
            " [ 0.22283293]] forward_w\n",
            "[[-0.77553426]\n",
            " [ 1.29303436]\n",
            " [ 0.22283293]] loss_w\n",
            "1.3532317407758576e-06 Loss 444\n",
            "[[-0.77553426]\n",
            " [ 1.29303436]\n",
            " [ 0.22283293]] grad_w\n",
            "[[-0.77554255]\n",
            " [ 1.29304039]\n",
            " [ 0.22285127]] init_w\n",
            "[[-0.77554255]\n",
            " [ 1.29304039]\n",
            " [ 0.22285127]] forward_w\n",
            "[[-0.77554255]\n",
            " [ 1.29304039]\n",
            " [ 0.22285127]] loss_w\n",
            "1.309457739114543e-06 Loss 445\n",
            "[[-0.77554255]\n",
            " [ 1.29304039]\n",
            " [ 0.22285127]] grad_w\n",
            "[[-0.7755507 ]\n",
            " [ 1.29304633]\n",
            " [ 0.22286931]] init_w\n",
            "[[-0.7755507 ]\n",
            " [ 1.29304633]\n",
            " [ 0.22286931]] forward_w\n",
            "[[-0.7755507 ]\n",
            " [ 1.29304633]\n",
            " [ 0.22286931]] loss_w\n",
            "1.2670997279029937e-06 Loss 446\n",
            "[[-0.7755507 ]\n",
            " [ 1.29304633]\n",
            " [ 0.22286931]] grad_w\n",
            "[[-0.77555872]\n",
            " [ 1.29305216]\n",
            " [ 0.22288706]] init_w\n",
            "[[-0.77555872]\n",
            " [ 1.29305216]\n",
            " [ 0.22288706]] forward_w\n",
            "[[-0.77555872]\n",
            " [ 1.29305216]\n",
            " [ 0.22288706]] loss_w\n",
            "1.2261119030368778e-06 Loss 447\n",
            "[[-0.77555872]\n",
            " [ 1.29305216]\n",
            " [ 0.22288706]] grad_w\n",
            "[[-0.77556661]\n",
            " [ 1.2930579 ]\n",
            " [ 0.22290451]] init_w\n",
            "[[-0.77556661]\n",
            " [ 1.2930579 ]\n",
            " [ 0.22290451]] forward_w\n",
            "[[-0.77556661]\n",
            " [ 1.2930579 ]\n",
            " [ 0.22290451]] loss_w\n",
            "1.1864499420711466e-06 Loss 448\n",
            "[[-0.77556661]\n",
            " [ 1.2930579 ]\n",
            " [ 0.22290451]] grad_w\n",
            "[[-0.77557437]\n",
            " [ 1.29306355]\n",
            " [ 0.22292168]] init_w\n",
            "[[-0.77557437]\n",
            " [ 1.29306355]\n",
            " [ 0.22292168]] forward_w\n",
            "[[-0.77557437]\n",
            " [ 1.29306355]\n",
            " [ 0.22292168]] loss_w\n",
            "1.1480709562918492e-06 Loss 449\n",
            "[[-0.77557437]\n",
            " [ 1.29306355]\n",
            " [ 0.22292168]] grad_w\n",
            "[[-0.775582  ]\n",
            " [ 1.2930691 ]\n",
            " [ 0.22293857]] init_w\n",
            "[[-0.775582  ]\n",
            " [ 1.2930691 ]\n",
            " [ 0.22293857]] forward_w\n",
            "[[-0.775582  ]\n",
            " [ 1.2930691 ]\n",
            " [ 0.22293857]] loss_w\n",
            "1.110933444339052e-06 Loss 450\n",
            "[[-0.775582  ]\n",
            " [ 1.2930691 ]\n",
            " [ 0.22293857]] grad_w\n",
            "[[-0.77558951]\n",
            " [ 1.29307457]\n",
            " [ 0.22295519]] init_w\n",
            "[[-0.77558951]\n",
            " [ 1.29307457]\n",
            " [ 0.22295519]] forward_w\n",
            "[[-0.77558951]\n",
            " [ 1.29307457]\n",
            " [ 0.22295519]] loss_w\n",
            "1.0749972473281058e-06 Loss 451\n",
            "[[-0.77558951]\n",
            " [ 1.29307457]\n",
            " [ 0.22295519]] grad_w\n",
            "[[-0.7755969 ]\n",
            " [ 1.29307994]\n",
            " [ 0.22297153]] init_w\n",
            "[[-0.7755969 ]\n",
            " [ 1.29307994]\n",
            " [ 0.22297153]] forward_w\n",
            "[[-0.7755969 ]\n",
            " [ 1.29307994]\n",
            " [ 0.22297153]] loss_w\n",
            "1.0402235054238397e-06 Loss 452\n",
            "[[-0.7755969 ]\n",
            " [ 1.29307994]\n",
            " [ 0.22297153]] grad_w\n",
            "[[-0.77560416]\n",
            " [ 1.29308523]\n",
            " [ 0.22298761]] init_w\n",
            "[[-0.77560416]\n",
            " [ 1.29308523]\n",
            " [ 0.22298761]] forward_w\n",
            "[[-0.77560416]\n",
            " [ 1.29308523]\n",
            " [ 0.22298761]] loss_w\n",
            "1.0065746158195401e-06 Loss 453\n",
            "[[-0.77560416]\n",
            " [ 1.29308523]\n",
            " [ 0.22298761]] grad_w\n",
            "[[-0.77561131]\n",
            " [ 1.29309043]\n",
            " [ 0.22300343]] init_w\n",
            "[[-0.77561131]\n",
            " [ 1.29309043]\n",
            " [ 0.22300343]] forward_w\n",
            "[[-0.77561131]\n",
            " [ 1.29309043]\n",
            " [ 0.22300343]] loss_w\n",
            "9.740141920743307e-07 Loss 454\n",
            "[[-0.77561131]\n",
            " [ 1.29309043]\n",
            " [ 0.22300343]] grad_w\n",
            "[[-0.77561834]\n",
            " [ 1.29309554]\n",
            " [ 0.22301899]] init_w\n",
            "[[-0.77561834]\n",
            " [ 1.29309554]\n",
            " [ 0.22301899]] forward_w\n",
            "[[-0.77561834]\n",
            " [ 1.29309554]\n",
            " [ 0.22301899]] loss_w\n",
            "9.425070247671155e-07 Loss 455\n",
            "[[-0.77561834]\n",
            " [ 1.29309554]\n",
            " [ 0.22301899]] grad_w\n",
            "[[-0.77562526]\n",
            " [ 1.29310058]\n",
            " [ 0.22303429]] init_w\n",
            "[[-0.77562526]\n",
            " [ 1.29310058]\n",
            " [ 0.22303429]] forward_w\n",
            "[[-0.77562526]\n",
            " [ 1.29310058]\n",
            " [ 0.22303429]] loss_w\n",
            "9.120190434223851e-07 Loss 456\n",
            "[[-0.77562526]\n",
            " [ 1.29310058]\n",
            " [ 0.22303429]] grad_w\n",
            "[[-0.77563206]\n",
            " [ 1.29310553]\n",
            " [ 0.22304934]] init_w\n",
            "[[-0.77563206]\n",
            " [ 1.29310553]\n",
            " [ 0.22304934]] forward_w\n",
            "[[-0.77563206]\n",
            " [ 1.29310553]\n",
            " [ 0.22304934]] loss_w\n",
            "8.825172796675665e-07 Loss 457\n",
            "[[-0.77563206]\n",
            " [ 1.29310553]\n",
            " [ 0.22304934]] grad_w\n",
            "[[-0.77563876]\n",
            " [ 1.2931104 ]\n",
            " [ 0.22306415]] init_w\n",
            "[[-0.77563876]\n",
            " [ 1.2931104 ]\n",
            " [ 0.22306415]] forward_w\n",
            "[[-0.77563876]\n",
            " [ 1.2931104 ]\n",
            " [ 0.22306415]] loss_w\n",
            "8.539698315829562e-07 Loss 458\n",
            "[[-0.77563876]\n",
            " [ 1.2931104 ]\n",
            " [ 0.22306415]] grad_w\n",
            "[[-0.77564534]\n",
            " [ 1.29311519]\n",
            " [ 0.22307872]] init_w\n",
            "[[-0.77564534]\n",
            " [ 1.29311519]\n",
            " [ 0.22307872]] forward_w\n",
            "[[-0.77564534]\n",
            " [ 1.29311519]\n",
            " [ 0.22307872]] loss_w\n",
            "8.263458292042173e-07 Loss 459\n",
            "[[-0.77564534]\n",
            " [ 1.29311519]\n",
            " [ 0.22307872]] grad_w\n",
            "[[-0.77565182]\n",
            " [ 1.2931199 ]\n",
            " [ 0.22309305]] init_w\n",
            "[[-0.77565182]\n",
            " [ 1.2931199 ]\n",
            " [ 0.22309305]] forward_w\n",
            "[[-0.77565182]\n",
            " [ 1.2931199 ]\n",
            " [ 0.22309305]] loss_w\n",
            "7.996154011402626e-07 Loss 460\n",
            "[[-0.77565182]\n",
            " [ 1.2931199 ]\n",
            " [ 0.22309305]] grad_w\n",
            "[[-0.77565819]\n",
            " [ 1.29312453]\n",
            " [ 0.22310715]] init_w\n",
            "[[-0.77565819]\n",
            " [ 1.29312453]\n",
            " [ 0.22310715]] forward_w\n",
            "[[-0.77565819]\n",
            " [ 1.29312453]\n",
            " [ 0.22310715]] loss_w\n",
            "7.737496422733911e-07 Loss 461\n",
            "[[-0.77565819]\n",
            " [ 1.29312453]\n",
            " [ 0.22310715]] grad_w\n",
            "[[-0.77566445]\n",
            " [ 1.29312909]\n",
            " [ 0.22312101]] init_w\n",
            "[[-0.77566445]\n",
            " [ 1.29312909]\n",
            " [ 0.22312101]] forward_w\n",
            "[[-0.77566445]\n",
            " [ 1.29312909]\n",
            " [ 0.22312101]] loss_w\n",
            "7.487205825008545e-07 Loss 462\n",
            "[[-0.77566445]\n",
            " [ 1.29312909]\n",
            " [ 0.22312101]] grad_w\n",
            "[[-0.77567062]\n",
            " [ 1.29313358]\n",
            " [ 0.22313466]] init_w\n",
            "[[-0.77567062]\n",
            " [ 1.29313358]\n",
            " [ 0.22313466]] forward_w\n",
            "[[-0.77567062]\n",
            " [ 1.29313358]\n",
            " [ 0.22313466]] loss_w\n",
            "7.245011564892005e-07 Loss 463\n",
            "[[-0.77567062]\n",
            " [ 1.29313358]\n",
            " [ 0.22313466]] grad_w\n",
            "[[-0.77567668]\n",
            " [ 1.29313799]\n",
            " [ 0.22314807]] init_w\n",
            "[[-0.77567668]\n",
            " [ 1.29313799]\n",
            " [ 0.22314807]] forward_w\n",
            "[[-0.77567668]\n",
            " [ 1.29313799]\n",
            " [ 0.22314807]] loss_w\n",
            "7.010651744086117e-07 Loss 464\n",
            "[[-0.77567668]\n",
            " [ 1.29313799]\n",
            " [ 0.22314807]] grad_w\n",
            "[[-0.77568265]\n",
            " [ 1.29314233]\n",
            " [ 0.22316127]] init_w\n",
            "[[-0.77568265]\n",
            " [ 1.29314233]\n",
            " [ 0.22316127]] forward_w\n",
            "[[-0.77568265]\n",
            " [ 1.29314233]\n",
            " [ 0.22316127]] loss_w\n",
            "6.783872936107465e-07 Loss 465\n",
            "[[-0.77568265]\n",
            " [ 1.29314233]\n",
            " [ 0.22316127]] grad_w\n",
            "[[-0.77568852]\n",
            " [ 1.2931466 ]\n",
            " [ 0.22317426]] init_w\n",
            "[[-0.77568852]\n",
            " [ 1.2931466 ]\n",
            " [ 0.22317426]] forward_w\n",
            "[[-0.77568852]\n",
            " [ 1.2931466 ]\n",
            " [ 0.22317426]] loss_w\n",
            "6.564429912250569e-07 Loss 466\n",
            "[[-0.77568852]\n",
            " [ 1.2931466 ]\n",
            " [ 0.22317426]] grad_w\n",
            "[[-0.77569429]\n",
            " [ 1.2931508 ]\n",
            " [ 0.22318703]] init_w\n",
            "[[-0.77569429]\n",
            " [ 1.2931508 ]\n",
            " [ 0.22318703]] forward_w\n",
            "[[-0.77569429]\n",
            " [ 1.2931508 ]\n",
            " [ 0.22318703]] loss_w\n",
            "6.352085376405834e-07 Loss 467\n",
            "[[-0.77569429]\n",
            " [ 1.2931508 ]\n",
            " [ 0.22318703]] grad_w\n",
            "[[-0.77569997]\n",
            " [ 1.29315493]\n",
            " [ 0.22319959]] init_w\n",
            "[[-0.77569997]\n",
            " [ 1.29315493]\n",
            " [ 0.22319959]] forward_w\n",
            "[[-0.77569997]\n",
            " [ 1.29315493]\n",
            " [ 0.22319959]] loss_w\n",
            "6.146609708462098e-07 Loss 468\n",
            "[[-0.77569997]\n",
            " [ 1.29315493]\n",
            " [ 0.22319959]] grad_w\n",
            "[[-0.77570555]\n",
            " [ 1.293159  ]\n",
            " [ 0.22321195]] init_w\n",
            "[[-0.77570555]\n",
            " [ 1.293159  ]\n",
            " [ 0.22321195]] forward_w\n",
            "[[-0.77570555]\n",
            " [ 1.293159  ]\n",
            " [ 0.22321195]] loss_w\n",
            "5.947780715998181e-07 Loss 469\n",
            "[[-0.77570555]\n",
            " [ 1.293159  ]\n",
            " [ 0.22321195]] grad_w\n",
            "[[-0.77571105]\n",
            " [ 1.293163  ]\n",
            " [ 0.22322411]] init_w\n",
            "[[-0.77571105]\n",
            " [ 1.293163  ]\n",
            " [ 0.22322411]] forward_w\n",
            "[[-0.77571105]\n",
            " [ 1.293163  ]\n",
            " [ 0.22322411]] loss_w\n",
            "5.755383394017511e-07 Loss 470\n",
            "[[-0.77571105]\n",
            " [ 1.293163  ]\n",
            " [ 0.22322411]] grad_w\n",
            "[[-0.77571645]\n",
            " [ 1.29316693]\n",
            " [ 0.22323607]] init_w\n",
            "[[-0.77571645]\n",
            " [ 1.29316693]\n",
            " [ 0.22323607]] forward_w\n",
            "[[-0.77571645]\n",
            " [ 1.29316693]\n",
            " [ 0.22323607]] loss_w\n",
            "5.569209692457175e-07 Loss 471\n",
            "[[-0.77571645]\n",
            " [ 1.29316693]\n",
            " [ 0.22323607]] grad_w\n",
            "[[-0.77572177]\n",
            " [ 1.2931708 ]\n",
            " [ 0.22324783]] init_w\n",
            "[[-0.77572177]\n",
            " [ 1.2931708 ]\n",
            " [ 0.22324783]] forward_w\n",
            "[[-0.77572177]\n",
            " [ 1.2931708 ]\n",
            " [ 0.22324783]] loss_w\n",
            "5.389058291196968e-07 Loss 472\n",
            "[[-0.77572177]\n",
            " [ 1.2931708 ]\n",
            " [ 0.22324783]] grad_w\n",
            "[[-0.775727  ]\n",
            " [ 1.2931746 ]\n",
            " [ 0.22325941]] init_w\n",
            "[[-0.775727  ]\n",
            " [ 1.2931746 ]\n",
            " [ 0.22325941]] forward_w\n",
            "[[-0.775727  ]\n",
            " [ 1.2931746 ]\n",
            " [ 0.22325941]] loss_w\n",
            "5.214734382377433e-07 Loss 473\n",
            "[[-0.775727  ]\n",
            " [ 1.2931746 ]\n",
            " [ 0.22325941]] grad_w\n",
            "[[-0.77573214]\n",
            " [ 1.29317835]\n",
            " [ 0.22327079]] init_w\n",
            "[[-0.77573214]\n",
            " [ 1.29317835]\n",
            " [ 0.22327079]] forward_w\n",
            "[[-0.77573214]\n",
            " [ 1.29317835]\n",
            " [ 0.22327079]] loss_w\n",
            "5.046049459729808e-07 Loss 474\n",
            "[[-0.77573214]\n",
            " [ 1.29317835]\n",
            " [ 0.22327079]] grad_w\n",
            "[[-0.7757372 ]\n",
            " [ 1.29318203]\n",
            " [ 0.22328199]] init_w\n",
            "[[-0.7757372 ]\n",
            " [ 1.29318203]\n",
            " [ 0.22328199]] forward_w\n",
            "[[-0.7757372 ]\n",
            " [ 1.29318203]\n",
            " [ 0.22328199]] loss_w\n",
            "4.882821114740473e-07 Loss 475\n",
            "[[-0.7757372 ]\n",
            " [ 1.29318203]\n",
            " [ 0.22328199]] grad_w\n",
            "[[-0.77574218]\n",
            " [ 1.29318565]\n",
            " [ 0.223293  ]] init_w\n",
            "[[-0.77574218]\n",
            " [ 1.29318565]\n",
            " [ 0.223293  ]] forward_w\n",
            "[[-0.77574218]\n",
            " [ 1.29318565]\n",
            " [ 0.223293  ]] loss_w\n",
            "4.724872839402955e-07 Loss 476\n",
            "[[-0.77574218]\n",
            " [ 1.29318565]\n",
            " [ 0.223293  ]] grad_w\n",
            "[[-0.77574708]\n",
            " [ 1.29318921]\n",
            " [ 0.22330384]] init_w\n",
            "[[-0.77574708]\n",
            " [ 1.29318921]\n",
            " [ 0.22330384]] forward_w\n",
            "[[-0.77574708]\n",
            " [ 1.29318921]\n",
            " [ 0.22330384]] loss_w\n",
            "4.572033835344168e-07 Loss 477\n",
            "[[-0.77574708]\n",
            " [ 1.29318921]\n",
            " [ 0.22330384]] grad_w\n",
            "[[-0.7757519 ]\n",
            " [ 1.29319272]\n",
            " [ 0.2233145 ]] init_w\n",
            "[[-0.7757519 ]\n",
            " [ 1.29319272]\n",
            " [ 0.2233145 ]] forward_w\n",
            "[[-0.7757519 ]\n",
            " [ 1.29319272]\n",
            " [ 0.2233145 ]] loss_w\n",
            "4.424138829134959e-07 Loss 478\n",
            "[[-0.7757519 ]\n",
            " [ 1.29319272]\n",
            " [ 0.2233145 ]] grad_w\n",
            "[[-0.77575664]\n",
            " [ 1.29319617]\n",
            " [ 0.22332498]] init_w\n",
            "[[-0.77575664]\n",
            " [ 1.29319617]\n",
            " [ 0.22332498]] forward_w\n",
            "[[-0.77575664]\n",
            " [ 1.29319617]\n",
            " [ 0.22332498]] loss_w\n",
            "4.281027893571221e-07 Loss 479\n",
            "[[-0.77575664]\n",
            " [ 1.29319617]\n",
            " [ 0.22332498]] grad_w\n",
            "[[-0.7757613 ]\n",
            " [ 1.29319956]\n",
            " [ 0.2233353 ]] init_w\n",
            "[[-0.7757613 ]\n",
            " [ 1.29319956]\n",
            " [ 0.2233353 ]] forward_w\n",
            "[[-0.7757613 ]\n",
            " [ 1.29319956]\n",
            " [ 0.2233353 ]] loss_w\n",
            "4.14254627473338e-07 Loss 480\n",
            "[[-0.7757613 ]\n",
            " [ 1.29319956]\n",
            " [ 0.2233353 ]] grad_w\n",
            "[[-0.77576588]\n",
            " [ 1.2932029 ]\n",
            " [ 0.22334544]] init_w\n",
            "[[-0.77576588]\n",
            " [ 1.2932029 ]\n",
            " [ 0.22334544]] forward_w\n",
            "[[-0.77576588]\n",
            " [ 1.2932029 ]\n",
            " [ 0.22334544]] loss_w\n",
            "4.0085442246435653e-07 Loss 481\n",
            "[[-0.77576588]\n",
            " [ 1.2932029 ]\n",
            " [ 0.22334544]] grad_w\n",
            "[[-0.77577039]\n",
            " [ 1.29320618]\n",
            " [ 0.22335542]] init_w\n",
            "[[-0.77577039]\n",
            " [ 1.29320618]\n",
            " [ 0.22335542]] forward_w\n",
            "[[-0.77577039]\n",
            " [ 1.29320618]\n",
            " [ 0.22335542]] loss_w\n",
            "3.8788768393304885e-07 Loss 482\n",
            "[[-0.77577039]\n",
            " [ 1.29320618]\n",
            " [ 0.22335542]] grad_w\n",
            "[[-0.77577483]\n",
            " [ 1.29320941]\n",
            " [ 0.22336524]] init_w\n",
            "[[-0.77577483]\n",
            " [ 1.29320941]\n",
            " [ 0.22336524]] forward_w\n",
            "[[-0.77577483]\n",
            " [ 1.29320941]\n",
            " [ 0.22336524]] loss_w\n",
            "3.7534039021449194e-07 Loss 483\n",
            "[[-0.77577483]\n",
            " [ 1.29320941]\n",
            " [ 0.22336524]] grad_w\n",
            "[[-0.7757792 ]\n",
            " [ 1.29321258]\n",
            " [ 0.2233749 ]] init_w\n",
            "[[-0.7757792 ]\n",
            " [ 1.29321258]\n",
            " [ 0.2233749 ]] forward_w\n",
            "[[-0.7757792 ]\n",
            " [ 1.29321258]\n",
            " [ 0.2233749 ]] loss_w\n",
            "3.6319897321274323e-07 Loss 484\n",
            "[[-0.7757792 ]\n",
            " [ 1.29321258]\n",
            " [ 0.2233749 ]] grad_w\n",
            "[[-0.77578349]\n",
            " [ 1.29321571]\n",
            " [ 0.2233844 ]] init_w\n",
            "[[-0.77578349]\n",
            " [ 1.29321571]\n",
            " [ 0.2233844 ]] forward_w\n",
            "[[-0.77578349]\n",
            " [ 1.29321571]\n",
            " [ 0.2233844 ]] loss_w\n",
            "3.5145030372935733e-07 Loss 485\n",
            "[[-0.77578349]\n",
            " [ 1.29321571]\n",
            " [ 0.2233844 ]] grad_w\n",
            "[[-0.77578771]\n",
            " [ 1.29321878]\n",
            " [ 0.22339375]] init_w\n",
            "[[-0.77578771]\n",
            " [ 1.29321878]\n",
            " [ 0.22339375]] forward_w\n",
            "[[-0.77578771]\n",
            " [ 1.29321878]\n",
            " [ 0.22339375]] loss_w\n",
            "3.4008167726602156e-07 Loss 486\n",
            "[[-0.77578771]\n",
            " [ 1.29321878]\n",
            " [ 0.22339375]] grad_w\n",
            "[[-0.77579187]\n",
            " [ 1.2932218 ]\n",
            " [ 0.22340294]] init_w\n",
            "[[-0.77579187]\n",
            " [ 1.2932218 ]\n",
            " [ 0.22340294]] forward_w\n",
            "[[-0.77579187]\n",
            " [ 1.2932218 ]\n",
            " [ 0.22340294]] loss_w\n",
            "3.290808002860533e-07 Loss 487\n",
            "[[-0.77579187]\n",
            " [ 1.2932218 ]\n",
            " [ 0.22340294]] grad_w\n",
            "[[-0.77579595]\n",
            " [ 1.29322478]\n",
            " [ 0.22341198]] init_w\n",
            "[[-0.77579595]\n",
            " [ 1.29322478]\n",
            " [ 0.22341198]] forward_w\n",
            "[[-0.77579595]\n",
            " [ 1.29322478]\n",
            " [ 0.22341198]] loss_w\n",
            "3.1843577692144274e-07 Loss 488\n",
            "[[-0.77579595]\n",
            " [ 1.29322478]\n",
            " [ 0.22341198]] grad_w\n",
            "[[-0.77579997]\n",
            " [ 1.2932277 ]\n",
            " [ 0.22342088]] init_w\n",
            "[[-0.77579997]\n",
            " [ 1.2932277 ]\n",
            " [ 0.22342088]] forward_w\n",
            "[[-0.77579997]\n",
            " [ 1.2932277 ]\n",
            " [ 0.22342088]] loss_w\n",
            "3.081350961082932e-07 Loss 489\n",
            "[[-0.77579997]\n",
            " [ 1.2932277 ]\n",
            " [ 0.22342088]] grad_w\n",
            "[[-0.77580393]\n",
            " [ 1.29323058]\n",
            " [ 0.22342963]] init_w\n",
            "[[-0.77580393]\n",
            " [ 1.29323058]\n",
            " [ 0.22342963]] forward_w\n",
            "[[-0.77580393]\n",
            " [ 1.29323058]\n",
            " [ 0.22342963]] loss_w\n",
            "2.9816761913996295e-07 Loss 490\n",
            "[[-0.77580393]\n",
            " [ 1.29323058]\n",
            " [ 0.22342963]] grad_w\n",
            "[[-0.77580782]\n",
            " [ 1.29323341]\n",
            " [ 0.22343824]] init_w\n",
            "[[-0.77580782]\n",
            " [ 1.29323341]\n",
            " [ 0.22343824]] forward_w\n",
            "[[-0.77580782]\n",
            " [ 1.29323341]\n",
            " [ 0.22343824]] loss_w\n",
            "2.8852256762200984e-07 Loss 491\n",
            "[[-0.77580782]\n",
            " [ 1.29323341]\n",
            " [ 0.22343824]] grad_w\n",
            "[[-0.77581165]\n",
            " [ 1.29323619]\n",
            " [ 0.2234467 ]] init_w\n",
            "[[-0.77581165]\n",
            " [ 1.29323619]\n",
            " [ 0.2234467 ]] forward_w\n",
            "[[-0.77581165]\n",
            " [ 1.29323619]\n",
            " [ 0.2234467 ]] loss_w\n",
            "2.791895118165154e-07 Loss 492\n",
            "[[-0.77581165]\n",
            " [ 1.29323619]\n",
            " [ 0.2234467 ]] grad_w\n",
            "[[-0.77581541]\n",
            " [ 1.29323893]\n",
            " [ 0.22345503]] init_w\n",
            "[[-0.77581541]\n",
            " [ 1.29323893]\n",
            " [ 0.22345503]] forward_w\n",
            "[[-0.77581541]\n",
            " [ 1.29323893]\n",
            " [ 0.22345503]] loss_w\n",
            "2.7015835936430814e-07 Loss 493\n",
            "[[-0.77581541]\n",
            " [ 1.29323893]\n",
            " [ 0.22345503]] grad_w\n",
            "[[-0.77581911]\n",
            " [ 1.29324163]\n",
            " [ 0.22346323]] init_w\n",
            "[[-0.77581911]\n",
            " [ 1.29324163]\n",
            " [ 0.22346323]] forward_w\n",
            "[[-0.77581911]\n",
            " [ 1.29324163]\n",
            " [ 0.22346323]] loss_w\n",
            "2.614193443714054e-07 Loss 494\n",
            "[[-0.77581911]\n",
            " [ 1.29324163]\n",
            " [ 0.22346323]] grad_w\n",
            "[[-0.77582276]\n",
            " [ 1.29324428]\n",
            " [ 0.22347129]] init_w\n",
            "[[-0.77582276]\n",
            " [ 1.29324428]\n",
            " [ 0.22347129]] forward_w\n",
            "[[-0.77582276]\n",
            " [ 1.29324428]\n",
            " [ 0.22347129]] loss_w\n",
            "2.5296301684818e-07 Loss 495\n",
            "[[-0.77582276]\n",
            " [ 1.29324428]\n",
            " [ 0.22347129]] grad_w\n",
            "[[-0.77582634]\n",
            " [ 1.29324688]\n",
            " [ 0.22347922]] init_w\n",
            "[[-0.77582634]\n",
            " [ 1.29324688]\n",
            " [ 0.22347922]] forward_w\n",
            "[[-0.77582634]\n",
            " [ 1.29324688]\n",
            " [ 0.22347922]] loss_w\n",
            "2.4478023249121346e-07 Loss 496\n",
            "[[-0.77582634]\n",
            " [ 1.29324688]\n",
            " [ 0.22347922]] grad_w\n",
            "[[-0.77582986]\n",
            " [ 1.29324945]\n",
            " [ 0.22348702]] init_w\n",
            "[[-0.77582986]\n",
            " [ 1.29324945]\n",
            " [ 0.22348702]] forward_w\n",
            "[[-0.77582986]\n",
            " [ 1.29324945]\n",
            " [ 0.22348702]] loss_w\n",
            "2.368621427946649e-07 Loss 497\n",
            "[[-0.77582986]\n",
            " [ 1.29324945]\n",
            " [ 0.22348702]] grad_w\n",
            "[[-0.77583333]\n",
            " [ 1.29325197]\n",
            " [ 0.22349469]] init_w\n",
            "[[-0.77583333]\n",
            " [ 1.29325197]\n",
            " [ 0.22349469]] forward_w\n",
            "[[-0.77583333]\n",
            " [ 1.29325197]\n",
            " [ 0.22349469]] loss_w\n",
            "2.2920018548181328e-07 Loss 498\n",
            "[[-0.77583333]\n",
            " [ 1.29325197]\n",
            " [ 0.22349469]] grad_w\n",
            "[[-0.77583674]\n",
            " [ 1.29325445]\n",
            " [ 0.22350224]] init_w\n",
            "[[-0.77583674]\n",
            " [ 1.29325445]\n",
            " [ 0.22350224]] forward_w\n",
            "[[-0.77583674]\n",
            " [ 1.29325445]\n",
            " [ 0.22350224]] loss_w\n",
            "2.2178607524655457e-07 Loss 499\n",
            "[[-0.77583674]\n",
            " [ 1.29325445]\n",
            " [ 0.22350224]] grad_w\n",
            "[[ 0.99991995]\n",
            " [-0.99934976]] predictions2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sZuF0jAAvkc",
        "outputId": "397d96fd-4be2-4292-bffc-734f13d79584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "class MODEL():\n",
        "  def __init__(self, X, y, w):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.w = w\n",
        "    print(self.w, 'init_w')\n",
        "\n",
        "  def forward(self):\n",
        "    print(self.w, 'forward_w')\n",
        "    return torch.matmul(self.X, self.w)\n",
        "\n",
        "  def loss(self, y_pred, y_act):\n",
        "    print(self.w, 'loss_w')\n",
        "    return torch.mean((y_act - y_pred)**2)\n",
        "\n",
        "\n",
        "X = torch.tensor([[2, 3, 4], [1, 2, 4]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "w = torch.tensor([[2], [3], [1]], dtype = torch.float32, requires_grad = True)\n",
        "lr = 0.01\n",
        "\n",
        "predictions = torch.matmul(X, w)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "for i in range(4):\n",
        "  cal = MODEL(X, y, w)\n",
        "  y_pred = cal.forward()\n",
        "  Loss = cal.loss(y, y_pred)\n",
        "  print(Loss, 'Loss')\n",
        "  Loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "X = torch.tensor([[4, 3, 1], [1, 0, -1]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "#w = np.array([0])\n",
        "predictions = torch.matmul(X, w)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "print('next')\n",
        "for i in range(500):\n",
        "  calcu = MODEL(X, y, w)\n",
        "  y_pred = calcu.forward()\n",
        "  Loss = calcu.loss(y_pred, y)\n",
        "  print(Loss, 'Loss', i)\n",
        "  Loss.backward()  \n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "predictions = torch.matmul(X, w)\n",
        "print(predictions, 'predictions2')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "tensor([[ 1.1433],\n",
            "        [ 1.5952],\n",
            "        [-1.1924]], requires_grad=True) forward_w\n",
            "tensor([[ 1.1433],\n",
            "        [ 1.5952],\n",
            "        [-1.1924]], requires_grad=True) loss_w\n",
            "tensor(31.2418, grad_fn=<MeanBackward0>) Loss 0\n",
            "tensor([[ 0.8233],\n",
            "        [ 1.3802],\n",
            "        [-1.2307]], requires_grad=True) init_w\n",
            "tensor([[ 0.8233],\n",
            "        [ 1.3802],\n",
            "        [-1.2307]], requires_grad=True) forward_w\n",
            "tensor([[ 0.8233],\n",
            "        [ 1.3802],\n",
            "        [-1.2307]], requires_grad=True) loss_w\n",
            "tensor(18.1992, grad_fn=<MeanBackward0>) Loss 1\n",
            "tensor([[ 0.5846],\n",
            "        [ 1.2241],\n",
            "        [-1.2522]], requires_grad=True) init_w\n",
            "tensor([[ 0.5846],\n",
            "        [ 1.2241],\n",
            "        [-1.2522]], requires_grad=True) forward_w\n",
            "tensor([[ 0.5846],\n",
            "        [ 1.2241],\n",
            "        [-1.2522]], requires_grad=True) loss_w\n",
            "tensor(11.0874, grad_fn=<MeanBackward0>) Loss 2\n",
            "tensor([[ 0.4059],\n",
            "        [ 1.1114],\n",
            "        [-1.2614]], requires_grad=True) init_w\n",
            "tensor([[ 0.4059],\n",
            "        [ 1.1114],\n",
            "        [-1.2614]], requires_grad=True) forward_w\n",
            "tensor([[ 0.4059],\n",
            "        [ 1.1114],\n",
            "        [-1.2614]], requires_grad=True) loss_w\n",
            "tensor(7.1923, grad_fn=<MeanBackward0>) Loss 3\n",
            "tensor([[ 0.2714],\n",
            "        [ 1.0305],\n",
            "        [-1.2617]], requires_grad=True) init_w\n",
            "tensor([[ 0.2714],\n",
            "        [ 1.0305],\n",
            "        [-1.2617]], requires_grad=True) forward_w\n",
            "tensor([[ 0.2714],\n",
            "        [ 1.0305],\n",
            "        [-1.2617]], requires_grad=True) loss_w\n",
            "tensor(5.0423, grad_fn=<MeanBackward0>) Loss 4\n",
            "tensor([[ 0.1694],\n",
            "        [ 0.9730],\n",
            "        [-1.2555]], requires_grad=True) init_w\n",
            "tensor([[ 0.1694],\n",
            "        [ 0.9730],\n",
            "        [-1.2555]], requires_grad=True) forward_w\n",
            "tensor([[ 0.1694],\n",
            "        [ 0.9730],\n",
            "        [-1.2555]], requires_grad=True) loss_w\n",
            "tensor(3.8397, grad_fn=<MeanBackward0>) Loss 5\n",
            "tensor([[ 0.0915],\n",
            "        [ 0.9328],\n",
            "        [-1.2447]], requires_grad=True) init_w\n",
            "tensor([[ 0.0915],\n",
            "        [ 0.9328],\n",
            "        [-1.2447]], requires_grad=True) forward_w\n",
            "tensor([[ 0.0915],\n",
            "        [ 0.9328],\n",
            "        [-1.2447]], requires_grad=True) loss_w\n",
            "tensor(3.1520, grad_fn=<MeanBackward0>) Loss 6\n",
            "tensor([[ 0.0314],\n",
            "        [ 0.9052],\n",
            "        [-1.2305]], requires_grad=True) init_w\n",
            "tensor([[ 0.0314],\n",
            "        [ 0.9052],\n",
            "        [-1.2305]], requires_grad=True) forward_w\n",
            "tensor([[ 0.0314],\n",
            "        [ 0.9052],\n",
            "        [-1.2305]], requires_grad=True) loss_w\n",
            "tensor(2.7445, grad_fn=<MeanBackward0>) Loss 7\n",
            "tensor([[-0.0157],\n",
            "        [ 0.8869],\n",
            "        [-1.2140]], requires_grad=True) init_w\n",
            "tensor([[-0.0157],\n",
            "        [ 0.8869],\n",
            "        [-1.2140]], requires_grad=True) forward_w\n",
            "tensor([[-0.0157],\n",
            "        [ 0.8869],\n",
            "        [-1.2140]], requires_grad=True) loss_w\n",
            "tensor(2.4901, grad_fn=<MeanBackward0>) Loss 8\n",
            "tensor([[-0.0530],\n",
            "        [ 0.8753],\n",
            "        [-1.1959]], requires_grad=True) init_w\n",
            "tensor([[-0.0530],\n",
            "        [ 0.8753],\n",
            "        [-1.1959]], requires_grad=True) forward_w\n",
            "tensor([[-0.0530],\n",
            "        [ 0.8753],\n",
            "        [-1.1959]], requires_grad=True) loss_w\n",
            "tensor(2.3197, grad_fn=<MeanBackward0>) Loss 9\n",
            "tensor([[-0.0832],\n",
            "        [ 0.8688],\n",
            "        [-1.1766]], requires_grad=True) init_w\n",
            "tensor([[-0.0832],\n",
            "        [ 0.8688],\n",
            "        [-1.1766]], requires_grad=True) forward_w\n",
            "tensor([[-0.0832],\n",
            "        [ 0.8688],\n",
            "        [-1.1766]], requires_grad=True) loss_w\n",
            "tensor(2.1960, grad_fn=<MeanBackward0>) Loss 10\n",
            "tensor([[-0.1080],\n",
            "        [ 0.8659],\n",
            "        [-1.1567]], requires_grad=True) init_w\n",
            "tensor([[-0.1080],\n",
            "        [ 0.8659],\n",
            "        [-1.1567]], requires_grad=True) forward_w\n",
            "tensor([[-0.1080],\n",
            "        [ 0.8659],\n",
            "        [-1.1567]], requires_grad=True) loss_w\n",
            "tensor(2.0986, grad_fn=<MeanBackward0>) Loss 11\n",
            "tensor([[-0.1288],\n",
            "        [ 0.8656],\n",
            "        [-1.1363]], requires_grad=True) init_w\n",
            "tensor([[-0.1288],\n",
            "        [ 0.8656],\n",
            "        [-1.1363]], requires_grad=True) forward_w\n",
            "tensor([[-0.1288],\n",
            "        [ 0.8656],\n",
            "        [-1.1363]], requires_grad=True) loss_w\n",
            "tensor(2.0164, grad_fn=<MeanBackward0>) Loss 12\n",
            "tensor([[-0.1467],\n",
            "        [ 0.8672],\n",
            "        [-1.1156]], requires_grad=True) init_w\n",
            "tensor([[-0.1467],\n",
            "        [ 0.8672],\n",
            "        [-1.1156]], requires_grad=True) forward_w\n",
            "tensor([[-0.1467],\n",
            "        [ 0.8672],\n",
            "        [-1.1156]], requires_grad=True) loss_w\n",
            "tensor(1.9434, grad_fn=<MeanBackward0>) Loss 13\n",
            "tensor([[-0.1624],\n",
            "        [ 0.8703],\n",
            "        [-1.0949]], requires_grad=True) init_w\n",
            "tensor([[-0.1624],\n",
            "        [ 0.8703],\n",
            "        [-1.0949]], requires_grad=True) forward_w\n",
            "tensor([[-0.1624],\n",
            "        [ 0.8703],\n",
            "        [-1.0949]], requires_grad=True) loss_w\n",
            "tensor(1.8763, grad_fn=<MeanBackward0>) Loss 14\n",
            "tensor([[-0.1764],\n",
            "        [ 0.8743],\n",
            "        [-1.0743]], requires_grad=True) init_w\n",
            "tensor([[-0.1764],\n",
            "        [ 0.8743],\n",
            "        [-1.0743]], requires_grad=True) forward_w\n",
            "tensor([[-0.1764],\n",
            "        [ 0.8743],\n",
            "        [-1.0743]], requires_grad=True) loss_w\n",
            "tensor(1.8134, grad_fn=<MeanBackward0>) Loss 15\n",
            "tensor([[-0.1891],\n",
            "        [ 0.8790],\n",
            "        [-1.0537]], requires_grad=True) init_w\n",
            "tensor([[-0.1891],\n",
            "        [ 0.8790],\n",
            "        [-1.0537]], requires_grad=True) forward_w\n",
            "tensor([[-0.1891],\n",
            "        [ 0.8790],\n",
            "        [-1.0537]], requires_grad=True) loss_w\n",
            "tensor(1.7535, grad_fn=<MeanBackward0>) Loss 16\n",
            "tensor([[-0.2008],\n",
            "        [ 0.8842],\n",
            "        [-1.0334]], requires_grad=True) init_w\n",
            "tensor([[-0.2008],\n",
            "        [ 0.8842],\n",
            "        [-1.0334]], requires_grad=True) forward_w\n",
            "tensor([[-0.2008],\n",
            "        [ 0.8842],\n",
            "        [-1.0334]], requires_grad=True) loss_w\n",
            "tensor(1.6961, grad_fn=<MeanBackward0>) Loss 17\n",
            "tensor([[-0.2118],\n",
            "        [ 0.8897],\n",
            "        [-1.0132]], requires_grad=True) init_w\n",
            "tensor([[-0.2118],\n",
            "        [ 0.8897],\n",
            "        [-1.0132]], requires_grad=True) forward_w\n",
            "tensor([[-0.2118],\n",
            "        [ 0.8897],\n",
            "        [-1.0132]], requires_grad=True) loss_w\n",
            "tensor(1.6408, grad_fn=<MeanBackward0>) Loss 18\n",
            "tensor([[-0.2221],\n",
            "        [ 0.8954],\n",
            "        [-0.9933]], requires_grad=True) init_w\n",
            "tensor([[-0.2221],\n",
            "        [ 0.8954],\n",
            "        [-0.9933]], requires_grad=True) forward_w\n",
            "tensor([[-0.2221],\n",
            "        [ 0.8954],\n",
            "        [-0.9933]], requires_grad=True) loss_w\n",
            "tensor(1.5876, grad_fn=<MeanBackward0>) Loss 19\n",
            "tensor([[-0.2320],\n",
            "        [ 0.9013],\n",
            "        [-0.9736]], requires_grad=True) init_w\n",
            "tensor([[-0.2320],\n",
            "        [ 0.9013],\n",
            "        [-0.9736]], requires_grad=True) forward_w\n",
            "tensor([[-0.2320],\n",
            "        [ 0.9013],\n",
            "        [-0.9736]], requires_grad=True) loss_w\n",
            "tensor(1.5361, grad_fn=<MeanBackward0>) Loss 20\n",
            "tensor([[-0.2415],\n",
            "        [ 0.9072],\n",
            "        [-0.9542]], requires_grad=True) init_w\n",
            "tensor([[-0.2415],\n",
            "        [ 0.9072],\n",
            "        [-0.9542]], requires_grad=True) forward_w\n",
            "tensor([[-0.2415],\n",
            "        [ 0.9072],\n",
            "        [-0.9542]], requires_grad=True) loss_w\n",
            "tensor(1.4864, grad_fn=<MeanBackward0>) Loss 21\n",
            "tensor([[-0.2507],\n",
            "        [ 0.9132],\n",
            "        [-0.9351]], requires_grad=True) init_w\n",
            "tensor([[-0.2507],\n",
            "        [ 0.9132],\n",
            "        [-0.9351]], requires_grad=True) forward_w\n",
            "tensor([[-0.2507],\n",
            "        [ 0.9132],\n",
            "        [-0.9351]], requires_grad=True) loss_w\n",
            "tensor(1.4383, grad_fn=<MeanBackward0>) Loss 22\n",
            "tensor([[-0.2596],\n",
            "        [ 0.9191],\n",
            "        [-0.9163]], requires_grad=True) init_w\n",
            "tensor([[-0.2596],\n",
            "        [ 0.9191],\n",
            "        [-0.9163]], requires_grad=True) forward_w\n",
            "tensor([[-0.2596],\n",
            "        [ 0.9191],\n",
            "        [-0.9163]], requires_grad=True) loss_w\n",
            "tensor(1.3917, grad_fn=<MeanBackward0>) Loss 23\n",
            "tensor([[-0.2683],\n",
            "        [ 0.9251],\n",
            "        [-0.8977]], requires_grad=True) init_w\n",
            "tensor([[-0.2683],\n",
            "        [ 0.9251],\n",
            "        [-0.8977]], requires_grad=True) forward_w\n",
            "tensor([[-0.2683],\n",
            "        [ 0.9251],\n",
            "        [-0.8977]], requires_grad=True) loss_w\n",
            "tensor(1.3467, grad_fn=<MeanBackward0>) Loss 24\n",
            "tensor([[-0.2768],\n",
            "        [ 0.9309],\n",
            "        [-0.8795]], requires_grad=True) init_w\n",
            "tensor([[-0.2768],\n",
            "        [ 0.9309],\n",
            "        [-0.8795]], requires_grad=True) forward_w\n",
            "tensor([[-0.2768],\n",
            "        [ 0.9309],\n",
            "        [-0.8795]], requires_grad=True) loss_w\n",
            "tensor(1.3031, grad_fn=<MeanBackward0>) Loss 25\n",
            "tensor([[-0.2850],\n",
            "        [ 0.9367],\n",
            "        [-0.8615]], requires_grad=True) init_w\n",
            "tensor([[-0.2850],\n",
            "        [ 0.9367],\n",
            "        [-0.8615]], requires_grad=True) forward_w\n",
            "tensor([[-0.2850],\n",
            "        [ 0.9367],\n",
            "        [-0.8615]], requires_grad=True) loss_w\n",
            "tensor(1.2610, grad_fn=<MeanBackward0>) Loss 26\n",
            "tensor([[-0.2931],\n",
            "        [ 0.9425],\n",
            "        [-0.8438]], requires_grad=True) init_w\n",
            "tensor([[-0.2931],\n",
            "        [ 0.9425],\n",
            "        [-0.8438]], requires_grad=True) forward_w\n",
            "tensor([[-0.2931],\n",
            "        [ 0.9425],\n",
            "        [-0.8438]], requires_grad=True) loss_w\n",
            "tensor(1.2202, grad_fn=<MeanBackward0>) Loss 27\n",
            "tensor([[-0.3011],\n",
            "        [ 0.9481],\n",
            "        [-0.8264]], requires_grad=True) init_w\n",
            "tensor([[-0.3011],\n",
            "        [ 0.9481],\n",
            "        [-0.8264]], requires_grad=True) forward_w\n",
            "tensor([[-0.3011],\n",
            "        [ 0.9481],\n",
            "        [-0.8264]], requires_grad=True) loss_w\n",
            "tensor(1.1807, grad_fn=<MeanBackward0>) Loss 28\n",
            "tensor([[-0.3089],\n",
            "        [ 0.9537],\n",
            "        [-0.8093]], requires_grad=True) init_w\n",
            "tensor([[-0.3089],\n",
            "        [ 0.9537],\n",
            "        [-0.8093]], requires_grad=True) forward_w\n",
            "tensor([[-0.3089],\n",
            "        [ 0.9537],\n",
            "        [-0.8093]], requires_grad=True) loss_w\n",
            "tensor(1.1425, grad_fn=<MeanBackward0>) Loss 29\n",
            "tensor([[-0.3165],\n",
            "        [ 0.9592],\n",
            "        [-0.7925]], requires_grad=True) init_w\n",
            "tensor([[-0.3165],\n",
            "        [ 0.9592],\n",
            "        [-0.7925]], requires_grad=True) forward_w\n",
            "tensor([[-0.3165],\n",
            "        [ 0.9592],\n",
            "        [-0.7925]], requires_grad=True) loss_w\n",
            "tensor(1.1055, grad_fn=<MeanBackward0>) Loss 30\n",
            "tensor([[-0.3241],\n",
            "        [ 0.9647],\n",
            "        [-0.7759]], requires_grad=True) init_w\n",
            "tensor([[-0.3241],\n",
            "        [ 0.9647],\n",
            "        [-0.7759]], requires_grad=True) forward_w\n",
            "tensor([[-0.3241],\n",
            "        [ 0.9647],\n",
            "        [-0.7759]], requires_grad=True) loss_w\n",
            "tensor(1.0698, grad_fn=<MeanBackward0>) Loss 31\n",
            "tensor([[-0.3315],\n",
            "        [ 0.9700],\n",
            "        [-0.7596]], requires_grad=True) init_w\n",
            "tensor([[-0.3315],\n",
            "        [ 0.9700],\n",
            "        [-0.7596]], requires_grad=True) forward_w\n",
            "tensor([[-0.3315],\n",
            "        [ 0.9700],\n",
            "        [-0.7596]], requires_grad=True) loss_w\n",
            "tensor(1.0352, grad_fn=<MeanBackward0>) Loss 32\n",
            "tensor([[-0.3387],\n",
            "        [ 0.9753],\n",
            "        [-0.7436]], requires_grad=True) init_w\n",
            "tensor([[-0.3387],\n",
            "        [ 0.9753],\n",
            "        [-0.7436]], requires_grad=True) forward_w\n",
            "tensor([[-0.3387],\n",
            "        [ 0.9753],\n",
            "        [-0.7436]], requires_grad=True) loss_w\n",
            "tensor(1.0017, grad_fn=<MeanBackward0>) Loss 33\n",
            "tensor([[-0.3459],\n",
            "        [ 0.9805],\n",
            "        [-0.7278]], requires_grad=True) init_w\n",
            "tensor([[-0.3459],\n",
            "        [ 0.9805],\n",
            "        [-0.7278]], requires_grad=True) forward_w\n",
            "tensor([[-0.3459],\n",
            "        [ 0.9805],\n",
            "        [-0.7278]], requires_grad=True) loss_w\n",
            "tensor(0.9693, grad_fn=<MeanBackward0>) Loss 34\n",
            "tensor([[-0.3529],\n",
            "        [ 0.9856],\n",
            "        [-0.7123]], requires_grad=True) init_w\n",
            "tensor([[-0.3529],\n",
            "        [ 0.9856],\n",
            "        [-0.7123]], requires_grad=True) forward_w\n",
            "tensor([[-0.3529],\n",
            "        [ 0.9856],\n",
            "        [-0.7123]], requires_grad=True) loss_w\n",
            "tensor(0.9379, grad_fn=<MeanBackward0>) Loss 35\n",
            "tensor([[-0.3598],\n",
            "        [ 0.9906],\n",
            "        [-0.6970]], requires_grad=True) init_w\n",
            "tensor([[-0.3598],\n",
            "        [ 0.9906],\n",
            "        [-0.6970]], requires_grad=True) forward_w\n",
            "tensor([[-0.3598],\n",
            "        [ 0.9906],\n",
            "        [-0.6970]], requires_grad=True) loss_w\n",
            "tensor(0.9076, grad_fn=<MeanBackward0>) Loss 36\n",
            "tensor([[-0.3666],\n",
            "        [ 0.9955],\n",
            "        [-0.6820]], requires_grad=True) init_w\n",
            "tensor([[-0.3666],\n",
            "        [ 0.9955],\n",
            "        [-0.6820]], requires_grad=True) forward_w\n",
            "tensor([[-0.3666],\n",
            "        [ 0.9955],\n",
            "        [-0.6820]], requires_grad=True) loss_w\n",
            "tensor(0.8782, grad_fn=<MeanBackward0>) Loss 37\n",
            "tensor([[-0.3733],\n",
            "        [ 1.0004],\n",
            "        [-0.6672]], requires_grad=True) init_w\n",
            "tensor([[-0.3733],\n",
            "        [ 1.0004],\n",
            "        [-0.6672]], requires_grad=True) forward_w\n",
            "tensor([[-0.3733],\n",
            "        [ 1.0004],\n",
            "        [-0.6672]], requires_grad=True) loss_w\n",
            "tensor(0.8498, grad_fn=<MeanBackward0>) Loss 38\n",
            "tensor([[-0.3798],\n",
            "        [ 1.0051],\n",
            "        [-0.6527]], requires_grad=True) init_w\n",
            "tensor([[-0.3798],\n",
            "        [ 1.0051],\n",
            "        [-0.6527]], requires_grad=True) forward_w\n",
            "tensor([[-0.3798],\n",
            "        [ 1.0051],\n",
            "        [-0.6527]], requires_grad=True) loss_w\n",
            "tensor(0.8223, grad_fn=<MeanBackward0>) Loss 39\n",
            "tensor([[-0.3863],\n",
            "        [ 1.0098],\n",
            "        [-0.6384]], requires_grad=True) init_w\n",
            "tensor([[-0.3863],\n",
            "        [ 1.0098],\n",
            "        [-0.6384]], requires_grad=True) forward_w\n",
            "tensor([[-0.3863],\n",
            "        [ 1.0098],\n",
            "        [-0.6384]], requires_grad=True) loss_w\n",
            "tensor(0.7957, grad_fn=<MeanBackward0>) Loss 40\n",
            "tensor([[-0.3927],\n",
            "        [ 1.0145],\n",
            "        [-0.6243]], requires_grad=True) init_w\n",
            "tensor([[-0.3927],\n",
            "        [ 1.0145],\n",
            "        [-0.6243]], requires_grad=True) forward_w\n",
            "tensor([[-0.3927],\n",
            "        [ 1.0145],\n",
            "        [-0.6243]], requires_grad=True) loss_w\n",
            "tensor(0.7700, grad_fn=<MeanBackward0>) Loss 41\n",
            "tensor([[-0.3989],\n",
            "        [ 1.0190],\n",
            "        [-0.6105]], requires_grad=True) init_w\n",
            "tensor([[-0.3989],\n",
            "        [ 1.0190],\n",
            "        [-0.6105]], requires_grad=True) forward_w\n",
            "tensor([[-0.3989],\n",
            "        [ 1.0190],\n",
            "        [-0.6105]], requires_grad=True) loss_w\n",
            "tensor(0.7451, grad_fn=<MeanBackward0>) Loss 42\n",
            "tensor([[-0.4051],\n",
            "        [ 1.0235],\n",
            "        [-0.5969]], requires_grad=True) init_w\n",
            "tensor([[-0.4051],\n",
            "        [ 1.0235],\n",
            "        [-0.5969]], requires_grad=True) forward_w\n",
            "tensor([[-0.4051],\n",
            "        [ 1.0235],\n",
            "        [-0.5969]], requires_grad=True) loss_w\n",
            "tensor(0.7210, grad_fn=<MeanBackward0>) Loss 43\n",
            "tensor([[-0.4111],\n",
            "        [ 1.0279],\n",
            "        [-0.5835]], requires_grad=True) init_w\n",
            "tensor([[-0.4111],\n",
            "        [ 1.0279],\n",
            "        [-0.5835]], requires_grad=True) forward_w\n",
            "tensor([[-0.4111],\n",
            "        [ 1.0279],\n",
            "        [-0.5835]], requires_grad=True) loss_w\n",
            "tensor(0.6977, grad_fn=<MeanBackward0>) Loss 44\n",
            "tensor([[-0.4171],\n",
            "        [ 1.0322],\n",
            "        [-0.5703]], requires_grad=True) init_w\n",
            "tensor([[-0.4171],\n",
            "        [ 1.0322],\n",
            "        [-0.5703]], requires_grad=True) forward_w\n",
            "tensor([[-0.4171],\n",
            "        [ 1.0322],\n",
            "        [-0.5703]], requires_grad=True) loss_w\n",
            "tensor(0.6751, grad_fn=<MeanBackward0>) Loss 45\n",
            "tensor([[-0.4229],\n",
            "        [ 1.0365],\n",
            "        [-0.5574]], requires_grad=True) init_w\n",
            "tensor([[-0.4229],\n",
            "        [ 1.0365],\n",
            "        [-0.5574]], requires_grad=True) forward_w\n",
            "tensor([[-0.4229],\n",
            "        [ 1.0365],\n",
            "        [-0.5574]], requires_grad=True) loss_w\n",
            "tensor(0.6533, grad_fn=<MeanBackward0>) Loss 46\n",
            "tensor([[-0.4287],\n",
            "        [ 1.0407],\n",
            "        [-0.5446]], requires_grad=True) init_w\n",
            "tensor([[-0.4287],\n",
            "        [ 1.0407],\n",
            "        [-0.5446]], requires_grad=True) forward_w\n",
            "tensor([[-0.4287],\n",
            "        [ 1.0407],\n",
            "        [-0.5446]], requires_grad=True) loss_w\n",
            "tensor(0.6321, grad_fn=<MeanBackward0>) Loss 47\n",
            "tensor([[-0.4343],\n",
            "        [ 1.0448],\n",
            "        [-0.5321]], requires_grad=True) init_w\n",
            "tensor([[-0.4343],\n",
            "        [ 1.0448],\n",
            "        [-0.5321]], requires_grad=True) forward_w\n",
            "tensor([[-0.4343],\n",
            "        [ 1.0448],\n",
            "        [-0.5321]], requires_grad=True) loss_w\n",
            "tensor(0.6117, grad_fn=<MeanBackward0>) Loss 48\n",
            "tensor([[-0.4399],\n",
            "        [ 1.0488],\n",
            "        [-0.5198]], requires_grad=True) init_w\n",
            "tensor([[-0.4399],\n",
            "        [ 1.0488],\n",
            "        [-0.5198]], requires_grad=True) forward_w\n",
            "tensor([[-0.4399],\n",
            "        [ 1.0488],\n",
            "        [-0.5198]], requires_grad=True) loss_w\n",
            "tensor(0.5919, grad_fn=<MeanBackward0>) Loss 49\n",
            "tensor([[-0.4454],\n",
            "        [ 1.0528],\n",
            "        [-0.5077]], requires_grad=True) init_w\n",
            "tensor([[-0.4454],\n",
            "        [ 1.0528],\n",
            "        [-0.5077]], requires_grad=True) forward_w\n",
            "tensor([[-0.4454],\n",
            "        [ 1.0528],\n",
            "        [-0.5077]], requires_grad=True) loss_w\n",
            "tensor(0.5727, grad_fn=<MeanBackward0>) Loss 50\n",
            "tensor([[-0.4508],\n",
            "        [ 1.0567],\n",
            "        [-0.4957]], requires_grad=True) init_w\n",
            "tensor([[-0.4508],\n",
            "        [ 1.0567],\n",
            "        [-0.4957]], requires_grad=True) forward_w\n",
            "tensor([[-0.4508],\n",
            "        [ 1.0567],\n",
            "        [-0.4957]], requires_grad=True) loss_w\n",
            "tensor(0.5542, grad_fn=<MeanBackward0>) Loss 51\n",
            "tensor([[-0.4561],\n",
            "        [ 1.0606],\n",
            "        [-0.4840]], requires_grad=True) init_w\n",
            "tensor([[-0.4561],\n",
            "        [ 1.0606],\n",
            "        [-0.4840]], requires_grad=True) forward_w\n",
            "tensor([[-0.4561],\n",
            "        [ 1.0606],\n",
            "        [-0.4840]], requires_grad=True) loss_w\n",
            "tensor(0.5363, grad_fn=<MeanBackward0>) Loss 52\n",
            "tensor([[-0.4613],\n",
            "        [ 1.0644],\n",
            "        [-0.4724]], requires_grad=True) init_w\n",
            "tensor([[-0.4613],\n",
            "        [ 1.0644],\n",
            "        [-0.4724]], requires_grad=True) forward_w\n",
            "tensor([[-0.4613],\n",
            "        [ 1.0644],\n",
            "        [-0.4724]], requires_grad=True) loss_w\n",
            "tensor(0.5189, grad_fn=<MeanBackward0>) Loss 53\n",
            "tensor([[-0.4664],\n",
            "        [ 1.0681],\n",
            "        [-0.4611]], requires_grad=True) init_w\n",
            "tensor([[-0.4664],\n",
            "        [ 1.0681],\n",
            "        [-0.4611]], requires_grad=True) forward_w\n",
            "tensor([[-0.4664],\n",
            "        [ 1.0681],\n",
            "        [-0.4611]], requires_grad=True) loss_w\n",
            "tensor(0.5022, grad_fn=<MeanBackward0>) Loss 54\n",
            "tensor([[-0.4715],\n",
            "        [ 1.0718],\n",
            "        [-0.4499]], requires_grad=True) init_w\n",
            "tensor([[-0.4715],\n",
            "        [ 1.0718],\n",
            "        [-0.4499]], requires_grad=True) forward_w\n",
            "tensor([[-0.4715],\n",
            "        [ 1.0718],\n",
            "        [-0.4499]], requires_grad=True) loss_w\n",
            "tensor(0.4859, grad_fn=<MeanBackward0>) Loss 55\n",
            "tensor([[-0.4765],\n",
            "        [ 1.0754],\n",
            "        [-0.4389]], requires_grad=True) init_w\n",
            "tensor([[-0.4765],\n",
            "        [ 1.0754],\n",
            "        [-0.4389]], requires_grad=True) forward_w\n",
            "tensor([[-0.4765],\n",
            "        [ 1.0754],\n",
            "        [-0.4389]], requires_grad=True) loss_w\n",
            "tensor(0.4702, grad_fn=<MeanBackward0>) Loss 56\n",
            "tensor([[-0.4813],\n",
            "        [ 1.0790],\n",
            "        [-0.4281]], requires_grad=True) init_w\n",
            "tensor([[-0.4813],\n",
            "        [ 1.0790],\n",
            "        [-0.4281]], requires_grad=True) forward_w\n",
            "tensor([[-0.4813],\n",
            "        [ 1.0790],\n",
            "        [-0.4281]], requires_grad=True) loss_w\n",
            "tensor(0.4550, grad_fn=<MeanBackward0>) Loss 57\n",
            "tensor([[-0.4861],\n",
            "        [ 1.0825],\n",
            "        [-0.4175]], requires_grad=True) init_w\n",
            "tensor([[-0.4861],\n",
            "        [ 1.0825],\n",
            "        [-0.4175]], requires_grad=True) forward_w\n",
            "tensor([[-0.4861],\n",
            "        [ 1.0825],\n",
            "        [-0.4175]], requires_grad=True) loss_w\n",
            "tensor(0.4403, grad_fn=<MeanBackward0>) Loss 58\n",
            "tensor([[-0.4909],\n",
            "        [ 1.0859],\n",
            "        [-0.4070]], requires_grad=True) init_w\n",
            "tensor([[-0.4909],\n",
            "        [ 1.0859],\n",
            "        [-0.4070]], requires_grad=True) forward_w\n",
            "tensor([[-0.4909],\n",
            "        [ 1.0859],\n",
            "        [-0.4070]], requires_grad=True) loss_w\n",
            "tensor(0.4260, grad_fn=<MeanBackward0>) Loss 59\n",
            "tensor([[-0.4955],\n",
            "        [ 1.0893],\n",
            "        [-0.3967]], requires_grad=True) init_w\n",
            "tensor([[-0.4955],\n",
            "        [ 1.0893],\n",
            "        [-0.3967]], requires_grad=True) forward_w\n",
            "tensor([[-0.4955],\n",
            "        [ 1.0893],\n",
            "        [-0.3967]], requires_grad=True) loss_w\n",
            "tensor(0.4122, grad_fn=<MeanBackward0>) Loss 60\n",
            "tensor([[-0.5001],\n",
            "        [ 1.0926],\n",
            "        [-0.3866]], requires_grad=True) init_w\n",
            "tensor([[-0.5001],\n",
            "        [ 1.0926],\n",
            "        [-0.3866]], requires_grad=True) forward_w\n",
            "tensor([[-0.5001],\n",
            "        [ 1.0926],\n",
            "        [-0.3866]], requires_grad=True) loss_w\n",
            "tensor(0.3989, grad_fn=<MeanBackward0>) Loss 61\n",
            "tensor([[-0.5046],\n",
            "        [ 1.0959],\n",
            "        [-0.3767]], requires_grad=True) init_w\n",
            "tensor([[-0.5046],\n",
            "        [ 1.0959],\n",
            "        [-0.3767]], requires_grad=True) forward_w\n",
            "tensor([[-0.5046],\n",
            "        [ 1.0959],\n",
            "        [-0.3767]], requires_grad=True) loss_w\n",
            "tensor(0.3860, grad_fn=<MeanBackward0>) Loss 62\n",
            "tensor([[-0.5090],\n",
            "        [ 1.0991],\n",
            "        [-0.3669]], requires_grad=True) init_w\n",
            "tensor([[-0.5090],\n",
            "        [ 1.0991],\n",
            "        [-0.3669]], requires_grad=True) forward_w\n",
            "tensor([[-0.5090],\n",
            "        [ 1.0991],\n",
            "        [-0.3669]], requires_grad=True) loss_w\n",
            "tensor(0.3735, grad_fn=<MeanBackward0>) Loss 63\n",
            "tensor([[-0.5134],\n",
            "        [ 1.1023],\n",
            "        [-0.3572]], requires_grad=True) init_w\n",
            "tensor([[-0.5134],\n",
            "        [ 1.1023],\n",
            "        [-0.3572]], requires_grad=True) forward_w\n",
            "tensor([[-0.5134],\n",
            "        [ 1.1023],\n",
            "        [-0.3572]], requires_grad=True) loss_w\n",
            "tensor(0.3614, grad_fn=<MeanBackward0>) Loss 64\n",
            "tensor([[-0.5177],\n",
            "        [ 1.1054],\n",
            "        [-0.3478]], requires_grad=True) init_w\n",
            "tensor([[-0.5177],\n",
            "        [ 1.1054],\n",
            "        [-0.3478]], requires_grad=True) forward_w\n",
            "tensor([[-0.5177],\n",
            "        [ 1.1054],\n",
            "        [-0.3478]], requires_grad=True) loss_w\n",
            "tensor(0.3497, grad_fn=<MeanBackward0>) Loss 65\n",
            "tensor([[-0.5219],\n",
            "        [ 1.1085],\n",
            "        [-0.3384]], requires_grad=True) init_w\n",
            "tensor([[-0.5219],\n",
            "        [ 1.1085],\n",
            "        [-0.3384]], requires_grad=True) forward_w\n",
            "tensor([[-0.5219],\n",
            "        [ 1.1085],\n",
            "        [-0.3384]], requires_grad=True) loss_w\n",
            "tensor(0.3384, grad_fn=<MeanBackward0>) Loss 66\n",
            "tensor([[-0.5260],\n",
            "        [ 1.1115],\n",
            "        [-0.3293]], requires_grad=True) init_w\n",
            "tensor([[-0.5260],\n",
            "        [ 1.1115],\n",
            "        [-0.3293]], requires_grad=True) forward_w\n",
            "tensor([[-0.5260],\n",
            "        [ 1.1115],\n",
            "        [-0.3293]], requires_grad=True) loss_w\n",
            "tensor(0.3275, grad_fn=<MeanBackward0>) Loss 67\n",
            "tensor([[-0.5301],\n",
            "        [ 1.1145],\n",
            "        [-0.3202]], requires_grad=True) init_w\n",
            "tensor([[-0.5301],\n",
            "        [ 1.1145],\n",
            "        [-0.3202]], requires_grad=True) forward_w\n",
            "tensor([[-0.5301],\n",
            "        [ 1.1145],\n",
            "        [-0.3202]], requires_grad=True) loss_w\n",
            "tensor(0.3169, grad_fn=<MeanBackward0>) Loss 68\n",
            "tensor([[-0.5341],\n",
            "        [ 1.1174],\n",
            "        [-0.3114]], requires_grad=True) init_w\n",
            "tensor([[-0.5341],\n",
            "        [ 1.1174],\n",
            "        [-0.3114]], requires_grad=True) forward_w\n",
            "tensor([[-0.5341],\n",
            "        [ 1.1174],\n",
            "        [-0.3114]], requires_grad=True) loss_w\n",
            "tensor(0.3066, grad_fn=<MeanBackward0>) Loss 69\n",
            "tensor([[-0.5381],\n",
            "        [ 1.1202],\n",
            "        [-0.3026]], requires_grad=True) init_w\n",
            "tensor([[-0.5381],\n",
            "        [ 1.1202],\n",
            "        [-0.3026]], requires_grad=True) forward_w\n",
            "tensor([[-0.5381],\n",
            "        [ 1.1202],\n",
            "        [-0.3026]], requires_grad=True) loss_w\n",
            "tensor(0.2967, grad_fn=<MeanBackward0>) Loss 70\n",
            "tensor([[-0.5419],\n",
            "        [ 1.1231],\n",
            "        [-0.2940]], requires_grad=True) init_w\n",
            "tensor([[-0.5419],\n",
            "        [ 1.1231],\n",
            "        [-0.2940]], requires_grad=True) forward_w\n",
            "tensor([[-0.5419],\n",
            "        [ 1.1231],\n",
            "        [-0.2940]], requires_grad=True) loss_w\n",
            "tensor(0.2871, grad_fn=<MeanBackward0>) Loss 71\n",
            "tensor([[-0.5458],\n",
            "        [ 1.1258],\n",
            "        [-0.2856]], requires_grad=True) init_w\n",
            "tensor([[-0.5458],\n",
            "        [ 1.1258],\n",
            "        [-0.2856]], requires_grad=True) forward_w\n",
            "tensor([[-0.5458],\n",
            "        [ 1.1258],\n",
            "        [-0.2856]], requires_grad=True) loss_w\n",
            "tensor(0.2778, grad_fn=<MeanBackward0>) Loss 72\n",
            "tensor([[-0.5495],\n",
            "        [ 1.1286],\n",
            "        [-0.2773]], requires_grad=True) init_w\n",
            "tensor([[-0.5495],\n",
            "        [ 1.1286],\n",
            "        [-0.2773]], requires_grad=True) forward_w\n",
            "tensor([[-0.5495],\n",
            "        [ 1.1286],\n",
            "        [-0.2773]], requires_grad=True) loss_w\n",
            "tensor(0.2688, grad_fn=<MeanBackward0>) Loss 73\n",
            "tensor([[-0.5532],\n",
            "        [ 1.1313],\n",
            "        [-0.2691]], requires_grad=True) init_w\n",
            "tensor([[-0.5532],\n",
            "        [ 1.1313],\n",
            "        [-0.2691]], requires_grad=True) forward_w\n",
            "tensor([[-0.5532],\n",
            "        [ 1.1313],\n",
            "        [-0.2691]], requires_grad=True) loss_w\n",
            "tensor(0.2602, grad_fn=<MeanBackward0>) Loss 74\n",
            "tensor([[-0.5568],\n",
            "        [ 1.1339],\n",
            "        [-0.2611]], requires_grad=True) init_w\n",
            "tensor([[-0.5568],\n",
            "        [ 1.1339],\n",
            "        [-0.2611]], requires_grad=True) forward_w\n",
            "tensor([[-0.5568],\n",
            "        [ 1.1339],\n",
            "        [-0.2611]], requires_grad=True) loss_w\n",
            "tensor(0.2517, grad_fn=<MeanBackward0>) Loss 75\n",
            "tensor([[-0.5604],\n",
            "        [ 1.1365],\n",
            "        [-0.2532]], requires_grad=True) init_w\n",
            "tensor([[-0.5604],\n",
            "        [ 1.1365],\n",
            "        [-0.2532]], requires_grad=True) forward_w\n",
            "tensor([[-0.5604],\n",
            "        [ 1.1365],\n",
            "        [-0.2532]], requires_grad=True) loss_w\n",
            "tensor(0.2436, grad_fn=<MeanBackward0>) Loss 76\n",
            "tensor([[-0.5639],\n",
            "        [ 1.1391],\n",
            "        [-0.2454]], requires_grad=True) init_w\n",
            "tensor([[-0.5639],\n",
            "        [ 1.1391],\n",
            "        [-0.2454]], requires_grad=True) forward_w\n",
            "tensor([[-0.5639],\n",
            "        [ 1.1391],\n",
            "        [-0.2454]], requires_grad=True) loss_w\n",
            "tensor(0.2357, grad_fn=<MeanBackward0>) Loss 77\n",
            "tensor([[-0.5674],\n",
            "        [ 1.1416],\n",
            "        [-0.2377]], requires_grad=True) init_w\n",
            "tensor([[-0.5674],\n",
            "        [ 1.1416],\n",
            "        [-0.2377]], requires_grad=True) forward_w\n",
            "tensor([[-0.5674],\n",
            "        [ 1.1416],\n",
            "        [-0.2377]], requires_grad=True) loss_w\n",
            "tensor(0.2281, grad_fn=<MeanBackward0>) Loss 78\n",
            "tensor([[-0.5708],\n",
            "        [ 1.1441],\n",
            "        [-0.2302]], requires_grad=True) init_w\n",
            "tensor([[-0.5708],\n",
            "        [ 1.1441],\n",
            "        [-0.2302]], requires_grad=True) forward_w\n",
            "tensor([[-0.5708],\n",
            "        [ 1.1441],\n",
            "        [-0.2302]], requires_grad=True) loss_w\n",
            "tensor(0.2207, grad_fn=<MeanBackward0>) Loss 79\n",
            "tensor([[-0.5741],\n",
            "        [ 1.1465],\n",
            "        [-0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.5741],\n",
            "        [ 1.1465],\n",
            "        [-0.2228]], requires_grad=True) forward_w\n",
            "tensor([[-0.5741],\n",
            "        [ 1.1465],\n",
            "        [-0.2228]], requires_grad=True) loss_w\n",
            "tensor(0.2136, grad_fn=<MeanBackward0>) Loss 80\n",
            "tensor([[-0.5774],\n",
            "        [ 1.1489],\n",
            "        [-0.2155]], requires_grad=True) init_w\n",
            "tensor([[-0.5774],\n",
            "        [ 1.1489],\n",
            "        [-0.2155]], requires_grad=True) forward_w\n",
            "tensor([[-0.5774],\n",
            "        [ 1.1489],\n",
            "        [-0.2155]], requires_grad=True) loss_w\n",
            "tensor(0.2067, grad_fn=<MeanBackward0>) Loss 81\n",
            "tensor([[-0.5807],\n",
            "        [ 1.1512],\n",
            "        [-0.2083]], requires_grad=True) init_w\n",
            "tensor([[-0.5807],\n",
            "        [ 1.1512],\n",
            "        [-0.2083]], requires_grad=True) forward_w\n",
            "tensor([[-0.5807],\n",
            "        [ 1.1512],\n",
            "        [-0.2083]], requires_grad=True) loss_w\n",
            "tensor(0.2000, grad_fn=<MeanBackward0>) Loss 82\n",
            "tensor([[-0.5839],\n",
            "        [ 1.1536],\n",
            "        [-0.2013]], requires_grad=True) init_w\n",
            "tensor([[-0.5839],\n",
            "        [ 1.1536],\n",
            "        [-0.2013]], requires_grad=True) forward_w\n",
            "tensor([[-0.5839],\n",
            "        [ 1.1536],\n",
            "        [-0.2013]], requires_grad=True) loss_w\n",
            "tensor(0.1935, grad_fn=<MeanBackward0>) Loss 83\n",
            "tensor([[-0.5870],\n",
            "        [ 1.1558],\n",
            "        [-0.1944]], requires_grad=True) init_w\n",
            "tensor([[-0.5870],\n",
            "        [ 1.1558],\n",
            "        [-0.1944]], requires_grad=True) forward_w\n",
            "tensor([[-0.5870],\n",
            "        [ 1.1558],\n",
            "        [-0.1944]], requires_grad=True) loss_w\n",
            "tensor(0.1872, grad_fn=<MeanBackward0>) Loss 84\n",
            "tensor([[-0.5901],\n",
            "        [ 1.1581],\n",
            "        [-0.1875]], requires_grad=True) init_w\n",
            "tensor([[-0.5901],\n",
            "        [ 1.1581],\n",
            "        [-0.1875]], requires_grad=True) forward_w\n",
            "tensor([[-0.5901],\n",
            "        [ 1.1581],\n",
            "        [-0.1875]], requires_grad=True) loss_w\n",
            "tensor(0.1812, grad_fn=<MeanBackward0>) Loss 85\n",
            "tensor([[-0.5931],\n",
            "        [ 1.1603],\n",
            "        [-0.1808]], requires_grad=True) init_w\n",
            "tensor([[-0.5931],\n",
            "        [ 1.1603],\n",
            "        [-0.1808]], requires_grad=True) forward_w\n",
            "tensor([[-0.5931],\n",
            "        [ 1.1603],\n",
            "        [-0.1808]], requires_grad=True) loss_w\n",
            "tensor(0.1753, grad_fn=<MeanBackward0>) Loss 86\n",
            "tensor([[-0.5961],\n",
            "        [ 1.1625],\n",
            "        [-0.1742]], requires_grad=True) init_w\n",
            "tensor([[-0.5961],\n",
            "        [ 1.1625],\n",
            "        [-0.1742]], requires_grad=True) forward_w\n",
            "tensor([[-0.5961],\n",
            "        [ 1.1625],\n",
            "        [-0.1742]], requires_grad=True) loss_w\n",
            "tensor(0.1697, grad_fn=<MeanBackward0>) Loss 87\n",
            "tensor([[-0.5990],\n",
            "        [ 1.1646],\n",
            "        [-0.1677]], requires_grad=True) init_w\n",
            "tensor([[-0.5990],\n",
            "        [ 1.1646],\n",
            "        [-0.1677]], requires_grad=True) forward_w\n",
            "tensor([[-0.5990],\n",
            "        [ 1.1646],\n",
            "        [-0.1677]], requires_grad=True) loss_w\n",
            "tensor(0.1642, grad_fn=<MeanBackward0>) Loss 88\n",
            "tensor([[-0.6019],\n",
            "        [ 1.1667],\n",
            "        [-0.1614]], requires_grad=True) init_w\n",
            "tensor([[-0.6019],\n",
            "        [ 1.1667],\n",
            "        [-0.1614]], requires_grad=True) forward_w\n",
            "tensor([[-0.6019],\n",
            "        [ 1.1667],\n",
            "        [-0.1614]], requires_grad=True) loss_w\n",
            "tensor(0.1589, grad_fn=<MeanBackward0>) Loss 89\n",
            "tensor([[-0.6047],\n",
            "        [ 1.1688],\n",
            "        [-0.1551]], requires_grad=True) init_w\n",
            "tensor([[-0.6047],\n",
            "        [ 1.1688],\n",
            "        [-0.1551]], requires_grad=True) forward_w\n",
            "tensor([[-0.6047],\n",
            "        [ 1.1688],\n",
            "        [-0.1551]], requires_grad=True) loss_w\n",
            "tensor(0.1537, grad_fn=<MeanBackward0>) Loss 90\n",
            "tensor([[-0.6075],\n",
            "        [ 1.1708],\n",
            "        [-0.1489]], requires_grad=True) init_w\n",
            "tensor([[-0.6075],\n",
            "        [ 1.1708],\n",
            "        [-0.1489]], requires_grad=True) forward_w\n",
            "tensor([[-0.6075],\n",
            "        [ 1.1708],\n",
            "        [-0.1489]], requires_grad=True) loss_w\n",
            "tensor(0.1487, grad_fn=<MeanBackward0>) Loss 91\n",
            "tensor([[-0.6103],\n",
            "        [ 1.1728],\n",
            "        [-0.1428]], requires_grad=True) init_w\n",
            "tensor([[-0.6103],\n",
            "        [ 1.1728],\n",
            "        [-0.1428]], requires_grad=True) forward_w\n",
            "tensor([[-0.6103],\n",
            "        [ 1.1728],\n",
            "        [-0.1428]], requires_grad=True) loss_w\n",
            "tensor(0.1439, grad_fn=<MeanBackward0>) Loss 92\n",
            "tensor([[-0.6130],\n",
            "        [ 1.1748],\n",
            "        [-0.1368]], requires_grad=True) init_w\n",
            "tensor([[-0.6130],\n",
            "        [ 1.1748],\n",
            "        [-0.1368]], requires_grad=True) forward_w\n",
            "tensor([[-0.6130],\n",
            "        [ 1.1748],\n",
            "        [-0.1368]], requires_grad=True) loss_w\n",
            "tensor(0.1393, grad_fn=<MeanBackward0>) Loss 93\n",
            "tensor([[-0.6156],\n",
            "        [ 1.1767],\n",
            "        [-0.1309]], requires_grad=True) init_w\n",
            "tensor([[-0.6156],\n",
            "        [ 1.1767],\n",
            "        [-0.1309]], requires_grad=True) forward_w\n",
            "tensor([[-0.6156],\n",
            "        [ 1.1767],\n",
            "        [-0.1309]], requires_grad=True) loss_w\n",
            "tensor(0.1348, grad_fn=<MeanBackward0>) Loss 94\n",
            "tensor([[-0.6183],\n",
            "        [ 1.1786],\n",
            "        [-0.1252]], requires_grad=True) init_w\n",
            "tensor([[-0.6183],\n",
            "        [ 1.1786],\n",
            "        [-0.1252]], requires_grad=True) forward_w\n",
            "tensor([[-0.6183],\n",
            "        [ 1.1786],\n",
            "        [-0.1252]], requires_grad=True) loss_w\n",
            "tensor(0.1304, grad_fn=<MeanBackward0>) Loss 95\n",
            "tensor([[-0.6208],\n",
            "        [ 1.1805],\n",
            "        [-0.1195]], requires_grad=True) init_w\n",
            "tensor([[-0.6208],\n",
            "        [ 1.1805],\n",
            "        [-0.1195]], requires_grad=True) forward_w\n",
            "tensor([[-0.6208],\n",
            "        [ 1.1805],\n",
            "        [-0.1195]], requires_grad=True) loss_w\n",
            "tensor(0.1262, grad_fn=<MeanBackward0>) Loss 96\n",
            "tensor([[-0.6234],\n",
            "        [ 1.1823],\n",
            "        [-0.1139]], requires_grad=True) init_w\n",
            "tensor([[-0.6234],\n",
            "        [ 1.1823],\n",
            "        [-0.1139]], requires_grad=True) forward_w\n",
            "tensor([[-0.6234],\n",
            "        [ 1.1823],\n",
            "        [-0.1139]], requires_grad=True) loss_w\n",
            "tensor(0.1221, grad_fn=<MeanBackward0>) Loss 97\n",
            "tensor([[-0.6259],\n",
            "        [ 1.1841],\n",
            "        [-0.1084]], requires_grad=True) init_w\n",
            "tensor([[-0.6259],\n",
            "        [ 1.1841],\n",
            "        [-0.1084]], requires_grad=True) forward_w\n",
            "tensor([[-0.6259],\n",
            "        [ 1.1841],\n",
            "        [-0.1084]], requires_grad=True) loss_w\n",
            "tensor(0.1182, grad_fn=<MeanBackward0>) Loss 98\n",
            "tensor([[-0.6283],\n",
            "        [ 1.1859],\n",
            "        [-0.1029]], requires_grad=True) init_w\n",
            "tensor([[-0.6283],\n",
            "        [ 1.1859],\n",
            "        [-0.1029]], requires_grad=True) forward_w\n",
            "tensor([[-0.6283],\n",
            "        [ 1.1859],\n",
            "        [-0.1029]], requires_grad=True) loss_w\n",
            "tensor(0.1143, grad_fn=<MeanBackward0>) Loss 99\n",
            "tensor([[-0.6307],\n",
            "        [ 1.1877],\n",
            "        [-0.0976]], requires_grad=True) init_w\n",
            "tensor([[-0.6307],\n",
            "        [ 1.1877],\n",
            "        [-0.0976]], requires_grad=True) forward_w\n",
            "tensor([[-0.6307],\n",
            "        [ 1.1877],\n",
            "        [-0.0976]], requires_grad=True) loss_w\n",
            "tensor(0.1106, grad_fn=<MeanBackward0>) Loss 100\n",
            "tensor([[-0.6331],\n",
            "        [ 1.1894],\n",
            "        [-0.0924]], requires_grad=True) init_w\n",
            "tensor([[-0.6331],\n",
            "        [ 1.1894],\n",
            "        [-0.0924]], requires_grad=True) forward_w\n",
            "tensor([[-0.6331],\n",
            "        [ 1.1894],\n",
            "        [-0.0924]], requires_grad=True) loss_w\n",
            "tensor(0.1071, grad_fn=<MeanBackward0>) Loss 101\n",
            "tensor([[-0.6354],\n",
            "        [ 1.1911],\n",
            "        [-0.0872]], requires_grad=True) init_w\n",
            "tensor([[-0.6354],\n",
            "        [ 1.1911],\n",
            "        [-0.0872]], requires_grad=True) forward_w\n",
            "tensor([[-0.6354],\n",
            "        [ 1.1911],\n",
            "        [-0.0872]], requires_grad=True) loss_w\n",
            "tensor(0.1036, grad_fn=<MeanBackward0>) Loss 102\n",
            "tensor([[-0.6377],\n",
            "        [ 1.1928],\n",
            "        [-0.0821]], requires_grad=True) init_w\n",
            "tensor([[-0.6377],\n",
            "        [ 1.1928],\n",
            "        [-0.0821]], requires_grad=True) forward_w\n",
            "tensor([[-0.6377],\n",
            "        [ 1.1928],\n",
            "        [-0.0821]], requires_grad=True) loss_w\n",
            "tensor(0.1003, grad_fn=<MeanBackward0>) Loss 103\n",
            "tensor([[-0.6400],\n",
            "        [ 1.1944],\n",
            "        [-0.0771]], requires_grad=True) init_w\n",
            "tensor([[-0.6400],\n",
            "        [ 1.1944],\n",
            "        [-0.0771]], requires_grad=True) forward_w\n",
            "tensor([[-0.6400],\n",
            "        [ 1.1944],\n",
            "        [-0.0771]], requires_grad=True) loss_w\n",
            "tensor(0.0970, grad_fn=<MeanBackward0>) Loss 104\n",
            "tensor([[-0.6422],\n",
            "        [ 1.1960],\n",
            "        [-0.0722]], requires_grad=True) init_w\n",
            "tensor([[-0.6422],\n",
            "        [ 1.1960],\n",
            "        [-0.0722]], requires_grad=True) forward_w\n",
            "tensor([[-0.6422],\n",
            "        [ 1.1960],\n",
            "        [-0.0722]], requires_grad=True) loss_w\n",
            "tensor(0.0939, grad_fn=<MeanBackward0>) Loss 105\n",
            "tensor([[-0.6444],\n",
            "        [ 1.1976],\n",
            "        [-0.0674]], requires_grad=True) init_w\n",
            "tensor([[-0.6444],\n",
            "        [ 1.1976],\n",
            "        [-0.0674]], requires_grad=True) forward_w\n",
            "tensor([[-0.6444],\n",
            "        [ 1.1976],\n",
            "        [-0.0674]], requires_grad=True) loss_w\n",
            "tensor(0.0908, grad_fn=<MeanBackward0>) Loss 106\n",
            "tensor([[-0.6465],\n",
            "        [ 1.1992],\n",
            "        [-0.0626]], requires_grad=True) init_w\n",
            "tensor([[-0.6465],\n",
            "        [ 1.1992],\n",
            "        [-0.0626]], requires_grad=True) forward_w\n",
            "tensor([[-0.6465],\n",
            "        [ 1.1992],\n",
            "        [-0.0626]], requires_grad=True) loss_w\n",
            "tensor(0.0879, grad_fn=<MeanBackward0>) Loss 107\n",
            "tensor([[-0.6486],\n",
            "        [ 1.2007],\n",
            "        [-0.0580]], requires_grad=True) init_w\n",
            "tensor([[-0.6486],\n",
            "        [ 1.2007],\n",
            "        [-0.0580]], requires_grad=True) forward_w\n",
            "tensor([[-0.6486],\n",
            "        [ 1.2007],\n",
            "        [-0.0580]], requires_grad=True) loss_w\n",
            "tensor(0.0851, grad_fn=<MeanBackward0>) Loss 108\n",
            "tensor([[-0.6507],\n",
            "        [ 1.2022],\n",
            "        [-0.0534]], requires_grad=True) init_w\n",
            "tensor([[-0.6507],\n",
            "        [ 1.2022],\n",
            "        [-0.0534]], requires_grad=True) forward_w\n",
            "tensor([[-0.6507],\n",
            "        [ 1.2022],\n",
            "        [-0.0534]], requires_grad=True) loss_w\n",
            "tensor(0.0823, grad_fn=<MeanBackward0>) Loss 109\n",
            "tensor([[-0.6527],\n",
            "        [ 1.2037],\n",
            "        [-0.0489]], requires_grad=True) init_w\n",
            "tensor([[-0.6527],\n",
            "        [ 1.2037],\n",
            "        [-0.0489]], requires_grad=True) forward_w\n",
            "tensor([[-0.6527],\n",
            "        [ 1.2037],\n",
            "        [-0.0489]], requires_grad=True) loss_w\n",
            "tensor(0.0796, grad_fn=<MeanBackward0>) Loss 110\n",
            "tensor([[-0.6548],\n",
            "        [ 1.2052],\n",
            "        [-0.0444]], requires_grad=True) init_w\n",
            "tensor([[-0.6548],\n",
            "        [ 1.2052],\n",
            "        [-0.0444]], requires_grad=True) forward_w\n",
            "tensor([[-0.6548],\n",
            "        [ 1.2052],\n",
            "        [-0.0444]], requires_grad=True) loss_w\n",
            "tensor(0.0771, grad_fn=<MeanBackward0>) Loss 111\n",
            "tensor([[-0.6567],\n",
            "        [ 1.2066],\n",
            "        [-0.0400]], requires_grad=True) init_w\n",
            "tensor([[-0.6567],\n",
            "        [ 1.2066],\n",
            "        [-0.0400]], requires_grad=True) forward_w\n",
            "tensor([[-0.6567],\n",
            "        [ 1.2066],\n",
            "        [-0.0400]], requires_grad=True) loss_w\n",
            "tensor(0.0746, grad_fn=<MeanBackward0>) Loss 112\n",
            "tensor([[-0.6587],\n",
            "        [ 1.2080],\n",
            "        [-0.0357]], requires_grad=True) init_w\n",
            "tensor([[-0.6587],\n",
            "        [ 1.2080],\n",
            "        [-0.0357]], requires_grad=True) forward_w\n",
            "tensor([[-0.6587],\n",
            "        [ 1.2080],\n",
            "        [-0.0357]], requires_grad=True) loss_w\n",
            "tensor(0.0722, grad_fn=<MeanBackward0>) Loss 113\n",
            "tensor([[-0.6606],\n",
            "        [ 1.2094],\n",
            "        [-0.0315]], requires_grad=True) init_w\n",
            "tensor([[-0.6606],\n",
            "        [ 1.2094],\n",
            "        [-0.0315]], requires_grad=True) forward_w\n",
            "tensor([[-0.6606],\n",
            "        [ 1.2094],\n",
            "        [-0.0315]], requires_grad=True) loss_w\n",
            "tensor(0.0698, grad_fn=<MeanBackward0>) Loss 114\n",
            "tensor([[-0.6625],\n",
            "        [ 1.2108],\n",
            "        [-0.0273]], requires_grad=True) init_w\n",
            "tensor([[-0.6625],\n",
            "        [ 1.2108],\n",
            "        [-0.0273]], requires_grad=True) forward_w\n",
            "tensor([[-0.6625],\n",
            "        [ 1.2108],\n",
            "        [-0.0273]], requires_grad=True) loss_w\n",
            "tensor(0.0676, grad_fn=<MeanBackward0>) Loss 115\n",
            "tensor([[-0.6643],\n",
            "        [ 1.2121],\n",
            "        [-0.0232]], requires_grad=True) init_w\n",
            "tensor([[-0.6643],\n",
            "        [ 1.2121],\n",
            "        [-0.0232]], requires_grad=True) forward_w\n",
            "tensor([[-0.6643],\n",
            "        [ 1.2121],\n",
            "        [-0.0232]], requires_grad=True) loss_w\n",
            "tensor(0.0654, grad_fn=<MeanBackward0>) Loss 116\n",
            "tensor([[-0.6662],\n",
            "        [ 1.2134],\n",
            "        [-0.0192]], requires_grad=True) init_w\n",
            "tensor([[-0.6662],\n",
            "        [ 1.2134],\n",
            "        [-0.0192]], requires_grad=True) forward_w\n",
            "tensor([[-0.6662],\n",
            "        [ 1.2134],\n",
            "        [-0.0192]], requires_grad=True) loss_w\n",
            "tensor(0.0633, grad_fn=<MeanBackward0>) Loss 117\n",
            "tensor([[-0.6679],\n",
            "        [ 1.2148],\n",
            "        [-0.0152]], requires_grad=True) init_w\n",
            "tensor([[-0.6679],\n",
            "        [ 1.2148],\n",
            "        [-0.0152]], requires_grad=True) forward_w\n",
            "tensor([[-0.6679],\n",
            "        [ 1.2148],\n",
            "        [-0.0152]], requires_grad=True) loss_w\n",
            "tensor(0.0612, grad_fn=<MeanBackward0>) Loss 118\n",
            "tensor([[-0.6697],\n",
            "        [ 1.2160],\n",
            "        [-0.0113]], requires_grad=True) init_w\n",
            "tensor([[-0.6697],\n",
            "        [ 1.2160],\n",
            "        [-0.0113]], requires_grad=True) forward_w\n",
            "tensor([[-0.6697],\n",
            "        [ 1.2160],\n",
            "        [-0.0113]], requires_grad=True) loss_w\n",
            "tensor(0.0592, grad_fn=<MeanBackward0>) Loss 119\n",
            "tensor([[-0.6714],\n",
            "        [ 1.2173],\n",
            "        [-0.0075]], requires_grad=True) init_w\n",
            "tensor([[-0.6714],\n",
            "        [ 1.2173],\n",
            "        [-0.0075]], requires_grad=True) forward_w\n",
            "tensor([[-0.6714],\n",
            "        [ 1.2173],\n",
            "        [-0.0075]], requires_grad=True) loss_w\n",
            "tensor(0.0573, grad_fn=<MeanBackward0>) Loss 120\n",
            "tensor([[-0.6731],\n",
            "        [ 1.2185],\n",
            "        [-0.0037]], requires_grad=True) init_w\n",
            "tensor([[-0.6731],\n",
            "        [ 1.2185],\n",
            "        [-0.0037]], requires_grad=True) forward_w\n",
            "tensor([[-0.6731],\n",
            "        [ 1.2185],\n",
            "        [-0.0037]], requires_grad=True) loss_w\n",
            "tensor(0.0555, grad_fn=<MeanBackward0>) Loss 121\n",
            "tensor([[-6.7482e-01],\n",
            "        [ 1.2198e+00],\n",
            "        [-5.8876e-06]], requires_grad=True) init_w\n",
            "tensor([[-6.7482e-01],\n",
            "        [ 1.2198e+00],\n",
            "        [-5.8876e-06]], requires_grad=True) forward_w\n",
            "tensor([[-6.7482e-01],\n",
            "        [ 1.2198e+00],\n",
            "        [-5.8876e-06]], requires_grad=True) loss_w\n",
            "tensor(0.0537, grad_fn=<MeanBackward0>) Loss 122\n",
            "tensor([[-0.6765],\n",
            "        [ 1.2210],\n",
            "        [ 0.0036]], requires_grad=True) init_w\n",
            "tensor([[-0.6765],\n",
            "        [ 1.2210],\n",
            "        [ 0.0036]], requires_grad=True) forward_w\n",
            "tensor([[-0.6765],\n",
            "        [ 1.2210],\n",
            "        [ 0.0036]], requires_grad=True) loss_w\n",
            "tensor(0.0519, grad_fn=<MeanBackward0>) Loss 123\n",
            "tensor([[-0.6781],\n",
            "        [ 1.2221],\n",
            "        [ 0.0072]], requires_grad=True) init_w\n",
            "tensor([[-0.6781],\n",
            "        [ 1.2221],\n",
            "        [ 0.0072]], requires_grad=True) forward_w\n",
            "tensor([[-0.6781],\n",
            "        [ 1.2221],\n",
            "        [ 0.0072]], requires_grad=True) loss_w\n",
            "tensor(0.0503, grad_fn=<MeanBackward0>) Loss 124\n",
            "tensor([[-0.6797],\n",
            "        [ 1.2233],\n",
            "        [ 0.0108]], requires_grad=True) init_w\n",
            "tensor([[-0.6797],\n",
            "        [ 1.2233],\n",
            "        [ 0.0108]], requires_grad=True) forward_w\n",
            "tensor([[-0.6797],\n",
            "        [ 1.2233],\n",
            "        [ 0.0108]], requires_grad=True) loss_w\n",
            "tensor(0.0486, grad_fn=<MeanBackward0>) Loss 125\n",
            "tensor([[-0.6813],\n",
            "        [ 1.2244],\n",
            "        [ 0.0142]], requires_grad=True) init_w\n",
            "tensor([[-0.6813],\n",
            "        [ 1.2244],\n",
            "        [ 0.0142]], requires_grad=True) forward_w\n",
            "tensor([[-0.6813],\n",
            "        [ 1.2244],\n",
            "        [ 0.0142]], requires_grad=True) loss_w\n",
            "tensor(0.0471, grad_fn=<MeanBackward0>) Loss 126\n",
            "tensor([[-0.6828],\n",
            "        [ 1.2256],\n",
            "        [ 0.0177]], requires_grad=True) init_w\n",
            "tensor([[-0.6828],\n",
            "        [ 1.2256],\n",
            "        [ 0.0177]], requires_grad=True) forward_w\n",
            "tensor([[-0.6828],\n",
            "        [ 1.2256],\n",
            "        [ 0.0177]], requires_grad=True) loss_w\n",
            "tensor(0.0455, grad_fn=<MeanBackward0>) Loss 127\n",
            "tensor([[-0.6843],\n",
            "        [ 1.2267],\n",
            "        [ 0.0210]], requires_grad=True) init_w\n",
            "tensor([[-0.6843],\n",
            "        [ 1.2267],\n",
            "        [ 0.0210]], requires_grad=True) forward_w\n",
            "tensor([[-0.6843],\n",
            "        [ 1.2267],\n",
            "        [ 0.0210]], requires_grad=True) loss_w\n",
            "tensor(0.0441, grad_fn=<MeanBackward0>) Loss 128\n",
            "tensor([[-0.6858],\n",
            "        [ 1.2278],\n",
            "        [ 0.0243]], requires_grad=True) init_w\n",
            "tensor([[-0.6858],\n",
            "        [ 1.2278],\n",
            "        [ 0.0243]], requires_grad=True) forward_w\n",
            "tensor([[-0.6858],\n",
            "        [ 1.2278],\n",
            "        [ 0.0243]], requires_grad=True) loss_w\n",
            "tensor(0.0426, grad_fn=<MeanBackward0>) Loss 129\n",
            "tensor([[-0.6873],\n",
            "        [ 1.2288],\n",
            "        [ 0.0276]], requires_grad=True) init_w\n",
            "tensor([[-0.6873],\n",
            "        [ 1.2288],\n",
            "        [ 0.0276]], requires_grad=True) forward_w\n",
            "tensor([[-0.6873],\n",
            "        [ 1.2288],\n",
            "        [ 0.0276]], requires_grad=True) loss_w\n",
            "tensor(0.0413, grad_fn=<MeanBackward0>) Loss 130\n",
            "tensor([[-0.6887],\n",
            "        [ 1.2299],\n",
            "        [ 0.0308]], requires_grad=True) init_w\n",
            "tensor([[-0.6887],\n",
            "        [ 1.2299],\n",
            "        [ 0.0308]], requires_grad=True) forward_w\n",
            "tensor([[-0.6887],\n",
            "        [ 1.2299],\n",
            "        [ 0.0308]], requires_grad=True) loss_w\n",
            "tensor(0.0399, grad_fn=<MeanBackward0>) Loss 131\n",
            "tensor([[-0.6902],\n",
            "        [ 1.2309],\n",
            "        [ 0.0339]], requires_grad=True) init_w\n",
            "tensor([[-0.6902],\n",
            "        [ 1.2309],\n",
            "        [ 0.0339]], requires_grad=True) forward_w\n",
            "tensor([[-0.6902],\n",
            "        [ 1.2309],\n",
            "        [ 0.0339]], requires_grad=True) loss_w\n",
            "tensor(0.0386, grad_fn=<MeanBackward0>) Loss 132\n",
            "tensor([[-0.6916],\n",
            "        [ 1.2319],\n",
            "        [ 0.0370]], requires_grad=True) init_w\n",
            "tensor([[-0.6916],\n",
            "        [ 1.2319],\n",
            "        [ 0.0370]], requires_grad=True) forward_w\n",
            "tensor([[-0.6916],\n",
            "        [ 1.2319],\n",
            "        [ 0.0370]], requires_grad=True) loss_w\n",
            "tensor(0.0374, grad_fn=<MeanBackward0>) Loss 133\n",
            "tensor([[-0.6929],\n",
            "        [ 1.2329],\n",
            "        [ 0.0401]], requires_grad=True) init_w\n",
            "tensor([[-0.6929],\n",
            "        [ 1.2329],\n",
            "        [ 0.0401]], requires_grad=True) forward_w\n",
            "tensor([[-0.6929],\n",
            "        [ 1.2329],\n",
            "        [ 0.0401]], requires_grad=True) loss_w\n",
            "tensor(0.0362, grad_fn=<MeanBackward0>) Loss 134\n",
            "tensor([[-0.6943],\n",
            "        [ 1.2339],\n",
            "        [ 0.0431]], requires_grad=True) init_w\n",
            "tensor([[-0.6943],\n",
            "        [ 1.2339],\n",
            "        [ 0.0431]], requires_grad=True) forward_w\n",
            "tensor([[-0.6943],\n",
            "        [ 1.2339],\n",
            "        [ 0.0431]], requires_grad=True) loss_w\n",
            "tensor(0.0350, grad_fn=<MeanBackward0>) Loss 135\n",
            "tensor([[-0.6956],\n",
            "        [ 1.2349],\n",
            "        [ 0.0460]], requires_grad=True) init_w\n",
            "tensor([[-0.6956],\n",
            "        [ 1.2349],\n",
            "        [ 0.0460]], requires_grad=True) forward_w\n",
            "tensor([[-0.6956],\n",
            "        [ 1.2349],\n",
            "        [ 0.0460]], requires_grad=True) loss_w\n",
            "tensor(0.0339, grad_fn=<MeanBackward0>) Loss 136\n",
            "tensor([[-0.6969],\n",
            "        [ 1.2359],\n",
            "        [ 0.0489]], requires_grad=True) init_w\n",
            "tensor([[-0.6969],\n",
            "        [ 1.2359],\n",
            "        [ 0.0489]], requires_grad=True) forward_w\n",
            "tensor([[-0.6969],\n",
            "        [ 1.2359],\n",
            "        [ 0.0489]], requires_grad=True) loss_w\n",
            "tensor(0.0328, grad_fn=<MeanBackward0>) Loss 137\n",
            "tensor([[-0.6982],\n",
            "        [ 1.2368],\n",
            "        [ 0.0518]], requires_grad=True) init_w\n",
            "tensor([[-0.6982],\n",
            "        [ 1.2368],\n",
            "        [ 0.0518]], requires_grad=True) forward_w\n",
            "tensor([[-0.6982],\n",
            "        [ 1.2368],\n",
            "        [ 0.0518]], requires_grad=True) loss_w\n",
            "tensor(0.0317, grad_fn=<MeanBackward0>) Loss 138\n",
            "tensor([[-0.6995],\n",
            "        [ 1.2377],\n",
            "        [ 0.0546]], requires_grad=True) init_w\n",
            "tensor([[-0.6995],\n",
            "        [ 1.2377],\n",
            "        [ 0.0546]], requires_grad=True) forward_w\n",
            "tensor([[-0.6995],\n",
            "        [ 1.2377],\n",
            "        [ 0.0546]], requires_grad=True) loss_w\n",
            "tensor(0.0307, grad_fn=<MeanBackward0>) Loss 139\n",
            "tensor([[-0.7008],\n",
            "        [ 1.2386],\n",
            "        [ 0.0574]], requires_grad=True) init_w\n",
            "tensor([[-0.7008],\n",
            "        [ 1.2386],\n",
            "        [ 0.0574]], requires_grad=True) forward_w\n",
            "tensor([[-0.7008],\n",
            "        [ 1.2386],\n",
            "        [ 0.0574]], requires_grad=True) loss_w\n",
            "tensor(0.0297, grad_fn=<MeanBackward0>) Loss 140\n",
            "tensor([[-0.7020],\n",
            "        [ 1.2395],\n",
            "        [ 0.0601]], requires_grad=True) init_w\n",
            "tensor([[-0.7020],\n",
            "        [ 1.2395],\n",
            "        [ 0.0601]], requires_grad=True) forward_w\n",
            "tensor([[-0.7020],\n",
            "        [ 1.2395],\n",
            "        [ 0.0601]], requires_grad=True) loss_w\n",
            "tensor(0.0287, grad_fn=<MeanBackward0>) Loss 141\n",
            "tensor([[-0.7032],\n",
            "        [ 1.2404],\n",
            "        [ 0.0628]], requires_grad=True) init_w\n",
            "tensor([[-0.7032],\n",
            "        [ 1.2404],\n",
            "        [ 0.0628]], requires_grad=True) forward_w\n",
            "tensor([[-0.7032],\n",
            "        [ 1.2404],\n",
            "        [ 0.0628]], requires_grad=True) loss_w\n",
            "tensor(0.0278, grad_fn=<MeanBackward0>) Loss 142\n",
            "tensor([[-0.7044],\n",
            "        [ 1.2413],\n",
            "        [ 0.0654]], requires_grad=True) init_w\n",
            "tensor([[-0.7044],\n",
            "        [ 1.2413],\n",
            "        [ 0.0654]], requires_grad=True) forward_w\n",
            "tensor([[-0.7044],\n",
            "        [ 1.2413],\n",
            "        [ 0.0654]], requires_grad=True) loss_w\n",
            "tensor(0.0269, grad_fn=<MeanBackward0>) Loss 143\n",
            "tensor([[-0.7055],\n",
            "        [ 1.2421],\n",
            "        [ 0.0680]], requires_grad=True) init_w\n",
            "tensor([[-0.7055],\n",
            "        [ 1.2421],\n",
            "        [ 0.0680]], requires_grad=True) forward_w\n",
            "tensor([[-0.7055],\n",
            "        [ 1.2421],\n",
            "        [ 0.0680]], requires_grad=True) loss_w\n",
            "tensor(0.0260, grad_fn=<MeanBackward0>) Loss 144\n",
            "tensor([[-0.7067],\n",
            "        [ 1.2429],\n",
            "        [ 0.0705]], requires_grad=True) init_w\n",
            "tensor([[-0.7067],\n",
            "        [ 1.2429],\n",
            "        [ 0.0705]], requires_grad=True) forward_w\n",
            "tensor([[-0.7067],\n",
            "        [ 1.2429],\n",
            "        [ 0.0705]], requires_grad=True) loss_w\n",
            "tensor(0.0252, grad_fn=<MeanBackward0>) Loss 145\n",
            "tensor([[-0.7078],\n",
            "        [ 1.2438],\n",
            "        [ 0.0730]], requires_grad=True) init_w\n",
            "tensor([[-0.7078],\n",
            "        [ 1.2438],\n",
            "        [ 0.0730]], requires_grad=True) forward_w\n",
            "tensor([[-0.7078],\n",
            "        [ 1.2438],\n",
            "        [ 0.0730]], requires_grad=True) loss_w\n",
            "tensor(0.0244, grad_fn=<MeanBackward0>) Loss 146\n",
            "tensor([[-0.7089],\n",
            "        [ 1.2446],\n",
            "        [ 0.0755]], requires_grad=True) init_w\n",
            "tensor([[-0.7089],\n",
            "        [ 1.2446],\n",
            "        [ 0.0755]], requires_grad=True) forward_w\n",
            "tensor([[-0.7089],\n",
            "        [ 1.2446],\n",
            "        [ 0.0755]], requires_grad=True) loss_w\n",
            "tensor(0.0236, grad_fn=<MeanBackward0>) Loss 147\n",
            "tensor([[-0.7100],\n",
            "        [ 1.2454],\n",
            "        [ 0.0779]], requires_grad=True) init_w\n",
            "tensor([[-0.7100],\n",
            "        [ 1.2454],\n",
            "        [ 0.0779]], requires_grad=True) forward_w\n",
            "tensor([[-0.7100],\n",
            "        [ 1.2454],\n",
            "        [ 0.0779]], requires_grad=True) loss_w\n",
            "tensor(0.0228, grad_fn=<MeanBackward0>) Loss 148\n",
            "tensor([[-0.7111],\n",
            "        [ 1.2462],\n",
            "        [ 0.0803]], requires_grad=True) init_w\n",
            "tensor([[-0.7111],\n",
            "        [ 1.2462],\n",
            "        [ 0.0803]], requires_grad=True) forward_w\n",
            "tensor([[-0.7111],\n",
            "        [ 1.2462],\n",
            "        [ 0.0803]], requires_grad=True) loss_w\n",
            "tensor(0.0221, grad_fn=<MeanBackward0>) Loss 149\n",
            "tensor([[-0.7122],\n",
            "        [ 1.2469],\n",
            "        [ 0.0826]], requires_grad=True) init_w\n",
            "tensor([[-0.7122],\n",
            "        [ 1.2469],\n",
            "        [ 0.0826]], requires_grad=True) forward_w\n",
            "tensor([[-0.7122],\n",
            "        [ 1.2469],\n",
            "        [ 0.0826]], requires_grad=True) loss_w\n",
            "tensor(0.0214, grad_fn=<MeanBackward0>) Loss 150\n",
            "tensor([[-0.7132],\n",
            "        [ 1.2477],\n",
            "        [ 0.0849]], requires_grad=True) init_w\n",
            "tensor([[-0.7132],\n",
            "        [ 1.2477],\n",
            "        [ 0.0849]], requires_grad=True) forward_w\n",
            "tensor([[-0.7132],\n",
            "        [ 1.2477],\n",
            "        [ 0.0849]], requires_grad=True) loss_w\n",
            "tensor(0.0207, grad_fn=<MeanBackward0>) Loss 151\n",
            "tensor([[-0.7142],\n",
            "        [ 1.2484],\n",
            "        [ 0.0872]], requires_grad=True) init_w\n",
            "tensor([[-0.7142],\n",
            "        [ 1.2484],\n",
            "        [ 0.0872]], requires_grad=True) forward_w\n",
            "tensor([[-0.7142],\n",
            "        [ 1.2484],\n",
            "        [ 0.0872]], requires_grad=True) loss_w\n",
            "tensor(0.0200, grad_fn=<MeanBackward0>) Loss 152\n",
            "tensor([[-0.7152],\n",
            "        [ 1.2492],\n",
            "        [ 0.0894]], requires_grad=True) init_w\n",
            "tensor([[-0.7152],\n",
            "        [ 1.2492],\n",
            "        [ 0.0894]], requires_grad=True) forward_w\n",
            "tensor([[-0.7152],\n",
            "        [ 1.2492],\n",
            "        [ 0.0894]], requires_grad=True) loss_w\n",
            "tensor(0.0194, grad_fn=<MeanBackward0>) Loss 153\n",
            "tensor([[-0.7162],\n",
            "        [ 1.2499],\n",
            "        [ 0.0916]], requires_grad=True) init_w\n",
            "tensor([[-0.7162],\n",
            "        [ 1.2499],\n",
            "        [ 0.0916]], requires_grad=True) forward_w\n",
            "tensor([[-0.7162],\n",
            "        [ 1.2499],\n",
            "        [ 0.0916]], requires_grad=True) loss_w\n",
            "tensor(0.0187, grad_fn=<MeanBackward0>) Loss 154\n",
            "tensor([[-0.7172],\n",
            "        [ 1.2506],\n",
            "        [ 0.0938]], requires_grad=True) init_w\n",
            "tensor([[-0.7172],\n",
            "        [ 1.2506],\n",
            "        [ 0.0938]], requires_grad=True) forward_w\n",
            "tensor([[-0.7172],\n",
            "        [ 1.2506],\n",
            "        [ 0.0938]], requires_grad=True) loss_w\n",
            "tensor(0.0181, grad_fn=<MeanBackward0>) Loss 155\n",
            "tensor([[-0.7182],\n",
            "        [ 1.2513],\n",
            "        [ 0.0959]], requires_grad=True) init_w\n",
            "tensor([[-0.7182],\n",
            "        [ 1.2513],\n",
            "        [ 0.0959]], requires_grad=True) forward_w\n",
            "tensor([[-0.7182],\n",
            "        [ 1.2513],\n",
            "        [ 0.0959]], requires_grad=True) loss_w\n",
            "tensor(0.0175, grad_fn=<MeanBackward0>) Loss 156\n",
            "tensor([[-0.7191],\n",
            "        [ 1.2520],\n",
            "        [ 0.0980]], requires_grad=True) init_w\n",
            "tensor([[-0.7191],\n",
            "        [ 1.2520],\n",
            "        [ 0.0980]], requires_grad=True) forward_w\n",
            "tensor([[-0.7191],\n",
            "        [ 1.2520],\n",
            "        [ 0.0980]], requires_grad=True) loss_w\n",
            "tensor(0.0170, grad_fn=<MeanBackward0>) Loss 157\n",
            "tensor([[-0.7200],\n",
            "        [ 1.2527],\n",
            "        [ 0.1000]], requires_grad=True) init_w\n",
            "tensor([[-0.7200],\n",
            "        [ 1.2527],\n",
            "        [ 0.1000]], requires_grad=True) forward_w\n",
            "tensor([[-0.7200],\n",
            "        [ 1.2527],\n",
            "        [ 0.1000]], requires_grad=True) loss_w\n",
            "tensor(0.0164, grad_fn=<MeanBackward0>) Loss 158\n",
            "tensor([[-0.7210],\n",
            "        [ 1.2533],\n",
            "        [ 0.1021]], requires_grad=True) init_w\n",
            "tensor([[-0.7210],\n",
            "        [ 1.2533],\n",
            "        [ 0.1021]], requires_grad=True) forward_w\n",
            "tensor([[-0.7210],\n",
            "        [ 1.2533],\n",
            "        [ 0.1021]], requires_grad=True) loss_w\n",
            "tensor(0.0159, grad_fn=<MeanBackward0>) Loss 159\n",
            "tensor([[-0.7219],\n",
            "        [ 1.2540],\n",
            "        [ 0.1041]], requires_grad=True) init_w\n",
            "tensor([[-0.7219],\n",
            "        [ 1.2540],\n",
            "        [ 0.1041]], requires_grad=True) forward_w\n",
            "tensor([[-0.7219],\n",
            "        [ 1.2540],\n",
            "        [ 0.1041]], requires_grad=True) loss_w\n",
            "tensor(0.0154, grad_fn=<MeanBackward0>) Loss 160\n",
            "tensor([[-0.7227],\n",
            "        [ 1.2546],\n",
            "        [ 0.1060]], requires_grad=True) init_w\n",
            "tensor([[-0.7227],\n",
            "        [ 1.2546],\n",
            "        [ 0.1060]], requires_grad=True) forward_w\n",
            "tensor([[-0.7227],\n",
            "        [ 1.2546],\n",
            "        [ 0.1060]], requires_grad=True) loss_w\n",
            "tensor(0.0149, grad_fn=<MeanBackward0>) Loss 161\n",
            "tensor([[-0.7236],\n",
            "        [ 1.2552],\n",
            "        [ 0.1079]], requires_grad=True) init_w\n",
            "tensor([[-0.7236],\n",
            "        [ 1.2552],\n",
            "        [ 0.1079]], requires_grad=True) forward_w\n",
            "tensor([[-0.7236],\n",
            "        [ 1.2552],\n",
            "        [ 0.1079]], requires_grad=True) loss_w\n",
            "tensor(0.0144, grad_fn=<MeanBackward0>) Loss 162\n",
            "tensor([[-0.7245],\n",
            "        [ 1.2559],\n",
            "        [ 0.1098]], requires_grad=True) init_w\n",
            "tensor([[-0.7245],\n",
            "        [ 1.2559],\n",
            "        [ 0.1098]], requires_grad=True) forward_w\n",
            "tensor([[-0.7245],\n",
            "        [ 1.2559],\n",
            "        [ 0.1098]], requires_grad=True) loss_w\n",
            "tensor(0.0139, grad_fn=<MeanBackward0>) Loss 163\n",
            "tensor([[-0.7253],\n",
            "        [ 1.2565],\n",
            "        [ 0.1117]], requires_grad=True) init_w\n",
            "tensor([[-0.7253],\n",
            "        [ 1.2565],\n",
            "        [ 0.1117]], requires_grad=True) forward_w\n",
            "tensor([[-0.7253],\n",
            "        [ 1.2565],\n",
            "        [ 0.1117]], requires_grad=True) loss_w\n",
            "tensor(0.0135, grad_fn=<MeanBackward0>) Loss 164\n",
            "tensor([[-0.7261],\n",
            "        [ 1.2571],\n",
            "        [ 0.1135]], requires_grad=True) init_w\n",
            "tensor([[-0.7261],\n",
            "        [ 1.2571],\n",
            "        [ 0.1135]], requires_grad=True) forward_w\n",
            "tensor([[-0.7261],\n",
            "        [ 1.2571],\n",
            "        [ 0.1135]], requires_grad=True) loss_w\n",
            "tensor(0.0131, grad_fn=<MeanBackward0>) Loss 165\n",
            "tensor([[-0.7269],\n",
            "        [ 1.2577],\n",
            "        [ 0.1153]], requires_grad=True) init_w\n",
            "tensor([[-0.7269],\n",
            "        [ 1.2577],\n",
            "        [ 0.1153]], requires_grad=True) forward_w\n",
            "tensor([[-0.7269],\n",
            "        [ 1.2577],\n",
            "        [ 0.1153]], requires_grad=True) loss_w\n",
            "tensor(0.0126, grad_fn=<MeanBackward0>) Loss 166\n",
            "tensor([[-0.7277],\n",
            "        [ 1.2583],\n",
            "        [ 0.1171]], requires_grad=True) init_w\n",
            "tensor([[-0.7277],\n",
            "        [ 1.2583],\n",
            "        [ 0.1171]], requires_grad=True) forward_w\n",
            "tensor([[-0.7277],\n",
            "        [ 1.2583],\n",
            "        [ 0.1171]], requires_grad=True) loss_w\n",
            "tensor(0.0122, grad_fn=<MeanBackward0>) Loss 167\n",
            "tensor([[-0.7285],\n",
            "        [ 1.2588],\n",
            "        [ 0.1188]], requires_grad=True) init_w\n",
            "tensor([[-0.7285],\n",
            "        [ 1.2588],\n",
            "        [ 0.1188]], requires_grad=True) forward_w\n",
            "tensor([[-0.7285],\n",
            "        [ 1.2588],\n",
            "        [ 0.1188]], requires_grad=True) loss_w\n",
            "tensor(0.0118, grad_fn=<MeanBackward0>) Loss 168\n",
            "tensor([[-0.7293],\n",
            "        [ 1.2594],\n",
            "        [ 0.1205]], requires_grad=True) init_w\n",
            "tensor([[-0.7293],\n",
            "        [ 1.2594],\n",
            "        [ 0.1205]], requires_grad=True) forward_w\n",
            "tensor([[-0.7293],\n",
            "        [ 1.2594],\n",
            "        [ 0.1205]], requires_grad=True) loss_w\n",
            "tensor(0.0114, grad_fn=<MeanBackward0>) Loss 169\n",
            "tensor([[-0.7301],\n",
            "        [ 1.2600],\n",
            "        [ 0.1222]], requires_grad=True) init_w\n",
            "tensor([[-0.7301],\n",
            "        [ 1.2600],\n",
            "        [ 0.1222]], requires_grad=True) forward_w\n",
            "tensor([[-0.7301],\n",
            "        [ 1.2600],\n",
            "        [ 0.1222]], requires_grad=True) loss_w\n",
            "tensor(0.0111, grad_fn=<MeanBackward0>) Loss 170\n",
            "tensor([[-0.7308],\n",
            "        [ 1.2605],\n",
            "        [ 0.1239]], requires_grad=True) init_w\n",
            "tensor([[-0.7308],\n",
            "        [ 1.2605],\n",
            "        [ 0.1239]], requires_grad=True) forward_w\n",
            "tensor([[-0.7308],\n",
            "        [ 1.2605],\n",
            "        [ 0.1239]], requires_grad=True) loss_w\n",
            "tensor(0.0107, grad_fn=<MeanBackward0>) Loss 171\n",
            "tensor([[-0.7316],\n",
            "        [ 1.2610],\n",
            "        [ 0.1255]], requires_grad=True) init_w\n",
            "tensor([[-0.7316],\n",
            "        [ 1.2610],\n",
            "        [ 0.1255]], requires_grad=True) forward_w\n",
            "tensor([[-0.7316],\n",
            "        [ 1.2610],\n",
            "        [ 0.1255]], requires_grad=True) loss_w\n",
            "tensor(0.0104, grad_fn=<MeanBackward0>) Loss 172\n",
            "tensor([[-0.7323],\n",
            "        [ 1.2616],\n",
            "        [ 0.1271]], requires_grad=True) init_w\n",
            "tensor([[-0.7323],\n",
            "        [ 1.2616],\n",
            "        [ 0.1271]], requires_grad=True) forward_w\n",
            "tensor([[-0.7323],\n",
            "        [ 1.2616],\n",
            "        [ 0.1271]], requires_grad=True) loss_w\n",
            "tensor(0.0100, grad_fn=<MeanBackward0>) Loss 173\n",
            "tensor([[-0.7330],\n",
            "        [ 1.2621],\n",
            "        [ 0.1287]], requires_grad=True) init_w\n",
            "tensor([[-0.7330],\n",
            "        [ 1.2621],\n",
            "        [ 0.1287]], requires_grad=True) forward_w\n",
            "tensor([[-0.7330],\n",
            "        [ 1.2621],\n",
            "        [ 0.1287]], requires_grad=True) loss_w\n",
            "tensor(0.0097, grad_fn=<MeanBackward0>) Loss 174\n",
            "tensor([[-0.7337],\n",
            "        [ 1.2626],\n",
            "        [ 0.1303]], requires_grad=True) init_w\n",
            "tensor([[-0.7337],\n",
            "        [ 1.2626],\n",
            "        [ 0.1303]], requires_grad=True) forward_w\n",
            "tensor([[-0.7337],\n",
            "        [ 1.2626],\n",
            "        [ 0.1303]], requires_grad=True) loss_w\n",
            "tensor(0.0094, grad_fn=<MeanBackward0>) Loss 175\n",
            "tensor([[-0.7344],\n",
            "        [ 1.2631],\n",
            "        [ 0.1318]], requires_grad=True) init_w\n",
            "tensor([[-0.7344],\n",
            "        [ 1.2631],\n",
            "        [ 0.1318]], requires_grad=True) forward_w\n",
            "tensor([[-0.7344],\n",
            "        [ 1.2631],\n",
            "        [ 0.1318]], requires_grad=True) loss_w\n",
            "tensor(0.0091, grad_fn=<MeanBackward0>) Loss 176\n",
            "tensor([[-0.7351],\n",
            "        [ 1.2636],\n",
            "        [ 0.1333]], requires_grad=True) init_w\n",
            "tensor([[-0.7351],\n",
            "        [ 1.2636],\n",
            "        [ 0.1333]], requires_grad=True) forward_w\n",
            "tensor([[-0.7351],\n",
            "        [ 1.2636],\n",
            "        [ 0.1333]], requires_grad=True) loss_w\n",
            "tensor(0.0088, grad_fn=<MeanBackward0>) Loss 177\n",
            "tensor([[-0.7357],\n",
            "        [ 1.2641],\n",
            "        [ 0.1348]], requires_grad=True) init_w\n",
            "tensor([[-0.7357],\n",
            "        [ 1.2641],\n",
            "        [ 0.1348]], requires_grad=True) forward_w\n",
            "tensor([[-0.7357],\n",
            "        [ 1.2641],\n",
            "        [ 0.1348]], requires_grad=True) loss_w\n",
            "tensor(0.0085, grad_fn=<MeanBackward0>) Loss 178\n",
            "tensor([[-0.7364],\n",
            "        [ 1.2646],\n",
            "        [ 0.1362]], requires_grad=True) init_w\n",
            "tensor([[-0.7364],\n",
            "        [ 1.2646],\n",
            "        [ 0.1362]], requires_grad=True) forward_w\n",
            "tensor([[-0.7364],\n",
            "        [ 1.2646],\n",
            "        [ 0.1362]], requires_grad=True) loss_w\n",
            "tensor(0.0082, grad_fn=<MeanBackward0>) Loss 179\n",
            "tensor([[-0.7370],\n",
            "        [ 1.2650],\n",
            "        [ 0.1377]], requires_grad=True) init_w\n",
            "tensor([[-0.7370],\n",
            "        [ 1.2650],\n",
            "        [ 0.1377]], requires_grad=True) forward_w\n",
            "tensor([[-0.7370],\n",
            "        [ 1.2650],\n",
            "        [ 0.1377]], requires_grad=True) loss_w\n",
            "tensor(0.0080, grad_fn=<MeanBackward0>) Loss 180\n",
            "tensor([[-0.7377],\n",
            "        [ 1.2655],\n",
            "        [ 0.1391]], requires_grad=True) init_w\n",
            "tensor([[-0.7377],\n",
            "        [ 1.2655],\n",
            "        [ 0.1391]], requires_grad=True) forward_w\n",
            "tensor([[-0.7377],\n",
            "        [ 1.2655],\n",
            "        [ 0.1391]], requires_grad=True) loss_w\n",
            "tensor(0.0077, grad_fn=<MeanBackward0>) Loss 181\n",
            "tensor([[-0.7383],\n",
            "        [ 1.2659],\n",
            "        [ 0.1404]], requires_grad=True) init_w\n",
            "tensor([[-0.7383],\n",
            "        [ 1.2659],\n",
            "        [ 0.1404]], requires_grad=True) forward_w\n",
            "tensor([[-0.7383],\n",
            "        [ 1.2659],\n",
            "        [ 0.1404]], requires_grad=True) loss_w\n",
            "tensor(0.0075, grad_fn=<MeanBackward0>) Loss 182\n",
            "tensor([[-0.7389],\n",
            "        [ 1.2664],\n",
            "        [ 0.1418]], requires_grad=True) init_w\n",
            "tensor([[-0.7389],\n",
            "        [ 1.2664],\n",
            "        [ 0.1418]], requires_grad=True) forward_w\n",
            "tensor([[-0.7389],\n",
            "        [ 1.2664],\n",
            "        [ 0.1418]], requires_grad=True) loss_w\n",
            "tensor(0.0072, grad_fn=<MeanBackward0>) Loss 183\n",
            "tensor([[-0.7395],\n",
            "        [ 1.2668],\n",
            "        [ 0.1431]], requires_grad=True) init_w\n",
            "tensor([[-0.7395],\n",
            "        [ 1.2668],\n",
            "        [ 0.1431]], requires_grad=True) forward_w\n",
            "tensor([[-0.7395],\n",
            "        [ 1.2668],\n",
            "        [ 0.1431]], requires_grad=True) loss_w\n",
            "tensor(0.0070, grad_fn=<MeanBackward0>) Loss 184\n",
            "tensor([[-0.7401],\n",
            "        [ 1.2673],\n",
            "        [ 0.1445]], requires_grad=True) init_w\n",
            "tensor([[-0.7401],\n",
            "        [ 1.2673],\n",
            "        [ 0.1445]], requires_grad=True) forward_w\n",
            "tensor([[-0.7401],\n",
            "        [ 1.2673],\n",
            "        [ 0.1445]], requires_grad=True) loss_w\n",
            "tensor(0.0068, grad_fn=<MeanBackward0>) Loss 185\n",
            "tensor([[-0.7407],\n",
            "        [ 1.2677],\n",
            "        [ 0.1458]], requires_grad=True) init_w\n",
            "tensor([[-0.7407],\n",
            "        [ 1.2677],\n",
            "        [ 0.1458]], requires_grad=True) forward_w\n",
            "tensor([[-0.7407],\n",
            "        [ 1.2677],\n",
            "        [ 0.1458]], requires_grad=True) loss_w\n",
            "tensor(0.0065, grad_fn=<MeanBackward0>) Loss 186\n",
            "tensor([[-0.7413],\n",
            "        [ 1.2681],\n",
            "        [ 0.1470]], requires_grad=True) init_w\n",
            "tensor([[-0.7413],\n",
            "        [ 1.2681],\n",
            "        [ 0.1470]], requires_grad=True) forward_w\n",
            "tensor([[-0.7413],\n",
            "        [ 1.2681],\n",
            "        [ 0.1470]], requires_grad=True) loss_w\n",
            "tensor(0.0063, grad_fn=<MeanBackward0>) Loss 187\n",
            "tensor([[-0.7418],\n",
            "        [ 1.2685],\n",
            "        [ 0.1483]], requires_grad=True) init_w\n",
            "tensor([[-0.7418],\n",
            "        [ 1.2685],\n",
            "        [ 0.1483]], requires_grad=True) forward_w\n",
            "tensor([[-0.7418],\n",
            "        [ 1.2685],\n",
            "        [ 0.1483]], requires_grad=True) loss_w\n",
            "tensor(0.0061, grad_fn=<MeanBackward0>) Loss 188\n",
            "tensor([[-0.7424],\n",
            "        [ 1.2689],\n",
            "        [ 0.1495]], requires_grad=True) init_w\n",
            "tensor([[-0.7424],\n",
            "        [ 1.2689],\n",
            "        [ 0.1495]], requires_grad=True) forward_w\n",
            "tensor([[-0.7424],\n",
            "        [ 1.2689],\n",
            "        [ 0.1495]], requires_grad=True) loss_w\n",
            "tensor(0.0059, grad_fn=<MeanBackward0>) Loss 189\n",
            "tensor([[-0.7430],\n",
            "        [ 1.2693],\n",
            "        [ 0.1507]], requires_grad=True) init_w\n",
            "tensor([[-0.7430],\n",
            "        [ 1.2693],\n",
            "        [ 0.1507]], requires_grad=True) forward_w\n",
            "tensor([[-0.7430],\n",
            "        [ 1.2693],\n",
            "        [ 0.1507]], requires_grad=True) loss_w\n",
            "tensor(0.0057, grad_fn=<MeanBackward0>) Loss 190\n",
            "tensor([[-0.7435],\n",
            "        [ 1.2697],\n",
            "        [ 0.1519]], requires_grad=True) init_w\n",
            "tensor([[-0.7435],\n",
            "        [ 1.2697],\n",
            "        [ 0.1519]], requires_grad=True) forward_w\n",
            "tensor([[-0.7435],\n",
            "        [ 1.2697],\n",
            "        [ 0.1519]], requires_grad=True) loss_w\n",
            "tensor(0.0056, grad_fn=<MeanBackward0>) Loss 191\n",
            "tensor([[-0.7440],\n",
            "        [ 1.2701],\n",
            "        [ 0.1531]], requires_grad=True) init_w\n",
            "tensor([[-0.7440],\n",
            "        [ 1.2701],\n",
            "        [ 0.1531]], requires_grad=True) forward_w\n",
            "tensor([[-0.7440],\n",
            "        [ 1.2701],\n",
            "        [ 0.1531]], requires_grad=True) loss_w\n",
            "tensor(0.0054, grad_fn=<MeanBackward0>) Loss 192\n",
            "tensor([[-0.7445],\n",
            "        [ 1.2705],\n",
            "        [ 0.1543]], requires_grad=True) init_w\n",
            "tensor([[-0.7445],\n",
            "        [ 1.2705],\n",
            "        [ 0.1543]], requires_grad=True) forward_w\n",
            "tensor([[-0.7445],\n",
            "        [ 1.2705],\n",
            "        [ 0.1543]], requires_grad=True) loss_w\n",
            "tensor(0.0052, grad_fn=<MeanBackward0>) Loss 193\n",
            "tensor([[-0.7451],\n",
            "        [ 1.2709],\n",
            "        [ 0.1554]], requires_grad=True) init_w\n",
            "tensor([[-0.7451],\n",
            "        [ 1.2709],\n",
            "        [ 0.1554]], requires_grad=True) forward_w\n",
            "tensor([[-0.7451],\n",
            "        [ 1.2709],\n",
            "        [ 0.1554]], requires_grad=True) loss_w\n",
            "tensor(0.0050, grad_fn=<MeanBackward0>) Loss 194\n",
            "tensor([[-0.7456],\n",
            "        [ 1.2712],\n",
            "        [ 0.1565]], requires_grad=True) init_w\n",
            "tensor([[-0.7456],\n",
            "        [ 1.2712],\n",
            "        [ 0.1565]], requires_grad=True) forward_w\n",
            "tensor([[-0.7456],\n",
            "        [ 1.2712],\n",
            "        [ 0.1565]], requires_grad=True) loss_w\n",
            "tensor(0.0049, grad_fn=<MeanBackward0>) Loss 195\n",
            "tensor([[-0.7461],\n",
            "        [ 1.2716],\n",
            "        [ 0.1576]], requires_grad=True) init_w\n",
            "tensor([[-0.7461],\n",
            "        [ 1.2716],\n",
            "        [ 0.1576]], requires_grad=True) forward_w\n",
            "tensor([[-0.7461],\n",
            "        [ 1.2716],\n",
            "        [ 0.1576]], requires_grad=True) loss_w\n",
            "tensor(0.0047, grad_fn=<MeanBackward0>) Loss 196\n",
            "tensor([[-0.7465],\n",
            "        [ 1.2719],\n",
            "        [ 0.1587]], requires_grad=True) init_w\n",
            "tensor([[-0.7465],\n",
            "        [ 1.2719],\n",
            "        [ 0.1587]], requires_grad=True) forward_w\n",
            "tensor([[-0.7465],\n",
            "        [ 1.2719],\n",
            "        [ 0.1587]], requires_grad=True) loss_w\n",
            "tensor(0.0046, grad_fn=<MeanBackward0>) Loss 197\n",
            "tensor([[-0.7470],\n",
            "        [ 1.2723],\n",
            "        [ 0.1598]], requires_grad=True) init_w\n",
            "tensor([[-0.7470],\n",
            "        [ 1.2723],\n",
            "        [ 0.1598]], requires_grad=True) forward_w\n",
            "tensor([[-0.7470],\n",
            "        [ 1.2723],\n",
            "        [ 0.1598]], requires_grad=True) loss_w\n",
            "tensor(0.0044, grad_fn=<MeanBackward0>) Loss 198\n",
            "tensor([[-0.7475],\n",
            "        [ 1.2726],\n",
            "        [ 0.1608]], requires_grad=True) init_w\n",
            "tensor([[-0.7475],\n",
            "        [ 1.2726],\n",
            "        [ 0.1608]], requires_grad=True) forward_w\n",
            "tensor([[-0.7475],\n",
            "        [ 1.2726],\n",
            "        [ 0.1608]], requires_grad=True) loss_w\n",
            "tensor(0.0043, grad_fn=<MeanBackward0>) Loss 199\n",
            "tensor([[-0.7480],\n",
            "        [ 1.2730],\n",
            "        [ 0.1618]], requires_grad=True) init_w\n",
            "tensor([[-0.7480],\n",
            "        [ 1.2730],\n",
            "        [ 0.1618]], requires_grad=True) forward_w\n",
            "tensor([[-0.7480],\n",
            "        [ 1.2730],\n",
            "        [ 0.1618]], requires_grad=True) loss_w\n",
            "tensor(0.0041, grad_fn=<MeanBackward0>) Loss 200\n",
            "tensor([[-0.7484],\n",
            "        [ 1.2733],\n",
            "        [ 0.1629]], requires_grad=True) init_w\n",
            "tensor([[-0.7484],\n",
            "        [ 1.2733],\n",
            "        [ 0.1629]], requires_grad=True) forward_w\n",
            "tensor([[-0.7484],\n",
            "        [ 1.2733],\n",
            "        [ 0.1629]], requires_grad=True) loss_w\n",
            "tensor(0.0040, grad_fn=<MeanBackward0>) Loss 201\n",
            "tensor([[-0.7489],\n",
            "        [ 1.2736],\n",
            "        [ 0.1638]], requires_grad=True) init_w\n",
            "tensor([[-0.7489],\n",
            "        [ 1.2736],\n",
            "        [ 0.1638]], requires_grad=True) forward_w\n",
            "tensor([[-0.7489],\n",
            "        [ 1.2736],\n",
            "        [ 0.1638]], requires_grad=True) loss_w\n",
            "tensor(0.0039, grad_fn=<MeanBackward0>) Loss 202\n",
            "tensor([[-0.7493],\n",
            "        [ 1.2740],\n",
            "        [ 0.1648]], requires_grad=True) init_w\n",
            "tensor([[-0.7493],\n",
            "        [ 1.2740],\n",
            "        [ 0.1648]], requires_grad=True) forward_w\n",
            "tensor([[-0.7493],\n",
            "        [ 1.2740],\n",
            "        [ 0.1648]], requires_grad=True) loss_w\n",
            "tensor(0.0037, grad_fn=<MeanBackward0>) Loss 203\n",
            "tensor([[-0.7498],\n",
            "        [ 1.2743],\n",
            "        [ 0.1658]], requires_grad=True) init_w\n",
            "tensor([[-0.7498],\n",
            "        [ 1.2743],\n",
            "        [ 0.1658]], requires_grad=True) forward_w\n",
            "tensor([[-0.7498],\n",
            "        [ 1.2743],\n",
            "        [ 0.1658]], requires_grad=True) loss_w\n",
            "tensor(0.0036, grad_fn=<MeanBackward0>) Loss 204\n",
            "tensor([[-0.7502],\n",
            "        [ 1.2746],\n",
            "        [ 0.1667]], requires_grad=True) init_w\n",
            "tensor([[-0.7502],\n",
            "        [ 1.2746],\n",
            "        [ 0.1667]], requires_grad=True) forward_w\n",
            "tensor([[-0.7502],\n",
            "        [ 1.2746],\n",
            "        [ 0.1667]], requires_grad=True) loss_w\n",
            "tensor(0.0035, grad_fn=<MeanBackward0>) Loss 205\n",
            "tensor([[-0.7506],\n",
            "        [ 1.2749],\n",
            "        [ 0.1677]], requires_grad=True) init_w\n",
            "tensor([[-0.7506],\n",
            "        [ 1.2749],\n",
            "        [ 0.1677]], requires_grad=True) forward_w\n",
            "tensor([[-0.7506],\n",
            "        [ 1.2749],\n",
            "        [ 0.1677]], requires_grad=True) loss_w\n",
            "tensor(0.0034, grad_fn=<MeanBackward0>) Loss 206\n",
            "tensor([[-0.7510],\n",
            "        [ 1.2752],\n",
            "        [ 0.1686]], requires_grad=True) init_w\n",
            "tensor([[-0.7510],\n",
            "        [ 1.2752],\n",
            "        [ 0.1686]], requires_grad=True) forward_w\n",
            "tensor([[-0.7510],\n",
            "        [ 1.2752],\n",
            "        [ 0.1686]], requires_grad=True) loss_w\n",
            "tensor(0.0033, grad_fn=<MeanBackward0>) Loss 207\n",
            "tensor([[-0.7514],\n",
            "        [ 1.2755],\n",
            "        [ 0.1695]], requires_grad=True) init_w\n",
            "tensor([[-0.7514],\n",
            "        [ 1.2755],\n",
            "        [ 0.1695]], requires_grad=True) forward_w\n",
            "tensor([[-0.7514],\n",
            "        [ 1.2755],\n",
            "        [ 0.1695]], requires_grad=True) loss_w\n",
            "tensor(0.0032, grad_fn=<MeanBackward0>) Loss 208\n",
            "tensor([[-0.7518],\n",
            "        [ 1.2758],\n",
            "        [ 0.1704]], requires_grad=True) init_w\n",
            "tensor([[-0.7518],\n",
            "        [ 1.2758],\n",
            "        [ 0.1704]], requires_grad=True) forward_w\n",
            "tensor([[-0.7518],\n",
            "        [ 1.2758],\n",
            "        [ 0.1704]], requires_grad=True) loss_w\n",
            "tensor(0.0031, grad_fn=<MeanBackward0>) Loss 209\n",
            "tensor([[-0.7522],\n",
            "        [ 1.2761],\n",
            "        [ 0.1713]], requires_grad=True) init_w\n",
            "tensor([[-0.7522],\n",
            "        [ 1.2761],\n",
            "        [ 0.1713]], requires_grad=True) forward_w\n",
            "tensor([[-0.7522],\n",
            "        [ 1.2761],\n",
            "        [ 0.1713]], requires_grad=True) loss_w\n",
            "tensor(0.0030, grad_fn=<MeanBackward0>) Loss 210\n",
            "tensor([[-0.7526],\n",
            "        [ 1.2764],\n",
            "        [ 0.1721]], requires_grad=True) init_w\n",
            "tensor([[-0.7526],\n",
            "        [ 1.2764],\n",
            "        [ 0.1721]], requires_grad=True) forward_w\n",
            "tensor([[-0.7526],\n",
            "        [ 1.2764],\n",
            "        [ 0.1721]], requires_grad=True) loss_w\n",
            "tensor(0.0029, grad_fn=<MeanBackward0>) Loss 211\n",
            "tensor([[-0.7530],\n",
            "        [ 1.2766],\n",
            "        [ 0.1730]], requires_grad=True) init_w\n",
            "tensor([[-0.7530],\n",
            "        [ 1.2766],\n",
            "        [ 0.1730]], requires_grad=True) forward_w\n",
            "tensor([[-0.7530],\n",
            "        [ 1.2766],\n",
            "        [ 0.1730]], requires_grad=True) loss_w\n",
            "tensor(0.0028, grad_fn=<MeanBackward0>) Loss 212\n",
            "tensor([[-0.7534],\n",
            "        [ 1.2769],\n",
            "        [ 0.1738]], requires_grad=True) init_w\n",
            "tensor([[-0.7534],\n",
            "        [ 1.2769],\n",
            "        [ 0.1738]], requires_grad=True) forward_w\n",
            "tensor([[-0.7534],\n",
            "        [ 1.2769],\n",
            "        [ 0.1738]], requires_grad=True) loss_w\n",
            "tensor(0.0027, grad_fn=<MeanBackward0>) Loss 213\n",
            "tensor([[-0.7537],\n",
            "        [ 1.2772],\n",
            "        [ 0.1746]], requires_grad=True) init_w\n",
            "tensor([[-0.7537],\n",
            "        [ 1.2772],\n",
            "        [ 0.1746]], requires_grad=True) forward_w\n",
            "tensor([[-0.7537],\n",
            "        [ 1.2772],\n",
            "        [ 0.1746]], requires_grad=True) loss_w\n",
            "tensor(0.0026, grad_fn=<MeanBackward0>) Loss 214\n",
            "tensor([[-0.7541],\n",
            "        [ 1.2774],\n",
            "        [ 0.1754]], requires_grad=True) init_w\n",
            "tensor([[-0.7541],\n",
            "        [ 1.2774],\n",
            "        [ 0.1754]], requires_grad=True) forward_w\n",
            "tensor([[-0.7541],\n",
            "        [ 1.2774],\n",
            "        [ 0.1754]], requires_grad=True) loss_w\n",
            "tensor(0.0025, grad_fn=<MeanBackward0>) Loss 215\n",
            "tensor([[-0.7545],\n",
            "        [ 1.2777],\n",
            "        [ 0.1762]], requires_grad=True) init_w\n",
            "tensor([[-0.7545],\n",
            "        [ 1.2777],\n",
            "        [ 0.1762]], requires_grad=True) forward_w\n",
            "tensor([[-0.7545],\n",
            "        [ 1.2777],\n",
            "        [ 0.1762]], requires_grad=True) loss_w\n",
            "tensor(0.0024, grad_fn=<MeanBackward0>) Loss 216\n",
            "tensor([[-0.7548],\n",
            "        [ 1.2780],\n",
            "        [ 0.1770]], requires_grad=True) init_w\n",
            "tensor([[-0.7548],\n",
            "        [ 1.2780],\n",
            "        [ 0.1770]], requires_grad=True) forward_w\n",
            "tensor([[-0.7548],\n",
            "        [ 1.2780],\n",
            "        [ 0.1770]], requires_grad=True) loss_w\n",
            "tensor(0.0024, grad_fn=<MeanBackward0>) Loss 217\n",
            "tensor([[-0.7552],\n",
            "        [ 1.2782],\n",
            "        [ 0.1778]], requires_grad=True) init_w\n",
            "tensor([[-0.7552],\n",
            "        [ 1.2782],\n",
            "        [ 0.1778]], requires_grad=True) forward_w\n",
            "tensor([[-0.7552],\n",
            "        [ 1.2782],\n",
            "        [ 0.1778]], requires_grad=True) loss_w\n",
            "tensor(0.0023, grad_fn=<MeanBackward0>) Loss 218\n",
            "tensor([[-0.7555],\n",
            "        [ 1.2785],\n",
            "        [ 0.1785]], requires_grad=True) init_w\n",
            "tensor([[-0.7555],\n",
            "        [ 1.2785],\n",
            "        [ 0.1785]], requires_grad=True) forward_w\n",
            "tensor([[-0.7555],\n",
            "        [ 1.2785],\n",
            "        [ 0.1785]], requires_grad=True) loss_w\n",
            "tensor(0.0022, grad_fn=<MeanBackward0>) Loss 219\n",
            "tensor([[-0.7558],\n",
            "        [ 1.2787],\n",
            "        [ 0.1792]], requires_grad=True) init_w\n",
            "tensor([[-0.7558],\n",
            "        [ 1.2787],\n",
            "        [ 0.1792]], requires_grad=True) forward_w\n",
            "tensor([[-0.7558],\n",
            "        [ 1.2787],\n",
            "        [ 0.1792]], requires_grad=True) loss_w\n",
            "tensor(0.0021, grad_fn=<MeanBackward0>) Loss 220\n",
            "tensor([[-0.7562],\n",
            "        [ 1.2789],\n",
            "        [ 0.1800]], requires_grad=True) init_w\n",
            "tensor([[-0.7562],\n",
            "        [ 1.2789],\n",
            "        [ 0.1800]], requires_grad=True) forward_w\n",
            "tensor([[-0.7562],\n",
            "        [ 1.2789],\n",
            "        [ 0.1800]], requires_grad=True) loss_w\n",
            "tensor(0.0021, grad_fn=<MeanBackward0>) Loss 221\n",
            "tensor([[-0.7565],\n",
            "        [ 1.2792],\n",
            "        [ 0.1807]], requires_grad=True) init_w\n",
            "tensor([[-0.7565],\n",
            "        [ 1.2792],\n",
            "        [ 0.1807]], requires_grad=True) forward_w\n",
            "tensor([[-0.7565],\n",
            "        [ 1.2792],\n",
            "        [ 0.1807]], requires_grad=True) loss_w\n",
            "tensor(0.0020, grad_fn=<MeanBackward0>) Loss 222\n",
            "tensor([[-0.7568],\n",
            "        [ 1.2794],\n",
            "        [ 0.1814]], requires_grad=True) init_w\n",
            "tensor([[-0.7568],\n",
            "        [ 1.2794],\n",
            "        [ 0.1814]], requires_grad=True) forward_w\n",
            "tensor([[-0.7568],\n",
            "        [ 1.2794],\n",
            "        [ 0.1814]], requires_grad=True) loss_w\n",
            "tensor(0.0019, grad_fn=<MeanBackward0>) Loss 223\n",
            "tensor([[-0.7571],\n",
            "        [ 1.2796],\n",
            "        [ 0.1821]], requires_grad=True) init_w\n",
            "tensor([[-0.7571],\n",
            "        [ 1.2796],\n",
            "        [ 0.1821]], requires_grad=True) forward_w\n",
            "tensor([[-0.7571],\n",
            "        [ 1.2796],\n",
            "        [ 0.1821]], requires_grad=True) loss_w\n",
            "tensor(0.0019, grad_fn=<MeanBackward0>) Loss 224\n",
            "tensor([[-0.7574],\n",
            "        [ 1.2799],\n",
            "        [ 0.1828]], requires_grad=True) init_w\n",
            "tensor([[-0.7574],\n",
            "        [ 1.2799],\n",
            "        [ 0.1828]], requires_grad=True) forward_w\n",
            "tensor([[-0.7574],\n",
            "        [ 1.2799],\n",
            "        [ 0.1828]], requires_grad=True) loss_w\n",
            "tensor(0.0018, grad_fn=<MeanBackward0>) Loss 225\n",
            "tensor([[-0.7577],\n",
            "        [ 1.2801],\n",
            "        [ 0.1834]], requires_grad=True) init_w\n",
            "tensor([[-0.7577],\n",
            "        [ 1.2801],\n",
            "        [ 0.1834]], requires_grad=True) forward_w\n",
            "tensor([[-0.7577],\n",
            "        [ 1.2801],\n",
            "        [ 0.1834]], requires_grad=True) loss_w\n",
            "tensor(0.0018, grad_fn=<MeanBackward0>) Loss 226\n",
            "tensor([[-0.7580],\n",
            "        [ 1.2803],\n",
            "        [ 0.1841]], requires_grad=True) init_w\n",
            "tensor([[-0.7580],\n",
            "        [ 1.2803],\n",
            "        [ 0.1841]], requires_grad=True) forward_w\n",
            "tensor([[-0.7580],\n",
            "        [ 1.2803],\n",
            "        [ 0.1841]], requires_grad=True) loss_w\n",
            "tensor(0.0017, grad_fn=<MeanBackward0>) Loss 227\n",
            "tensor([[-0.7583],\n",
            "        [ 1.2805],\n",
            "        [ 0.1848]], requires_grad=True) init_w\n",
            "tensor([[-0.7583],\n",
            "        [ 1.2805],\n",
            "        [ 0.1848]], requires_grad=True) forward_w\n",
            "tensor([[-0.7583],\n",
            "        [ 1.2805],\n",
            "        [ 0.1848]], requires_grad=True) loss_w\n",
            "tensor(0.0016, grad_fn=<MeanBackward0>) Loss 228\n",
            "tensor([[-0.7586],\n",
            "        [ 1.2807],\n",
            "        [ 0.1854]], requires_grad=True) init_w\n",
            "tensor([[-0.7586],\n",
            "        [ 1.2807],\n",
            "        [ 0.1854]], requires_grad=True) forward_w\n",
            "tensor([[-0.7586],\n",
            "        [ 1.2807],\n",
            "        [ 0.1854]], requires_grad=True) loss_w\n",
            "tensor(0.0016, grad_fn=<MeanBackward0>) Loss 229\n",
            "tensor([[-0.7589],\n",
            "        [ 1.2809],\n",
            "        [ 0.1860]], requires_grad=True) init_w\n",
            "tensor([[-0.7589],\n",
            "        [ 1.2809],\n",
            "        [ 0.1860]], requires_grad=True) forward_w\n",
            "tensor([[-0.7589],\n",
            "        [ 1.2809],\n",
            "        [ 0.1860]], requires_grad=True) loss_w\n",
            "tensor(0.0015, grad_fn=<MeanBackward0>) Loss 230\n",
            "tensor([[-0.7592],\n",
            "        [ 1.2811],\n",
            "        [ 0.1866]], requires_grad=True) init_w\n",
            "tensor([[-0.7592],\n",
            "        [ 1.2811],\n",
            "        [ 0.1866]], requires_grad=True) forward_w\n",
            "tensor([[-0.7592],\n",
            "        [ 1.2811],\n",
            "        [ 0.1866]], requires_grad=True) loss_w\n",
            "tensor(0.0015, grad_fn=<MeanBackward0>) Loss 231\n",
            "tensor([[-0.7595],\n",
            "        [ 1.2813],\n",
            "        [ 0.1873]], requires_grad=True) init_w\n",
            "tensor([[-0.7595],\n",
            "        [ 1.2813],\n",
            "        [ 0.1873]], requires_grad=True) forward_w\n",
            "tensor([[-0.7595],\n",
            "        [ 1.2813],\n",
            "        [ 0.1873]], requires_grad=True) loss_w\n",
            "tensor(0.0014, grad_fn=<MeanBackward0>) Loss 232\n",
            "tensor([[-0.7597],\n",
            "        [ 1.2815],\n",
            "        [ 0.1878]], requires_grad=True) init_w\n",
            "tensor([[-0.7597],\n",
            "        [ 1.2815],\n",
            "        [ 0.1878]], requires_grad=True) forward_w\n",
            "tensor([[-0.7597],\n",
            "        [ 1.2815],\n",
            "        [ 0.1878]], requires_grad=True) loss_w\n",
            "tensor(0.0014, grad_fn=<MeanBackward0>) Loss 233\n",
            "tensor([[-0.7600],\n",
            "        [ 1.2817],\n",
            "        [ 0.1884]], requires_grad=True) init_w\n",
            "tensor([[-0.7600],\n",
            "        [ 1.2817],\n",
            "        [ 0.1884]], requires_grad=True) forward_w\n",
            "tensor([[-0.7600],\n",
            "        [ 1.2817],\n",
            "        [ 0.1884]], requires_grad=True) loss_w\n",
            "tensor(0.0013, grad_fn=<MeanBackward0>) Loss 234\n",
            "tensor([[-0.7603],\n",
            "        [ 1.2819],\n",
            "        [ 0.1890]], requires_grad=True) init_w\n",
            "tensor([[-0.7603],\n",
            "        [ 1.2819],\n",
            "        [ 0.1890]], requires_grad=True) forward_w\n",
            "tensor([[-0.7603],\n",
            "        [ 1.2819],\n",
            "        [ 0.1890]], requires_grad=True) loss_w\n",
            "tensor(0.0013, grad_fn=<MeanBackward0>) Loss 235\n",
            "tensor([[-0.7605],\n",
            "        [ 1.2821],\n",
            "        [ 0.1896]], requires_grad=True) init_w\n",
            "tensor([[-0.7605],\n",
            "        [ 1.2821],\n",
            "        [ 0.1896]], requires_grad=True) forward_w\n",
            "tensor([[-0.7605],\n",
            "        [ 1.2821],\n",
            "        [ 0.1896]], requires_grad=True) loss_w\n",
            "tensor(0.0013, grad_fn=<MeanBackward0>) Loss 236\n",
            "tensor([[-0.7608],\n",
            "        [ 1.2823],\n",
            "        [ 0.1901]], requires_grad=True) init_w\n",
            "tensor([[-0.7608],\n",
            "        [ 1.2823],\n",
            "        [ 0.1901]], requires_grad=True) forward_w\n",
            "tensor([[-0.7608],\n",
            "        [ 1.2823],\n",
            "        [ 0.1901]], requires_grad=True) loss_w\n",
            "tensor(0.0012, grad_fn=<MeanBackward0>) Loss 237\n",
            "tensor([[-0.7610],\n",
            "        [ 1.2825],\n",
            "        [ 0.1907]], requires_grad=True) init_w\n",
            "tensor([[-0.7610],\n",
            "        [ 1.2825],\n",
            "        [ 0.1907]], requires_grad=True) forward_w\n",
            "tensor([[-0.7610],\n",
            "        [ 1.2825],\n",
            "        [ 0.1907]], requires_grad=True) loss_w\n",
            "tensor(0.0012, grad_fn=<MeanBackward0>) Loss 238\n",
            "tensor([[-0.7613],\n",
            "        [ 1.2826],\n",
            "        [ 0.1912]], requires_grad=True) init_w\n",
            "tensor([[-0.7613],\n",
            "        [ 1.2826],\n",
            "        [ 0.1912]], requires_grad=True) forward_w\n",
            "tensor([[-0.7613],\n",
            "        [ 1.2826],\n",
            "        [ 0.1912]], requires_grad=True) loss_w\n",
            "tensor(0.0011, grad_fn=<MeanBackward0>) Loss 239\n",
            "tensor([[-0.7615],\n",
            "        [ 1.2828],\n",
            "        [ 0.1918]], requires_grad=True) init_w\n",
            "tensor([[-0.7615],\n",
            "        [ 1.2828],\n",
            "        [ 0.1918]], requires_grad=True) forward_w\n",
            "tensor([[-0.7615],\n",
            "        [ 1.2828],\n",
            "        [ 0.1918]], requires_grad=True) loss_w\n",
            "tensor(0.0011, grad_fn=<MeanBackward0>) Loss 240\n",
            "tensor([[-0.7617],\n",
            "        [ 1.2830],\n",
            "        [ 0.1923]], requires_grad=True) init_w\n",
            "tensor([[-0.7617],\n",
            "        [ 1.2830],\n",
            "        [ 0.1923]], requires_grad=True) forward_w\n",
            "tensor([[-0.7617],\n",
            "        [ 1.2830],\n",
            "        [ 0.1923]], requires_grad=True) loss_w\n",
            "tensor(0.0011, grad_fn=<MeanBackward0>) Loss 241\n",
            "tensor([[-0.7620],\n",
            "        [ 1.2832],\n",
            "        [ 0.1928]], requires_grad=True) init_w\n",
            "tensor([[-0.7620],\n",
            "        [ 1.2832],\n",
            "        [ 0.1928]], requires_grad=True) forward_w\n",
            "tensor([[-0.7620],\n",
            "        [ 1.2832],\n",
            "        [ 0.1928]], requires_grad=True) loss_w\n",
            "tensor(0.0010, grad_fn=<MeanBackward0>) Loss 242\n",
            "tensor([[-0.7622],\n",
            "        [ 1.2833],\n",
            "        [ 0.1933]], requires_grad=True) init_w\n",
            "tensor([[-0.7622],\n",
            "        [ 1.2833],\n",
            "        [ 0.1933]], requires_grad=True) forward_w\n",
            "tensor([[-0.7622],\n",
            "        [ 1.2833],\n",
            "        [ 0.1933]], requires_grad=True) loss_w\n",
            "tensor(0.0010, grad_fn=<MeanBackward0>) Loss 243\n",
            "tensor([[-0.7624],\n",
            "        [ 1.2835],\n",
            "        [ 0.1938]], requires_grad=True) init_w\n",
            "tensor([[-0.7624],\n",
            "        [ 1.2835],\n",
            "        [ 0.1938]], requires_grad=True) forward_w\n",
            "tensor([[-0.7624],\n",
            "        [ 1.2835],\n",
            "        [ 0.1938]], requires_grad=True) loss_w\n",
            "tensor(0.0010, grad_fn=<MeanBackward0>) Loss 244\n",
            "tensor([[-0.7626],\n",
            "        [ 1.2837],\n",
            "        [ 0.1943]], requires_grad=True) init_w\n",
            "tensor([[-0.7626],\n",
            "        [ 1.2837],\n",
            "        [ 0.1943]], requires_grad=True) forward_w\n",
            "tensor([[-0.7626],\n",
            "        [ 1.2837],\n",
            "        [ 0.1943]], requires_grad=True) loss_w\n",
            "tensor(0.0009, grad_fn=<MeanBackward0>) Loss 245\n",
            "tensor([[-0.7629],\n",
            "        [ 1.2838],\n",
            "        [ 0.1948]], requires_grad=True) init_w\n",
            "tensor([[-0.7629],\n",
            "        [ 1.2838],\n",
            "        [ 0.1948]], requires_grad=True) forward_w\n",
            "tensor([[-0.7629],\n",
            "        [ 1.2838],\n",
            "        [ 0.1948]], requires_grad=True) loss_w\n",
            "tensor(0.0009, grad_fn=<MeanBackward0>) Loss 246\n",
            "tensor([[-0.7631],\n",
            "        [ 1.2840],\n",
            "        [ 0.1953]], requires_grad=True) init_w\n",
            "tensor([[-0.7631],\n",
            "        [ 1.2840],\n",
            "        [ 0.1953]], requires_grad=True) forward_w\n",
            "tensor([[-0.7631],\n",
            "        [ 1.2840],\n",
            "        [ 0.1953]], requires_grad=True) loss_w\n",
            "tensor(0.0009, grad_fn=<MeanBackward0>) Loss 247\n",
            "tensor([[-0.7633],\n",
            "        [ 1.2841],\n",
            "        [ 0.1957]], requires_grad=True) init_w\n",
            "tensor([[-0.7633],\n",
            "        [ 1.2841],\n",
            "        [ 0.1957]], requires_grad=True) forward_w\n",
            "tensor([[-0.7633],\n",
            "        [ 1.2841],\n",
            "        [ 0.1957]], requires_grad=True) loss_w\n",
            "tensor(0.0009, grad_fn=<MeanBackward0>) Loss 248\n",
            "tensor([[-0.7635],\n",
            "        [ 1.2843],\n",
            "        [ 0.1962]], requires_grad=True) init_w\n",
            "tensor([[-0.7635],\n",
            "        [ 1.2843],\n",
            "        [ 0.1962]], requires_grad=True) forward_w\n",
            "tensor([[-0.7635],\n",
            "        [ 1.2843],\n",
            "        [ 0.1962]], requires_grad=True) loss_w\n",
            "tensor(0.0008, grad_fn=<MeanBackward0>) Loss 249\n",
            "tensor([[-0.7637],\n",
            "        [ 1.2844],\n",
            "        [ 0.1967]], requires_grad=True) init_w\n",
            "tensor([[-0.7637],\n",
            "        [ 1.2844],\n",
            "        [ 0.1967]], requires_grad=True) forward_w\n",
            "tensor([[-0.7637],\n",
            "        [ 1.2844],\n",
            "        [ 0.1967]], requires_grad=True) loss_w\n",
            "tensor(0.0008, grad_fn=<MeanBackward0>) Loss 250\n",
            "tensor([[-0.7639],\n",
            "        [ 1.2846],\n",
            "        [ 0.1971]], requires_grad=True) init_w\n",
            "tensor([[-0.7639],\n",
            "        [ 1.2846],\n",
            "        [ 0.1971]], requires_grad=True) forward_w\n",
            "tensor([[-0.7639],\n",
            "        [ 1.2846],\n",
            "        [ 0.1971]], requires_grad=True) loss_w\n",
            "tensor(0.0008, grad_fn=<MeanBackward0>) Loss 251\n",
            "tensor([[-0.7641],\n",
            "        [ 1.2847],\n",
            "        [ 0.1975]], requires_grad=True) init_w\n",
            "tensor([[-0.7641],\n",
            "        [ 1.2847],\n",
            "        [ 0.1975]], requires_grad=True) forward_w\n",
            "tensor([[-0.7641],\n",
            "        [ 1.2847],\n",
            "        [ 0.1975]], requires_grad=True) loss_w\n",
            "tensor(0.0007, grad_fn=<MeanBackward0>) Loss 252\n",
            "tensor([[-0.7643],\n",
            "        [ 1.2849],\n",
            "        [ 0.1980]], requires_grad=True) init_w\n",
            "tensor([[-0.7643],\n",
            "        [ 1.2849],\n",
            "        [ 0.1980]], requires_grad=True) forward_w\n",
            "tensor([[-0.7643],\n",
            "        [ 1.2849],\n",
            "        [ 0.1980]], requires_grad=True) loss_w\n",
            "tensor(0.0007, grad_fn=<MeanBackward0>) Loss 253\n",
            "tensor([[-0.7645],\n",
            "        [ 1.2850],\n",
            "        [ 0.1984]], requires_grad=True) init_w\n",
            "tensor([[-0.7645],\n",
            "        [ 1.2850],\n",
            "        [ 0.1984]], requires_grad=True) forward_w\n",
            "tensor([[-0.7645],\n",
            "        [ 1.2850],\n",
            "        [ 0.1984]], requires_grad=True) loss_w\n",
            "tensor(0.0007, grad_fn=<MeanBackward0>) Loss 254\n",
            "tensor([[-0.7647],\n",
            "        [ 1.2851],\n",
            "        [ 0.1988]], requires_grad=True) init_w\n",
            "tensor([[-0.7647],\n",
            "        [ 1.2851],\n",
            "        [ 0.1988]], requires_grad=True) forward_w\n",
            "tensor([[-0.7647],\n",
            "        [ 1.2851],\n",
            "        [ 0.1988]], requires_grad=True) loss_w\n",
            "tensor(0.0007, grad_fn=<MeanBackward0>) Loss 255\n",
            "tensor([[-0.7649],\n",
            "        [ 1.2853],\n",
            "        [ 0.1992]], requires_grad=True) init_w\n",
            "tensor([[-0.7649],\n",
            "        [ 1.2853],\n",
            "        [ 0.1992]], requires_grad=True) forward_w\n",
            "tensor([[-0.7649],\n",
            "        [ 1.2853],\n",
            "        [ 0.1992]], requires_grad=True) loss_w\n",
            "tensor(0.0007, grad_fn=<MeanBackward0>) Loss 256\n",
            "tensor([[-0.7650],\n",
            "        [ 1.2854],\n",
            "        [ 0.1996]], requires_grad=True) init_w\n",
            "tensor([[-0.7650],\n",
            "        [ 1.2854],\n",
            "        [ 0.1996]], requires_grad=True) forward_w\n",
            "tensor([[-0.7650],\n",
            "        [ 1.2854],\n",
            "        [ 0.1996]], requires_grad=True) loss_w\n",
            "tensor(0.0006, grad_fn=<MeanBackward0>) Loss 257\n",
            "tensor([[-0.7652],\n",
            "        [ 1.2855],\n",
            "        [ 0.2000]], requires_grad=True) init_w\n",
            "tensor([[-0.7652],\n",
            "        [ 1.2855],\n",
            "        [ 0.2000]], requires_grad=True) forward_w\n",
            "tensor([[-0.7652],\n",
            "        [ 1.2855],\n",
            "        [ 0.2000]], requires_grad=True) loss_w\n",
            "tensor(0.0006, grad_fn=<MeanBackward0>) Loss 258\n",
            "tensor([[-0.7654],\n",
            "        [ 1.2857],\n",
            "        [ 0.2004]], requires_grad=True) init_w\n",
            "tensor([[-0.7654],\n",
            "        [ 1.2857],\n",
            "        [ 0.2004]], requires_grad=True) forward_w\n",
            "tensor([[-0.7654],\n",
            "        [ 1.2857],\n",
            "        [ 0.2004]], requires_grad=True) loss_w\n",
            "tensor(0.0006, grad_fn=<MeanBackward0>) Loss 259\n",
            "tensor([[-0.7656],\n",
            "        [ 1.2858],\n",
            "        [ 0.2008]], requires_grad=True) init_w\n",
            "tensor([[-0.7656],\n",
            "        [ 1.2858],\n",
            "        [ 0.2008]], requires_grad=True) forward_w\n",
            "tensor([[-0.7656],\n",
            "        [ 1.2858],\n",
            "        [ 0.2008]], requires_grad=True) loss_w\n",
            "tensor(0.0006, grad_fn=<MeanBackward0>) Loss 260\n",
            "tensor([[-0.7657],\n",
            "        [ 1.2859],\n",
            "        [ 0.2012]], requires_grad=True) init_w\n",
            "tensor([[-0.7657],\n",
            "        [ 1.2859],\n",
            "        [ 0.2012]], requires_grad=True) forward_w\n",
            "tensor([[-0.7657],\n",
            "        [ 1.2859],\n",
            "        [ 0.2012]], requires_grad=True) loss_w\n",
            "tensor(0.0006, grad_fn=<MeanBackward0>) Loss 261\n",
            "tensor([[-0.7659],\n",
            "        [ 1.2860],\n",
            "        [ 0.2015]], requires_grad=True) init_w\n",
            "tensor([[-0.7659],\n",
            "        [ 1.2860],\n",
            "        [ 0.2015]], requires_grad=True) forward_w\n",
            "tensor([[-0.7659],\n",
            "        [ 1.2860],\n",
            "        [ 0.2015]], requires_grad=True) loss_w\n",
            "tensor(0.0005, grad_fn=<MeanBackward0>) Loss 262\n",
            "tensor([[-0.7661],\n",
            "        [ 1.2862],\n",
            "        [ 0.2019]], requires_grad=True) init_w\n",
            "tensor([[-0.7661],\n",
            "        [ 1.2862],\n",
            "        [ 0.2019]], requires_grad=True) forward_w\n",
            "tensor([[-0.7661],\n",
            "        [ 1.2862],\n",
            "        [ 0.2019]], requires_grad=True) loss_w\n",
            "tensor(0.0005, grad_fn=<MeanBackward0>) Loss 263\n",
            "tensor([[-0.7662],\n",
            "        [ 1.2863],\n",
            "        [ 0.2023]], requires_grad=True) init_w\n",
            "tensor([[-0.7662],\n",
            "        [ 1.2863],\n",
            "        [ 0.2023]], requires_grad=True) forward_w\n",
            "tensor([[-0.7662],\n",
            "        [ 1.2863],\n",
            "        [ 0.2023]], requires_grad=True) loss_w\n",
            "tensor(0.0005, grad_fn=<MeanBackward0>) Loss 264\n",
            "tensor([[-0.7664],\n",
            "        [ 1.2864],\n",
            "        [ 0.2026]], requires_grad=True) init_w\n",
            "tensor([[-0.7664],\n",
            "        [ 1.2864],\n",
            "        [ 0.2026]], requires_grad=True) forward_w\n",
            "tensor([[-0.7664],\n",
            "        [ 1.2864],\n",
            "        [ 0.2026]], requires_grad=True) loss_w\n",
            "tensor(0.0005, grad_fn=<MeanBackward0>) Loss 265\n",
            "tensor([[-0.7666],\n",
            "        [ 1.2865],\n",
            "        [ 0.2030]], requires_grad=True) init_w\n",
            "tensor([[-0.7666],\n",
            "        [ 1.2865],\n",
            "        [ 0.2030]], requires_grad=True) forward_w\n",
            "tensor([[-0.7666],\n",
            "        [ 1.2865],\n",
            "        [ 0.2030]], requires_grad=True) loss_w\n",
            "tensor(0.0005, grad_fn=<MeanBackward0>) Loss 266\n",
            "tensor([[-0.7667],\n",
            "        [ 1.2866],\n",
            "        [ 0.2033]], requires_grad=True) init_w\n",
            "tensor([[-0.7667],\n",
            "        [ 1.2866],\n",
            "        [ 0.2033]], requires_grad=True) forward_w\n",
            "tensor([[-0.7667],\n",
            "        [ 1.2866],\n",
            "        [ 0.2033]], requires_grad=True) loss_w\n",
            "tensor(0.0005, grad_fn=<MeanBackward0>) Loss 267\n",
            "tensor([[-0.7669],\n",
            "        [ 1.2867],\n",
            "        [ 0.2036]], requires_grad=True) init_w\n",
            "tensor([[-0.7669],\n",
            "        [ 1.2867],\n",
            "        [ 0.2036]], requires_grad=True) forward_w\n",
            "tensor([[-0.7669],\n",
            "        [ 1.2867],\n",
            "        [ 0.2036]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 268\n",
            "tensor([[-0.7670],\n",
            "        [ 1.2868],\n",
            "        [ 0.2040]], requires_grad=True) init_w\n",
            "tensor([[-0.7670],\n",
            "        [ 1.2868],\n",
            "        [ 0.2040]], requires_grad=True) forward_w\n",
            "tensor([[-0.7670],\n",
            "        [ 1.2868],\n",
            "        [ 0.2040]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 269\n",
            "tensor([[-0.7672],\n",
            "        [ 1.2869],\n",
            "        [ 0.2043]], requires_grad=True) init_w\n",
            "tensor([[-0.7672],\n",
            "        [ 1.2869],\n",
            "        [ 0.2043]], requires_grad=True) forward_w\n",
            "tensor([[-0.7672],\n",
            "        [ 1.2869],\n",
            "        [ 0.2043]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 270\n",
            "tensor([[-0.7673],\n",
            "        [ 1.2870],\n",
            "        [ 0.2046]], requires_grad=True) init_w\n",
            "tensor([[-0.7673],\n",
            "        [ 1.2870],\n",
            "        [ 0.2046]], requires_grad=True) forward_w\n",
            "tensor([[-0.7673],\n",
            "        [ 1.2870],\n",
            "        [ 0.2046]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 271\n",
            "tensor([[-0.7674],\n",
            "        [ 1.2872],\n",
            "        [ 0.2049]], requires_grad=True) init_w\n",
            "tensor([[-0.7674],\n",
            "        [ 1.2872],\n",
            "        [ 0.2049]], requires_grad=True) forward_w\n",
            "tensor([[-0.7674],\n",
            "        [ 1.2872],\n",
            "        [ 0.2049]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 272\n",
            "tensor([[-0.7676],\n",
            "        [ 1.2873],\n",
            "        [ 0.2053]], requires_grad=True) init_w\n",
            "tensor([[-0.7676],\n",
            "        [ 1.2873],\n",
            "        [ 0.2053]], requires_grad=True) forward_w\n",
            "tensor([[-0.7676],\n",
            "        [ 1.2873],\n",
            "        [ 0.2053]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 273\n",
            "tensor([[-0.7677],\n",
            "        [ 1.2874],\n",
            "        [ 0.2056]], requires_grad=True) init_w\n",
            "tensor([[-0.7677],\n",
            "        [ 1.2874],\n",
            "        [ 0.2056]], requires_grad=True) forward_w\n",
            "tensor([[-0.7677],\n",
            "        [ 1.2874],\n",
            "        [ 0.2056]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 274\n",
            "tensor([[-0.7679],\n",
            "        [ 1.2875],\n",
            "        [ 0.2059]], requires_grad=True) init_w\n",
            "tensor([[-0.7679],\n",
            "        [ 1.2875],\n",
            "        [ 0.2059]], requires_grad=True) forward_w\n",
            "tensor([[-0.7679],\n",
            "        [ 1.2875],\n",
            "        [ 0.2059]], requires_grad=True) loss_w\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>) Loss 275\n",
            "tensor([[-0.7680],\n",
            "        [ 1.2875],\n",
            "        [ 0.2062]], requires_grad=True) init_w\n",
            "tensor([[-0.7680],\n",
            "        [ 1.2875],\n",
            "        [ 0.2062]], requires_grad=True) forward_w\n",
            "tensor([[-0.7680],\n",
            "        [ 1.2875],\n",
            "        [ 0.2062]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 276\n",
            "tensor([[-0.7681],\n",
            "        [ 1.2876],\n",
            "        [ 0.2064]], requires_grad=True) init_w\n",
            "tensor([[-0.7681],\n",
            "        [ 1.2876],\n",
            "        [ 0.2064]], requires_grad=True) forward_w\n",
            "tensor([[-0.7681],\n",
            "        [ 1.2876],\n",
            "        [ 0.2064]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 277\n",
            "tensor([[-0.7683],\n",
            "        [ 1.2877],\n",
            "        [ 0.2067]], requires_grad=True) init_w\n",
            "tensor([[-0.7683],\n",
            "        [ 1.2877],\n",
            "        [ 0.2067]], requires_grad=True) forward_w\n",
            "tensor([[-0.7683],\n",
            "        [ 1.2877],\n",
            "        [ 0.2067]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 278\n",
            "tensor([[-0.7684],\n",
            "        [ 1.2878],\n",
            "        [ 0.2070]], requires_grad=True) init_w\n",
            "tensor([[-0.7684],\n",
            "        [ 1.2878],\n",
            "        [ 0.2070]], requires_grad=True) forward_w\n",
            "tensor([[-0.7684],\n",
            "        [ 1.2878],\n",
            "        [ 0.2070]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 279\n",
            "tensor([[-0.7685],\n",
            "        [ 1.2879],\n",
            "        [ 0.2073]], requires_grad=True) init_w\n",
            "tensor([[-0.7685],\n",
            "        [ 1.2879],\n",
            "        [ 0.2073]], requires_grad=True) forward_w\n",
            "tensor([[-0.7685],\n",
            "        [ 1.2879],\n",
            "        [ 0.2073]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 280\n",
            "tensor([[-0.7686],\n",
            "        [ 1.2880],\n",
            "        [ 0.2076]], requires_grad=True) init_w\n",
            "tensor([[-0.7686],\n",
            "        [ 1.2880],\n",
            "        [ 0.2076]], requires_grad=True) forward_w\n",
            "tensor([[-0.7686],\n",
            "        [ 1.2880],\n",
            "        [ 0.2076]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 281\n",
            "tensor([[-0.7688],\n",
            "        [ 1.2881],\n",
            "        [ 0.2078]], requires_grad=True) init_w\n",
            "tensor([[-0.7688],\n",
            "        [ 1.2881],\n",
            "        [ 0.2078]], requires_grad=True) forward_w\n",
            "tensor([[-0.7688],\n",
            "        [ 1.2881],\n",
            "        [ 0.2078]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 282\n",
            "tensor([[-0.7689],\n",
            "        [ 1.2882],\n",
            "        [ 0.2081]], requires_grad=True) init_w\n",
            "tensor([[-0.7689],\n",
            "        [ 1.2882],\n",
            "        [ 0.2081]], requires_grad=True) forward_w\n",
            "tensor([[-0.7689],\n",
            "        [ 1.2882],\n",
            "        [ 0.2081]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 283\n",
            "tensor([[-0.7690],\n",
            "        [ 1.2883],\n",
            "        [ 0.2083]], requires_grad=True) init_w\n",
            "tensor([[-0.7690],\n",
            "        [ 1.2883],\n",
            "        [ 0.2083]], requires_grad=True) forward_w\n",
            "tensor([[-0.7690],\n",
            "        [ 1.2883],\n",
            "        [ 0.2083]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 284\n",
            "tensor([[-0.7691],\n",
            "        [ 1.2884],\n",
            "        [ 0.2086]], requires_grad=True) init_w\n",
            "tensor([[-0.7691],\n",
            "        [ 1.2884],\n",
            "        [ 0.2086]], requires_grad=True) forward_w\n",
            "tensor([[-0.7691],\n",
            "        [ 1.2884],\n",
            "        [ 0.2086]], requires_grad=True) loss_w\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) Loss 285\n",
            "tensor([[-0.7692],\n",
            "        [ 1.2884],\n",
            "        [ 0.2089]], requires_grad=True) init_w\n",
            "tensor([[-0.7692],\n",
            "        [ 1.2884],\n",
            "        [ 0.2089]], requires_grad=True) forward_w\n",
            "tensor([[-0.7692],\n",
            "        [ 1.2884],\n",
            "        [ 0.2089]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 286\n",
            "tensor([[-0.7693],\n",
            "        [ 1.2885],\n",
            "        [ 0.2091]], requires_grad=True) init_w\n",
            "tensor([[-0.7693],\n",
            "        [ 1.2885],\n",
            "        [ 0.2091]], requires_grad=True) forward_w\n",
            "tensor([[-0.7693],\n",
            "        [ 1.2885],\n",
            "        [ 0.2091]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 287\n",
            "tensor([[-0.7694],\n",
            "        [ 1.2886],\n",
            "        [ 0.2093]], requires_grad=True) init_w\n",
            "tensor([[-0.7694],\n",
            "        [ 1.2886],\n",
            "        [ 0.2093]], requires_grad=True) forward_w\n",
            "tensor([[-0.7694],\n",
            "        [ 1.2886],\n",
            "        [ 0.2093]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 288\n",
            "tensor([[-0.7695],\n",
            "        [ 1.2887],\n",
            "        [ 0.2096]], requires_grad=True) init_w\n",
            "tensor([[-0.7695],\n",
            "        [ 1.2887],\n",
            "        [ 0.2096]], requires_grad=True) forward_w\n",
            "tensor([[-0.7695],\n",
            "        [ 1.2887],\n",
            "        [ 0.2096]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 289\n",
            "tensor([[-0.7696],\n",
            "        [ 1.2888],\n",
            "        [ 0.2098]], requires_grad=True) init_w\n",
            "tensor([[-0.7696],\n",
            "        [ 1.2888],\n",
            "        [ 0.2098]], requires_grad=True) forward_w\n",
            "tensor([[-0.7696],\n",
            "        [ 1.2888],\n",
            "        [ 0.2098]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 290\n",
            "tensor([[-0.7698],\n",
            "        [ 1.2888],\n",
            "        [ 0.2100]], requires_grad=True) init_w\n",
            "tensor([[-0.7698],\n",
            "        [ 1.2888],\n",
            "        [ 0.2100]], requires_grad=True) forward_w\n",
            "tensor([[-0.7698],\n",
            "        [ 1.2888],\n",
            "        [ 0.2100]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 291\n",
            "tensor([[-0.7699],\n",
            "        [ 1.2889],\n",
            "        [ 0.2103]], requires_grad=True) init_w\n",
            "tensor([[-0.7699],\n",
            "        [ 1.2889],\n",
            "        [ 0.2103]], requires_grad=True) forward_w\n",
            "tensor([[-0.7699],\n",
            "        [ 1.2889],\n",
            "        [ 0.2103]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 292\n",
            "tensor([[-0.7700],\n",
            "        [ 1.2890],\n",
            "        [ 0.2105]], requires_grad=True) init_w\n",
            "tensor([[-0.7700],\n",
            "        [ 1.2890],\n",
            "        [ 0.2105]], requires_grad=True) forward_w\n",
            "tensor([[-0.7700],\n",
            "        [ 1.2890],\n",
            "        [ 0.2105]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 293\n",
            "tensor([[-0.7701],\n",
            "        [ 1.2890],\n",
            "        [ 0.2107]], requires_grad=True) init_w\n",
            "tensor([[-0.7701],\n",
            "        [ 1.2890],\n",
            "        [ 0.2107]], requires_grad=True) forward_w\n",
            "tensor([[-0.7701],\n",
            "        [ 1.2890],\n",
            "        [ 0.2107]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 294\n",
            "tensor([[-0.7702],\n",
            "        [ 1.2891],\n",
            "        [ 0.2109]], requires_grad=True) init_w\n",
            "tensor([[-0.7702],\n",
            "        [ 1.2891],\n",
            "        [ 0.2109]], requires_grad=True) forward_w\n",
            "tensor([[-0.7702],\n",
            "        [ 1.2891],\n",
            "        [ 0.2109]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 295\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2892],\n",
            "        [ 0.2111]], requires_grad=True) init_w\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2892],\n",
            "        [ 0.2111]], requires_grad=True) forward_w\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2892],\n",
            "        [ 0.2111]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 296\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2893],\n",
            "        [ 0.2114]], requires_grad=True) init_w\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2893],\n",
            "        [ 0.2114]], requires_grad=True) forward_w\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2893],\n",
            "        [ 0.2114]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 297\n",
            "tensor([[-0.7704],\n",
            "        [ 1.2893],\n",
            "        [ 0.2116]], requires_grad=True) init_w\n",
            "tensor([[-0.7704],\n",
            "        [ 1.2893],\n",
            "        [ 0.2116]], requires_grad=True) forward_w\n",
            "tensor([[-0.7704],\n",
            "        [ 1.2893],\n",
            "        [ 0.2116]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 298\n",
            "tensor([[-0.7705],\n",
            "        [ 1.2894],\n",
            "        [ 0.2118]], requires_grad=True) init_w\n",
            "tensor([[-0.7705],\n",
            "        [ 1.2894],\n",
            "        [ 0.2118]], requires_grad=True) forward_w\n",
            "tensor([[-0.7705],\n",
            "        [ 1.2894],\n",
            "        [ 0.2118]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 299\n",
            "tensor([[-0.7706],\n",
            "        [ 1.2895],\n",
            "        [ 0.2120]], requires_grad=True) init_w\n",
            "tensor([[-0.7706],\n",
            "        [ 1.2895],\n",
            "        [ 0.2120]], requires_grad=True) forward_w\n",
            "tensor([[-0.7706],\n",
            "        [ 1.2895],\n",
            "        [ 0.2120]], requires_grad=True) loss_w\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>) Loss 300\n",
            "tensor([[-0.7707],\n",
            "        [ 1.2895],\n",
            "        [ 0.2122]], requires_grad=True) init_w\n",
            "tensor([[-0.7707],\n",
            "        [ 1.2895],\n",
            "        [ 0.2122]], requires_grad=True) forward_w\n",
            "tensor([[-0.7707],\n",
            "        [ 1.2895],\n",
            "        [ 0.2122]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 301\n",
            "tensor([[-0.7708],\n",
            "        [ 1.2896],\n",
            "        [ 0.2123]], requires_grad=True) init_w\n",
            "tensor([[-0.7708],\n",
            "        [ 1.2896],\n",
            "        [ 0.2123]], requires_grad=True) forward_w\n",
            "tensor([[-0.7708],\n",
            "        [ 1.2896],\n",
            "        [ 0.2123]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 302\n",
            "tensor([[-0.7709],\n",
            "        [ 1.2896],\n",
            "        [ 0.2125]], requires_grad=True) init_w\n",
            "tensor([[-0.7709],\n",
            "        [ 1.2896],\n",
            "        [ 0.2125]], requires_grad=True) forward_w\n",
            "tensor([[-0.7709],\n",
            "        [ 1.2896],\n",
            "        [ 0.2125]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 303\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2897],\n",
            "        [ 0.2127]], requires_grad=True) init_w\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2897],\n",
            "        [ 0.2127]], requires_grad=True) forward_w\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2897],\n",
            "        [ 0.2127]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 304\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2898],\n",
            "        [ 0.2129]], requires_grad=True) init_w\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2898],\n",
            "        [ 0.2129]], requires_grad=True) forward_w\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2898],\n",
            "        [ 0.2129]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 305\n",
            "tensor([[-0.7711],\n",
            "        [ 1.2898],\n",
            "        [ 0.2131]], requires_grad=True) init_w\n",
            "tensor([[-0.7711],\n",
            "        [ 1.2898],\n",
            "        [ 0.2131]], requires_grad=True) forward_w\n",
            "tensor([[-0.7711],\n",
            "        [ 1.2898],\n",
            "        [ 0.2131]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 306\n",
            "tensor([[-0.7712],\n",
            "        [ 1.2899],\n",
            "        [ 0.2133]], requires_grad=True) init_w\n",
            "tensor([[-0.7712],\n",
            "        [ 1.2899],\n",
            "        [ 0.2133]], requires_grad=True) forward_w\n",
            "tensor([[-0.7712],\n",
            "        [ 1.2899],\n",
            "        [ 0.2133]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 307\n",
            "tensor([[-0.7713],\n",
            "        [ 1.2899],\n",
            "        [ 0.2134]], requires_grad=True) init_w\n",
            "tensor([[-0.7713],\n",
            "        [ 1.2899],\n",
            "        [ 0.2134]], requires_grad=True) forward_w\n",
            "tensor([[-0.7713],\n",
            "        [ 1.2899],\n",
            "        [ 0.2134]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 308\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2900],\n",
            "        [ 0.2136]], requires_grad=True) init_w\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2900],\n",
            "        [ 0.2136]], requires_grad=True) forward_w\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2900],\n",
            "        [ 0.2136]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 309\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2901],\n",
            "        [ 0.2138]], requires_grad=True) init_w\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2901],\n",
            "        [ 0.2138]], requires_grad=True) forward_w\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2901],\n",
            "        [ 0.2138]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 310\n",
            "tensor([[-0.7715],\n",
            "        [ 1.2901],\n",
            "        [ 0.2139]], requires_grad=True) init_w\n",
            "tensor([[-0.7715],\n",
            "        [ 1.2901],\n",
            "        [ 0.2139]], requires_grad=True) forward_w\n",
            "tensor([[-0.7715],\n",
            "        [ 1.2901],\n",
            "        [ 0.2139]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 311\n",
            "tensor([[-0.7716],\n",
            "        [ 1.2902],\n",
            "        [ 0.2141]], requires_grad=True) init_w\n",
            "tensor([[-0.7716],\n",
            "        [ 1.2902],\n",
            "        [ 0.2141]], requires_grad=True) forward_w\n",
            "tensor([[-0.7716],\n",
            "        [ 1.2902],\n",
            "        [ 0.2141]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 312\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2902],\n",
            "        [ 0.2143]], requires_grad=True) init_w\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2902],\n",
            "        [ 0.2143]], requires_grad=True) forward_w\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2902],\n",
            "        [ 0.2143]], requires_grad=True) loss_w\n",
            "tensor(0.0001, grad_fn=<MeanBackward0>) Loss 313\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2903],\n",
            "        [ 0.2144]], requires_grad=True) init_w\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2903],\n",
            "        [ 0.2144]], requires_grad=True) forward_w\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2903],\n",
            "        [ 0.2144]], requires_grad=True) loss_w\n",
            "tensor(9.7244e-05, grad_fn=<MeanBackward0>) Loss 314\n",
            "tensor([[-0.7718],\n",
            "        [ 1.2903],\n",
            "        [ 0.2146]], requires_grad=True) init_w\n",
            "tensor([[-0.7718],\n",
            "        [ 1.2903],\n",
            "        [ 0.2146]], requires_grad=True) forward_w\n",
            "tensor([[-0.7718],\n",
            "        [ 1.2903],\n",
            "        [ 0.2146]], requires_grad=True) loss_w\n",
            "tensor(9.4098e-05, grad_fn=<MeanBackward0>) Loss 315\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2147]], requires_grad=True) init_w\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2147]], requires_grad=True) forward_w\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2147]], requires_grad=True) loss_w\n",
            "tensor(9.1054e-05, grad_fn=<MeanBackward0>) Loss 316\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2149]], requires_grad=True) init_w\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2149]], requires_grad=True) forward_w\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2149]], requires_grad=True) loss_w\n",
            "tensor(8.8109e-05, grad_fn=<MeanBackward0>) Loss 317\n",
            "tensor([[-0.7720],\n",
            "        [ 1.2905],\n",
            "        [ 0.2150]], requires_grad=True) init_w\n",
            "tensor([[-0.7720],\n",
            "        [ 1.2905],\n",
            "        [ 0.2150]], requires_grad=True) forward_w\n",
            "tensor([[-0.7720],\n",
            "        [ 1.2905],\n",
            "        [ 0.2150]], requires_grad=True) loss_w\n",
            "tensor(8.5259e-05, grad_fn=<MeanBackward0>) Loss 318\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2905],\n",
            "        [ 0.2152]], requires_grad=True) init_w\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2905],\n",
            "        [ 0.2152]], requires_grad=True) forward_w\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2905],\n",
            "        [ 0.2152]], requires_grad=True) loss_w\n",
            "tensor(8.2500e-05, grad_fn=<MeanBackward0>) Loss 319\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2906],\n",
            "        [ 0.2153]], requires_grad=True) init_w\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2906],\n",
            "        [ 0.2153]], requires_grad=True) forward_w\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2906],\n",
            "        [ 0.2153]], requires_grad=True) loss_w\n",
            "tensor(7.9831e-05, grad_fn=<MeanBackward0>) Loss 320\n",
            "tensor([[-0.7722],\n",
            "        [ 1.2906],\n",
            "        [ 0.2155]], requires_grad=True) init_w\n",
            "tensor([[-0.7722],\n",
            "        [ 1.2906],\n",
            "        [ 0.2155]], requires_grad=True) forward_w\n",
            "tensor([[-0.7722],\n",
            "        [ 1.2906],\n",
            "        [ 0.2155]], requires_grad=True) loss_w\n",
            "tensor(7.7249e-05, grad_fn=<MeanBackward0>) Loss 321\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2156]], requires_grad=True) init_w\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2156]], requires_grad=True) forward_w\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2156]], requires_grad=True) loss_w\n",
            "tensor(7.4750e-05, grad_fn=<MeanBackward0>) Loss 322\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2157]], requires_grad=True) init_w\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2157]], requires_grad=True) forward_w\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2157]], requires_grad=True) loss_w\n",
            "tensor(7.2332e-05, grad_fn=<MeanBackward0>) Loss 323\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2907],\n",
            "        [ 0.2159]], requires_grad=True) init_w\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2907],\n",
            "        [ 0.2159]], requires_grad=True) forward_w\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2907],\n",
            "        [ 0.2159]], requires_grad=True) loss_w\n",
            "tensor(6.9992e-05, grad_fn=<MeanBackward0>) Loss 324\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2908],\n",
            "        [ 0.2160]], requires_grad=True) init_w\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2908],\n",
            "        [ 0.2160]], requires_grad=True) forward_w\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2908],\n",
            "        [ 0.2160]], requires_grad=True) loss_w\n",
            "tensor(6.7728e-05, grad_fn=<MeanBackward0>) Loss 325\n",
            "tensor([[-0.7725],\n",
            "        [ 1.2908],\n",
            "        [ 0.2161]], requires_grad=True) init_w\n",
            "tensor([[-0.7725],\n",
            "        [ 1.2908],\n",
            "        [ 0.2161]], requires_grad=True) forward_w\n",
            "tensor([[-0.7725],\n",
            "        [ 1.2908],\n",
            "        [ 0.2161]], requires_grad=True) loss_w\n",
            "tensor(6.5537e-05, grad_fn=<MeanBackward0>) Loss 326\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2163]], requires_grad=True) init_w\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2163]], requires_grad=True) forward_w\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2163]], requires_grad=True) loss_w\n",
            "tensor(6.3417e-05, grad_fn=<MeanBackward0>) Loss 327\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2164]], requires_grad=True) init_w\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2164]], requires_grad=True) forward_w\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2164]], requires_grad=True) loss_w\n",
            "tensor(6.1366e-05, grad_fn=<MeanBackward0>) Loss 328\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2165]], requires_grad=True) init_w\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2165]], requires_grad=True) forward_w\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2165]], requires_grad=True) loss_w\n",
            "tensor(5.9381e-05, grad_fn=<MeanBackward0>) Loss 329\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2166]], requires_grad=True) init_w\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2166]], requires_grad=True) forward_w\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2166]], requires_grad=True) loss_w\n",
            "tensor(5.7460e-05, grad_fn=<MeanBackward0>) Loss 330\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2910],\n",
            "        [ 0.2167]], requires_grad=True) init_w\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2910],\n",
            "        [ 0.2167]], requires_grad=True) forward_w\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2910],\n",
            "        [ 0.2167]], requires_grad=True) loss_w\n",
            "tensor(5.5601e-05, grad_fn=<MeanBackward0>) Loss 331\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2911],\n",
            "        [ 0.2169]], requires_grad=True) init_w\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2911],\n",
            "        [ 0.2169]], requires_grad=True) forward_w\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2911],\n",
            "        [ 0.2169]], requires_grad=True) loss_w\n",
            "tensor(5.3802e-05, grad_fn=<MeanBackward0>) Loss 332\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2170]], requires_grad=True) init_w\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2170]], requires_grad=True) forward_w\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2170]], requires_grad=True) loss_w\n",
            "tensor(5.2062e-05, grad_fn=<MeanBackward0>) Loss 333\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2171]], requires_grad=True) init_w\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2171]], requires_grad=True) forward_w\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2171]], requires_grad=True) loss_w\n",
            "tensor(5.0378e-05, grad_fn=<MeanBackward0>) Loss 334\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2172]], requires_grad=True) init_w\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2172]], requires_grad=True) forward_w\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2172]], requires_grad=True) loss_w\n",
            "tensor(4.8748e-05, grad_fn=<MeanBackward0>) Loss 335\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2173]], requires_grad=True) init_w\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2173]], requires_grad=True) forward_w\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2173]], requires_grad=True) loss_w\n",
            "tensor(4.7171e-05, grad_fn=<MeanBackward0>) Loss 336\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2174]], requires_grad=True) init_w\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2174]], requires_grad=True) forward_w\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2174]], requires_grad=True) loss_w\n",
            "tensor(4.5645e-05, grad_fn=<MeanBackward0>) Loss 337\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2175]], requires_grad=True) init_w\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2175]], requires_grad=True) forward_w\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2175]], requires_grad=True) loss_w\n",
            "tensor(4.4168e-05, grad_fn=<MeanBackward0>) Loss 338\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2913],\n",
            "        [ 0.2176]], requires_grad=True) init_w\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2913],\n",
            "        [ 0.2176]], requires_grad=True) forward_w\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2913],\n",
            "        [ 0.2176]], requires_grad=True) loss_w\n",
            "tensor(4.2739e-05, grad_fn=<MeanBackward0>) Loss 339\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2914],\n",
            "        [ 0.2177]], requires_grad=True) init_w\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2914],\n",
            "        [ 0.2177]], requires_grad=True) forward_w\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2914],\n",
            "        [ 0.2177]], requires_grad=True) loss_w\n",
            "tensor(4.1357e-05, grad_fn=<MeanBackward0>) Loss 340\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2178]], requires_grad=True) init_w\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2178]], requires_grad=True) forward_w\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2178]], requires_grad=True) loss_w\n",
            "tensor(4.0019e-05, grad_fn=<MeanBackward0>) Loss 341\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2179]], requires_grad=True) init_w\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2179]], requires_grad=True) forward_w\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2179]], requires_grad=True) loss_w\n",
            "tensor(3.8725e-05, grad_fn=<MeanBackward0>) Loss 342\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2180]], requires_grad=True) init_w\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2180]], requires_grad=True) forward_w\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2180]], requires_grad=True) loss_w\n",
            "tensor(3.7472e-05, grad_fn=<MeanBackward0>) Loss 343\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2181]], requires_grad=True) init_w\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2181]], requires_grad=True) forward_w\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2181]], requires_grad=True) loss_w\n",
            "tensor(3.6260e-05, grad_fn=<MeanBackward0>) Loss 344\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2915],\n",
            "        [ 0.2182]], requires_grad=True) init_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2915],\n",
            "        [ 0.2182]], requires_grad=True) forward_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2915],\n",
            "        [ 0.2182]], requires_grad=True) loss_w\n",
            "tensor(3.5086e-05, grad_fn=<MeanBackward0>) Loss 345\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2183]], requires_grad=True) init_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2183]], requires_grad=True) forward_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2183]], requires_grad=True) loss_w\n",
            "tensor(3.3951e-05, grad_fn=<MeanBackward0>) Loss 346\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2184]], requires_grad=True) init_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2184]], requires_grad=True) forward_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2184]], requires_grad=True) loss_w\n",
            "tensor(3.2853e-05, grad_fn=<MeanBackward0>) Loss 347\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2185]], requires_grad=True) init_w\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2185]], requires_grad=True) forward_w\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2185]], requires_grad=True) loss_w\n",
            "tensor(3.1791e-05, grad_fn=<MeanBackward0>) Loss 348\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2186]], requires_grad=True) init_w\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2186]], requires_grad=True) forward_w\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2186]], requires_grad=True) loss_w\n",
            "tensor(3.0762e-05, grad_fn=<MeanBackward0>) Loss 349\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2187]], requires_grad=True) init_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2187]], requires_grad=True) forward_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2187]], requires_grad=True) loss_w\n",
            "tensor(2.9767e-05, grad_fn=<MeanBackward0>) Loss 350\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2188]], requires_grad=True) init_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2188]], requires_grad=True) forward_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2188]], requires_grad=True) loss_w\n",
            "tensor(2.8804e-05, grad_fn=<MeanBackward0>) Loss 351\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2189]], requires_grad=True) init_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2189]], requires_grad=True) forward_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2189]], requires_grad=True) loss_w\n",
            "tensor(2.7873e-05, grad_fn=<MeanBackward0>) Loss 352\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2189]], requires_grad=True) init_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2189]], requires_grad=True) forward_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2189]], requires_grad=True) loss_w\n",
            "tensor(2.6971e-05, grad_fn=<MeanBackward0>) Loss 353\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2190]], requires_grad=True) init_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2190]], requires_grad=True) forward_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2190]], requires_grad=True) loss_w\n",
            "tensor(2.6098e-05, grad_fn=<MeanBackward0>) Loss 354\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2191]], requires_grad=True) init_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2191]], requires_grad=True) forward_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2191]], requires_grad=True) loss_w\n",
            "tensor(2.5254e-05, grad_fn=<MeanBackward0>) Loss 355\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2918],\n",
            "        [ 0.2192]], requires_grad=True) init_w\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2918],\n",
            "        [ 0.2192]], requires_grad=True) forward_w\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2918],\n",
            "        [ 0.2192]], requires_grad=True) loss_w\n",
            "tensor(2.4437e-05, grad_fn=<MeanBackward0>) Loss 356\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) init_w\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) forward_w\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) loss_w\n",
            "tensor(2.3646e-05, grad_fn=<MeanBackward0>) Loss 357\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) init_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) forward_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) loss_w\n",
            "tensor(2.2882e-05, grad_fn=<MeanBackward0>) Loss 358\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2194]], requires_grad=True) init_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2194]], requires_grad=True) forward_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2194]], requires_grad=True) loss_w\n",
            "tensor(2.2142e-05, grad_fn=<MeanBackward0>) Loss 359\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2195]], requires_grad=True) init_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2195]], requires_grad=True) forward_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2195]], requires_grad=True) loss_w\n",
            "tensor(2.1426e-05, grad_fn=<MeanBackward0>) Loss 360\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) forward_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) loss_w\n",
            "tensor(2.0732e-05, grad_fn=<MeanBackward0>) Loss 361\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) forward_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) loss_w\n",
            "tensor(2.0062e-05, grad_fn=<MeanBackward0>) Loss 362\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2197]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2197]], requires_grad=True) forward_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2197]], requires_grad=True) loss_w\n",
            "tensor(1.9413e-05, grad_fn=<MeanBackward0>) Loss 363\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) forward_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) loss_w\n",
            "tensor(1.8785e-05, grad_fn=<MeanBackward0>) Loss 364\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) init_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) forward_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) loss_w\n",
            "tensor(1.8177e-05, grad_fn=<MeanBackward0>) Loss 365\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2199]], requires_grad=True) init_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2199]], requires_grad=True) forward_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2199]], requires_grad=True) loss_w\n",
            "tensor(1.7589e-05, grad_fn=<MeanBackward0>) Loss 366\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) init_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) forward_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) loss_w\n",
            "tensor(1.7021e-05, grad_fn=<MeanBackward0>) Loss 367\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) init_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) forward_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) loss_w\n",
            "tensor(1.6470e-05, grad_fn=<MeanBackward0>) Loss 368\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2201]], requires_grad=True) init_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2201]], requires_grad=True) forward_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2201]], requires_grad=True) loss_w\n",
            "tensor(1.5937e-05, grad_fn=<MeanBackward0>) Loss 369\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) init_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) forward_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) loss_w\n",
            "tensor(1.5421e-05, grad_fn=<MeanBackward0>) Loss 370\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) forward_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) loss_w\n",
            "tensor(1.4923e-05, grad_fn=<MeanBackward0>) Loss 371\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) forward_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) loss_w\n",
            "tensor(1.4440e-05, grad_fn=<MeanBackward0>) Loss 372\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) forward_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) loss_w\n",
            "tensor(1.3973e-05, grad_fn=<MeanBackward0>) Loss 373\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2204]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2204]], requires_grad=True) forward_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2204]], requires_grad=True) loss_w\n",
            "tensor(1.3521e-05, grad_fn=<MeanBackward0>) Loss 374\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) forward_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) loss_w\n",
            "tensor(1.3084e-05, grad_fn=<MeanBackward0>) Loss 375\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) forward_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) loss_w\n",
            "tensor(1.2660e-05, grad_fn=<MeanBackward0>) Loss 376\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) forward_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) loss_w\n",
            "tensor(1.2251e-05, grad_fn=<MeanBackward0>) Loss 377\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) forward_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) loss_w\n",
            "tensor(1.1855e-05, grad_fn=<MeanBackward0>) Loss 378\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) forward_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) loss_w\n",
            "tensor(1.1471e-05, grad_fn=<MeanBackward0>) Loss 379\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) forward_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) loss_w\n",
            "tensor(1.1100e-05, grad_fn=<MeanBackward0>) Loss 380\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) forward_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) loss_w\n",
            "tensor(1.0741e-05, grad_fn=<MeanBackward0>) Loss 381\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) forward_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) loss_w\n",
            "tensor(1.0394e-05, grad_fn=<MeanBackward0>) Loss 382\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) forward_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) loss_w\n",
            "tensor(1.0058e-05, grad_fn=<MeanBackward0>) Loss 383\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) forward_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) loss_w\n",
            "tensor(9.7322e-06, grad_fn=<MeanBackward0>) Loss 384\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) forward_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) loss_w\n",
            "tensor(9.4173e-06, grad_fn=<MeanBackward0>) Loss 385\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) forward_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) loss_w\n",
            "tensor(9.1130e-06, grad_fn=<MeanBackward0>) Loss 386\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) forward_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) loss_w\n",
            "tensor(8.8180e-06, grad_fn=<MeanBackward0>) Loss 387\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) forward_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) loss_w\n",
            "tensor(8.5329e-06, grad_fn=<MeanBackward0>) Loss 388\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) forward_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) loss_w\n",
            "tensor(8.2567e-06, grad_fn=<MeanBackward0>) Loss 389\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) forward_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) loss_w\n",
            "tensor(7.9897e-06, grad_fn=<MeanBackward0>) Loss 390\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) forward_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) loss_w\n",
            "tensor(7.7314e-06, grad_fn=<MeanBackward0>) Loss 391\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) forward_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) loss_w\n",
            "tensor(7.4812e-06, grad_fn=<MeanBackward0>) Loss 392\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2925],\n",
            "        [ 0.2214]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2925],\n",
            "        [ 0.2214]], requires_grad=True) forward_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2925],\n",
            "        [ 0.2214]], requires_grad=True) loss_w\n",
            "tensor(7.2392e-06, grad_fn=<MeanBackward0>) Loss 393\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) forward_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) loss_w\n",
            "tensor(7.0049e-06, grad_fn=<MeanBackward0>) Loss 394\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) forward_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) loss_w\n",
            "tensor(6.7784e-06, grad_fn=<MeanBackward0>) Loss 395\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) forward_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) loss_w\n",
            "tensor(6.5591e-06, grad_fn=<MeanBackward0>) Loss 396\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) forward_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) loss_w\n",
            "tensor(6.3470e-06, grad_fn=<MeanBackward0>) Loss 397\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) forward_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) loss_w\n",
            "tensor(6.1418e-06, grad_fn=<MeanBackward0>) Loss 398\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) forward_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) loss_w\n",
            "tensor(5.9430e-06, grad_fn=<MeanBackward0>) Loss 399\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) forward_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) loss_w\n",
            "tensor(5.7509e-06, grad_fn=<MeanBackward0>) Loss 400\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) forward_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) loss_w\n",
            "tensor(5.5649e-06, grad_fn=<MeanBackward0>) Loss 401\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) forward_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) loss_w\n",
            "tensor(5.3848e-06, grad_fn=<MeanBackward0>) Loss 402\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) forward_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) loss_w\n",
            "tensor(5.2106e-06, grad_fn=<MeanBackward0>) Loss 403\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) forward_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) loss_w\n",
            "tensor(5.0421e-06, grad_fn=<MeanBackward0>) Loss 404\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) forward_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) loss_w\n",
            "tensor(4.8788e-06, grad_fn=<MeanBackward0>) Loss 405\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) forward_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) loss_w\n",
            "tensor(4.7210e-06, grad_fn=<MeanBackward0>) Loss 406\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) forward_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) loss_w\n",
            "tensor(4.5683e-06, grad_fn=<MeanBackward0>) Loss 407\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) forward_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) loss_w\n",
            "tensor(4.4206e-06, grad_fn=<MeanBackward0>) Loss 408\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2220]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2220]], requires_grad=True) forward_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2220]], requires_grad=True) loss_w\n",
            "tensor(4.2777e-06, grad_fn=<MeanBackward0>) Loss 409\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) forward_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) loss_w\n",
            "tensor(4.1394e-06, grad_fn=<MeanBackward0>) Loss 410\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) forward_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) loss_w\n",
            "tensor(4.0055e-06, grad_fn=<MeanBackward0>) Loss 411\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) loss_w\n",
            "tensor(3.8760e-06, grad_fn=<MeanBackward0>) Loss 412\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) loss_w\n",
            "tensor(3.7506e-06, grad_fn=<MeanBackward0>) Loss 413\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) loss_w\n",
            "tensor(3.6293e-06, grad_fn=<MeanBackward0>) Loss 414\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) loss_w\n",
            "tensor(3.5118e-06, grad_fn=<MeanBackward0>) Loss 415\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) forward_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) loss_w\n",
            "tensor(3.3981e-06, grad_fn=<MeanBackward0>) Loss 416\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) loss_w\n",
            "tensor(3.2882e-06, grad_fn=<MeanBackward0>) Loss 417\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) loss_w\n",
            "tensor(3.1819e-06, grad_fn=<MeanBackward0>) Loss 418\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2223]], requires_grad=True) loss_w\n",
            "tensor(3.0788e-06, grad_fn=<MeanBackward0>) Loss 419\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) loss_w\n",
            "tensor(2.9792e-06, grad_fn=<MeanBackward0>) Loss 420\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) loss_w\n",
            "tensor(2.8829e-06, grad_fn=<MeanBackward0>) Loss 421\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) loss_w\n",
            "tensor(2.7897e-06, grad_fn=<MeanBackward0>) Loss 422\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) loss_w\n",
            "tensor(2.6994e-06, grad_fn=<MeanBackward0>) Loss 423\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) loss_w\n",
            "tensor(2.6122e-06, grad_fn=<MeanBackward0>) Loss 424\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) loss_w\n",
            "tensor(2.5276e-06, grad_fn=<MeanBackward0>) Loss 425\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) loss_w\n",
            "tensor(2.4458e-06, grad_fn=<MeanBackward0>) Loss 426\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) loss_w\n",
            "tensor(2.3667e-06, grad_fn=<MeanBackward0>) Loss 427\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) loss_w\n",
            "tensor(2.2902e-06, grad_fn=<MeanBackward0>) Loss 428\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) loss_w\n",
            "tensor(2.2161e-06, grad_fn=<MeanBackward0>) Loss 429\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) loss_w\n",
            "tensor(2.1444e-06, grad_fn=<MeanBackward0>) Loss 430\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2226]], requires_grad=True) loss_w\n",
            "tensor(2.0750e-06, grad_fn=<MeanBackward0>) Loss 431\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) loss_w\n",
            "tensor(2.0079e-06, grad_fn=<MeanBackward0>) Loss 432\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) loss_w\n",
            "tensor(1.9428e-06, grad_fn=<MeanBackward0>) Loss 433\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) loss_w\n",
            "tensor(1.8798e-06, grad_fn=<MeanBackward0>) Loss 434\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) loss_w\n",
            "tensor(1.8191e-06, grad_fn=<MeanBackward0>) Loss 435\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) loss_w\n",
            "tensor(1.7601e-06, grad_fn=<MeanBackward0>) Loss 436\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) loss_w\n",
            "tensor(1.7033e-06, grad_fn=<MeanBackward0>) Loss 437\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) loss_w\n",
            "tensor(1.6481e-06, grad_fn=<MeanBackward0>) Loss 438\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) loss_w\n",
            "tensor(1.5948e-06, grad_fn=<MeanBackward0>) Loss 439\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) loss_w\n",
            "tensor(1.5432e-06, grad_fn=<MeanBackward0>) Loss 440\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) loss_w\n",
            "tensor(1.4932e-06, grad_fn=<MeanBackward0>) Loss 441\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) loss_w\n",
            "tensor(1.4449e-06, grad_fn=<MeanBackward0>) Loss 442\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) loss_w\n",
            "tensor(1.3982e-06, grad_fn=<MeanBackward0>) Loss 443\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) loss_w\n",
            "tensor(1.3530e-06, grad_fn=<MeanBackward0>) Loss 444\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) loss_w\n",
            "tensor(1.3092e-06, grad_fn=<MeanBackward0>) Loss 445\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) loss_w\n",
            "tensor(1.2669e-06, grad_fn=<MeanBackward0>) Loss 446\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) loss_w\n",
            "tensor(1.2259e-06, grad_fn=<MeanBackward0>) Loss 447\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) loss_w\n",
            "tensor(1.1862e-06, grad_fn=<MeanBackward0>) Loss 448\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) loss_w\n",
            "tensor(1.1479e-06, grad_fn=<MeanBackward0>) Loss 449\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) loss_w\n",
            "tensor(1.1108e-06, grad_fn=<MeanBackward0>) Loss 450\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) loss_w\n",
            "tensor(1.0748e-06, grad_fn=<MeanBackward0>) Loss 451\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) loss_w\n",
            "tensor(1.0401e-06, grad_fn=<MeanBackward0>) Loss 452\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) loss_w\n",
            "tensor(1.0064e-06, grad_fn=<MeanBackward0>) Loss 453\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) loss_w\n",
            "tensor(9.7386e-07, grad_fn=<MeanBackward0>) Loss 454\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) loss_w\n",
            "tensor(9.4233e-07, grad_fn=<MeanBackward0>) Loss 455\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) loss_w\n",
            "tensor(9.1184e-07, grad_fn=<MeanBackward0>) Loss 456\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) loss_w\n",
            "tensor(8.8228e-07, grad_fn=<MeanBackward0>) Loss 457\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) loss_w\n",
            "tensor(8.5380e-07, grad_fn=<MeanBackward0>) Loss 458\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) loss_w\n",
            "tensor(8.2620e-07, grad_fn=<MeanBackward0>) Loss 459\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) loss_w\n",
            "tensor(7.9939e-07, grad_fn=<MeanBackward0>) Loss 460\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) loss_w\n",
            "tensor(7.7354e-07, grad_fn=<MeanBackward0>) Loss 461\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) loss_w\n",
            "tensor(7.4850e-07, grad_fn=<MeanBackward0>) Loss 462\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) loss_w\n",
            "tensor(7.2431e-07, grad_fn=<MeanBackward0>) Loss 463\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) loss_w\n",
            "tensor(7.0082e-07, grad_fn=<MeanBackward0>) Loss 464\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(6.7821e-07, grad_fn=<MeanBackward0>) Loss 465\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(6.5626e-07, grad_fn=<MeanBackward0>) Loss 466\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(6.3504e-07, grad_fn=<MeanBackward0>) Loss 467\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(6.1445e-07, grad_fn=<MeanBackward0>) Loss 468\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(5.9463e-07, grad_fn=<MeanBackward0>) Loss 469\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(5.7534e-07, grad_fn=<MeanBackward0>) Loss 470\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(5.5677e-07, grad_fn=<MeanBackward0>) Loss 471\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) loss_w\n",
            "tensor(5.3875e-07, grad_fn=<MeanBackward0>) Loss 472\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(5.2132e-07, grad_fn=<MeanBackward0>) Loss 473\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(5.0446e-07, grad_fn=<MeanBackward0>) Loss 474\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(4.8820e-07, grad_fn=<MeanBackward0>) Loss 475\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(4.7240e-07, grad_fn=<MeanBackward0>) Loss 476\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(4.5711e-07, grad_fn=<MeanBackward0>) Loss 477\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(4.4231e-07, grad_fn=<MeanBackward0>) Loss 478\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(4.2802e-07, grad_fn=<MeanBackward0>) Loss 479\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(4.1419e-07, grad_fn=<MeanBackward0>) Loss 480\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) loss_w\n",
            "tensor(4.0081e-07, grad_fn=<MeanBackward0>) Loss 481\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.8786e-07, grad_fn=<MeanBackward0>) Loss 482\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.7528e-07, grad_fn=<MeanBackward0>) Loss 483\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.6315e-07, grad_fn=<MeanBackward0>) Loss 484\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.5142e-07, grad_fn=<MeanBackward0>) Loss 485\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.4004e-07, grad_fn=<MeanBackward0>) Loss 486\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.2904e-07, grad_fn=<MeanBackward0>) Loss 487\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.1838e-07, grad_fn=<MeanBackward0>) Loss 488\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(3.0805e-07, grad_fn=<MeanBackward0>) Loss 489\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(2.9814e-07, grad_fn=<MeanBackward0>) Loss 490\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(2.8846e-07, grad_fn=<MeanBackward0>) Loss 491\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) loss_w\n",
            "tensor(2.7914e-07, grad_fn=<MeanBackward0>) Loss 492\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) loss_w\n",
            "tensor(2.7014e-07, grad_fn=<MeanBackward0>) Loss 493\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) loss_w\n",
            "tensor(2.6137e-07, grad_fn=<MeanBackward0>) Loss 494\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) loss_w\n",
            "tensor(2.5294e-07, grad_fn=<MeanBackward0>) Loss 495\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) loss_w\n",
            "tensor(2.4476e-07, grad_fn=<MeanBackward0>) Loss 496\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) loss_w\n",
            "tensor(2.3684e-07, grad_fn=<MeanBackward0>) Loss 497\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) loss_w\n",
            "tensor(2.2918e-07, grad_fn=<MeanBackward0>) Loss 498\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) loss_w\n",
            "tensor(2.2181e-07, grad_fn=<MeanBackward0>) Loss 499\n",
            "tensor([[ 0.9999],\n",
            "        [-0.9993]], grad_fn=<MmBackward>) predictions2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQlcookCfLkC",
        "outputId": "30c57a0e-72fc-4cc6-8c48-340977f75941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#1) Design model(input, output size, forward pass)\n",
        "#2) compute loss, optimizer\n",
        "#3)Training loop\n",
        "  # forward pass(compute prediction)\n",
        "  # backward pass(compute gradients and update weights)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MODEL():\n",
        "  def __init__(self, X, y, w):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.w = w\n",
        "    print(self.w, 'init_w')\n",
        "\n",
        "  def forward(self):\n",
        "    print(self.w, 'forward_w')\n",
        "    return torch.matmul(self.X, self.w)\n",
        "\n",
        "\n",
        "\n",
        "X = torch.tensor([[2, 3, 4], [1, 2, 4]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "w = torch.tensor([[2], [3], [1]], dtype = torch.float32, requires_grad = True)\n",
        "lr = 0.01\n",
        "\n",
        "predictions = torch.matmul(X, w)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w], lr = lr)\n",
        "\n",
        "for i in range(4):\n",
        "  cal = MODEL(X, y, w)\n",
        "  y_pred = cal.forward()\n",
        "  Loss = loss(y_pred, y)\n",
        "\n",
        "  Loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "X = torch.tensor([[4, 3, 1], [1, 0, -1]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "#w = np.array([0])\n",
        "predictions = torch.matmul(X, w)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "print('next')\n",
        "for i in range(500):\n",
        "  calcu = MODEL(X, y, w)\n",
        "  y_pred = calcu.forward()\n",
        "  Loss = loss(y_pred, y)\n",
        "  print(Loss, 'Loss', i)\n",
        "  Loss.backward()  \n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "predictions = torch.matmul(X, w)\n",
        "print(predictions, 'predictions2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[17.],\n",
            "        [12.]], grad_fn=<MmBackward>) predictions1\n",
            "tensor([[2.],\n",
            "        [3.],\n",
            "        [1.]], requires_grad=True) init_w\n",
            "tensor([[2.],\n",
            "        [3.],\n",
            "        [1.]], requires_grad=True) forward_w\n",
            "tensor([[ 1.5500],\n",
            "        [ 2.2600],\n",
            "        [-0.1600]], requires_grad=True) init_w\n",
            "tensor([[ 1.5500],\n",
            "        [ 2.2600],\n",
            "        [-0.1600]], requires_grad=True) forward_w\n",
            "tensor([[ 1.3209],\n",
            "        [ 1.8842],\n",
            "        [-0.7468]], requires_grad=True) init_w\n",
            "tensor([[ 1.3209],\n",
            "        [ 1.8842],\n",
            "        [-0.7468]], requires_grad=True) forward_w\n",
            "tensor([[ 1.2037],\n",
            "        [ 1.6929],\n",
            "        [-1.0432]], requires_grad=True) init_w\n",
            "tensor([[ 1.2037],\n",
            "        [ 1.6929],\n",
            "        [-1.0432]], requires_grad=True) forward_w\n",
            "tensor([[8.1664],\n",
            "        [2.3357]], grad_fn=<MmBackward>) predictions1\n",
            "next\n",
            "tensor([[ 1.1433],\n",
            "        [ 1.5952],\n",
            "        [-1.1924]], requires_grad=True) init_w\n",
            "tensor([[ 1.1433],\n",
            "        [ 1.5952],\n",
            "        [-1.1924]], requires_grad=True) forward_w\n",
            "tensor(31.2418, grad_fn=<MseLossBackward>) Loss 0\n",
            "tensor([[ 0.8233],\n",
            "        [ 1.3802],\n",
            "        [-1.2307]], requires_grad=True) init_w\n",
            "tensor([[ 0.8233],\n",
            "        [ 1.3802],\n",
            "        [-1.2307]], requires_grad=True) forward_w\n",
            "tensor(18.1992, grad_fn=<MseLossBackward>) Loss 1\n",
            "tensor([[ 0.5846],\n",
            "        [ 1.2241],\n",
            "        [-1.2522]], requires_grad=True) init_w\n",
            "tensor([[ 0.5846],\n",
            "        [ 1.2241],\n",
            "        [-1.2522]], requires_grad=True) forward_w\n",
            "tensor(11.0874, grad_fn=<MseLossBackward>) Loss 2\n",
            "tensor([[ 0.4059],\n",
            "        [ 1.1114],\n",
            "        [-1.2614]], requires_grad=True) init_w\n",
            "tensor([[ 0.4059],\n",
            "        [ 1.1114],\n",
            "        [-1.2614]], requires_grad=True) forward_w\n",
            "tensor(7.1923, grad_fn=<MseLossBackward>) Loss 3\n",
            "tensor([[ 0.2714],\n",
            "        [ 1.0305],\n",
            "        [-1.2617]], requires_grad=True) init_w\n",
            "tensor([[ 0.2714],\n",
            "        [ 1.0305],\n",
            "        [-1.2617]], requires_grad=True) forward_w\n",
            "tensor(5.0423, grad_fn=<MseLossBackward>) Loss 4\n",
            "tensor([[ 0.1694],\n",
            "        [ 0.9730],\n",
            "        [-1.2555]], requires_grad=True) init_w\n",
            "tensor([[ 0.1694],\n",
            "        [ 0.9730],\n",
            "        [-1.2555]], requires_grad=True) forward_w\n",
            "tensor(3.8397, grad_fn=<MseLossBackward>) Loss 5\n",
            "tensor([[ 0.0915],\n",
            "        [ 0.9328],\n",
            "        [-1.2447]], requires_grad=True) init_w\n",
            "tensor([[ 0.0915],\n",
            "        [ 0.9328],\n",
            "        [-1.2447]], requires_grad=True) forward_w\n",
            "tensor(3.1520, grad_fn=<MseLossBackward>) Loss 6\n",
            "tensor([[ 0.0314],\n",
            "        [ 0.9052],\n",
            "        [-1.2305]], requires_grad=True) init_w\n",
            "tensor([[ 0.0314],\n",
            "        [ 0.9052],\n",
            "        [-1.2305]], requires_grad=True) forward_w\n",
            "tensor(2.7445, grad_fn=<MseLossBackward>) Loss 7\n",
            "tensor([[-0.0157],\n",
            "        [ 0.8869],\n",
            "        [-1.2140]], requires_grad=True) init_w\n",
            "tensor([[-0.0157],\n",
            "        [ 0.8869],\n",
            "        [-1.2140]], requires_grad=True) forward_w\n",
            "tensor(2.4901, grad_fn=<MseLossBackward>) Loss 8\n",
            "tensor([[-0.0530],\n",
            "        [ 0.8753],\n",
            "        [-1.1959]], requires_grad=True) init_w\n",
            "tensor([[-0.0530],\n",
            "        [ 0.8753],\n",
            "        [-1.1959]], requires_grad=True) forward_w\n",
            "tensor(2.3197, grad_fn=<MseLossBackward>) Loss 9\n",
            "tensor([[-0.0832],\n",
            "        [ 0.8688],\n",
            "        [-1.1766]], requires_grad=True) init_w\n",
            "tensor([[-0.0832],\n",
            "        [ 0.8688],\n",
            "        [-1.1766]], requires_grad=True) forward_w\n",
            "tensor(2.1960, grad_fn=<MseLossBackward>) Loss 10\n",
            "tensor([[-0.1080],\n",
            "        [ 0.8659],\n",
            "        [-1.1567]], requires_grad=True) init_w\n",
            "tensor([[-0.1080],\n",
            "        [ 0.8659],\n",
            "        [-1.1567]], requires_grad=True) forward_w\n",
            "tensor(2.0986, grad_fn=<MseLossBackward>) Loss 11\n",
            "tensor([[-0.1288],\n",
            "        [ 0.8656],\n",
            "        [-1.1363]], requires_grad=True) init_w\n",
            "tensor([[-0.1288],\n",
            "        [ 0.8656],\n",
            "        [-1.1363]], requires_grad=True) forward_w\n",
            "tensor(2.0164, grad_fn=<MseLossBackward>) Loss 12\n",
            "tensor([[-0.1467],\n",
            "        [ 0.8672],\n",
            "        [-1.1156]], requires_grad=True) init_w\n",
            "tensor([[-0.1467],\n",
            "        [ 0.8672],\n",
            "        [-1.1156]], requires_grad=True) forward_w\n",
            "tensor(1.9434, grad_fn=<MseLossBackward>) Loss 13\n",
            "tensor([[-0.1624],\n",
            "        [ 0.8703],\n",
            "        [-1.0949]], requires_grad=True) init_w\n",
            "tensor([[-0.1624],\n",
            "        [ 0.8703],\n",
            "        [-1.0949]], requires_grad=True) forward_w\n",
            "tensor(1.8763, grad_fn=<MseLossBackward>) Loss 14\n",
            "tensor([[-0.1764],\n",
            "        [ 0.8743],\n",
            "        [-1.0743]], requires_grad=True) init_w\n",
            "tensor([[-0.1764],\n",
            "        [ 0.8743],\n",
            "        [-1.0743]], requires_grad=True) forward_w\n",
            "tensor(1.8134, grad_fn=<MseLossBackward>) Loss 15\n",
            "tensor([[-0.1891],\n",
            "        [ 0.8790],\n",
            "        [-1.0537]], requires_grad=True) init_w\n",
            "tensor([[-0.1891],\n",
            "        [ 0.8790],\n",
            "        [-1.0537]], requires_grad=True) forward_w\n",
            "tensor(1.7535, grad_fn=<MseLossBackward>) Loss 16\n",
            "tensor([[-0.2008],\n",
            "        [ 0.8842],\n",
            "        [-1.0334]], requires_grad=True) init_w\n",
            "tensor([[-0.2008],\n",
            "        [ 0.8842],\n",
            "        [-1.0334]], requires_grad=True) forward_w\n",
            "tensor(1.6961, grad_fn=<MseLossBackward>) Loss 17\n",
            "tensor([[-0.2118],\n",
            "        [ 0.8897],\n",
            "        [-1.0132]], requires_grad=True) init_w\n",
            "tensor([[-0.2118],\n",
            "        [ 0.8897],\n",
            "        [-1.0132]], requires_grad=True) forward_w\n",
            "tensor(1.6408, grad_fn=<MseLossBackward>) Loss 18\n",
            "tensor([[-0.2221],\n",
            "        [ 0.8954],\n",
            "        [-0.9933]], requires_grad=True) init_w\n",
            "tensor([[-0.2221],\n",
            "        [ 0.8954],\n",
            "        [-0.9933]], requires_grad=True) forward_w\n",
            "tensor(1.5876, grad_fn=<MseLossBackward>) Loss 19\n",
            "tensor([[-0.2320],\n",
            "        [ 0.9013],\n",
            "        [-0.9736]], requires_grad=True) init_w\n",
            "tensor([[-0.2320],\n",
            "        [ 0.9013],\n",
            "        [-0.9736]], requires_grad=True) forward_w\n",
            "tensor(1.5361, grad_fn=<MseLossBackward>) Loss 20\n",
            "tensor([[-0.2415],\n",
            "        [ 0.9072],\n",
            "        [-0.9542]], requires_grad=True) init_w\n",
            "tensor([[-0.2415],\n",
            "        [ 0.9072],\n",
            "        [-0.9542]], requires_grad=True) forward_w\n",
            "tensor(1.4864, grad_fn=<MseLossBackward>) Loss 21\n",
            "tensor([[-0.2507],\n",
            "        [ 0.9132],\n",
            "        [-0.9351]], requires_grad=True) init_w\n",
            "tensor([[-0.2507],\n",
            "        [ 0.9132],\n",
            "        [-0.9351]], requires_grad=True) forward_w\n",
            "tensor(1.4383, grad_fn=<MseLossBackward>) Loss 22\n",
            "tensor([[-0.2596],\n",
            "        [ 0.9191],\n",
            "        [-0.9163]], requires_grad=True) init_w\n",
            "tensor([[-0.2596],\n",
            "        [ 0.9191],\n",
            "        [-0.9163]], requires_grad=True) forward_w\n",
            "tensor(1.3917, grad_fn=<MseLossBackward>) Loss 23\n",
            "tensor([[-0.2683],\n",
            "        [ 0.9251],\n",
            "        [-0.8977]], requires_grad=True) init_w\n",
            "tensor([[-0.2683],\n",
            "        [ 0.9251],\n",
            "        [-0.8977]], requires_grad=True) forward_w\n",
            "tensor(1.3467, grad_fn=<MseLossBackward>) Loss 24\n",
            "tensor([[-0.2768],\n",
            "        [ 0.9309],\n",
            "        [-0.8795]], requires_grad=True) init_w\n",
            "tensor([[-0.2768],\n",
            "        [ 0.9309],\n",
            "        [-0.8795]], requires_grad=True) forward_w\n",
            "tensor(1.3031, grad_fn=<MseLossBackward>) Loss 25\n",
            "tensor([[-0.2850],\n",
            "        [ 0.9367],\n",
            "        [-0.8615]], requires_grad=True) init_w\n",
            "tensor([[-0.2850],\n",
            "        [ 0.9367],\n",
            "        [-0.8615]], requires_grad=True) forward_w\n",
            "tensor(1.2610, grad_fn=<MseLossBackward>) Loss 26\n",
            "tensor([[-0.2931],\n",
            "        [ 0.9425],\n",
            "        [-0.8438]], requires_grad=True) init_w\n",
            "tensor([[-0.2931],\n",
            "        [ 0.9425],\n",
            "        [-0.8438]], requires_grad=True) forward_w\n",
            "tensor(1.2202, grad_fn=<MseLossBackward>) Loss 27\n",
            "tensor([[-0.3011],\n",
            "        [ 0.9481],\n",
            "        [-0.8264]], requires_grad=True) init_w\n",
            "tensor([[-0.3011],\n",
            "        [ 0.9481],\n",
            "        [-0.8264]], requires_grad=True) forward_w\n",
            "tensor(1.1807, grad_fn=<MseLossBackward>) Loss 28\n",
            "tensor([[-0.3089],\n",
            "        [ 0.9537],\n",
            "        [-0.8093]], requires_grad=True) init_w\n",
            "tensor([[-0.3089],\n",
            "        [ 0.9537],\n",
            "        [-0.8093]], requires_grad=True) forward_w\n",
            "tensor(1.1425, grad_fn=<MseLossBackward>) Loss 29\n",
            "tensor([[-0.3165],\n",
            "        [ 0.9592],\n",
            "        [-0.7925]], requires_grad=True) init_w\n",
            "tensor([[-0.3165],\n",
            "        [ 0.9592],\n",
            "        [-0.7925]], requires_grad=True) forward_w\n",
            "tensor(1.1055, grad_fn=<MseLossBackward>) Loss 30\n",
            "tensor([[-0.3241],\n",
            "        [ 0.9647],\n",
            "        [-0.7759]], requires_grad=True) init_w\n",
            "tensor([[-0.3241],\n",
            "        [ 0.9647],\n",
            "        [-0.7759]], requires_grad=True) forward_w\n",
            "tensor(1.0698, grad_fn=<MseLossBackward>) Loss 31\n",
            "tensor([[-0.3315],\n",
            "        [ 0.9700],\n",
            "        [-0.7596]], requires_grad=True) init_w\n",
            "tensor([[-0.3315],\n",
            "        [ 0.9700],\n",
            "        [-0.7596]], requires_grad=True) forward_w\n",
            "tensor(1.0352, grad_fn=<MseLossBackward>) Loss 32\n",
            "tensor([[-0.3387],\n",
            "        [ 0.9753],\n",
            "        [-0.7436]], requires_grad=True) init_w\n",
            "tensor([[-0.3387],\n",
            "        [ 0.9753],\n",
            "        [-0.7436]], requires_grad=True) forward_w\n",
            "tensor(1.0017, grad_fn=<MseLossBackward>) Loss 33\n",
            "tensor([[-0.3459],\n",
            "        [ 0.9805],\n",
            "        [-0.7278]], requires_grad=True) init_w\n",
            "tensor([[-0.3459],\n",
            "        [ 0.9805],\n",
            "        [-0.7278]], requires_grad=True) forward_w\n",
            "tensor(0.9693, grad_fn=<MseLossBackward>) Loss 34\n",
            "tensor([[-0.3529],\n",
            "        [ 0.9856],\n",
            "        [-0.7123]], requires_grad=True) init_w\n",
            "tensor([[-0.3529],\n",
            "        [ 0.9856],\n",
            "        [-0.7123]], requires_grad=True) forward_w\n",
            "tensor(0.9379, grad_fn=<MseLossBackward>) Loss 35\n",
            "tensor([[-0.3598],\n",
            "        [ 0.9906],\n",
            "        [-0.6970]], requires_grad=True) init_w\n",
            "tensor([[-0.3598],\n",
            "        [ 0.9906],\n",
            "        [-0.6970]], requires_grad=True) forward_w\n",
            "tensor(0.9076, grad_fn=<MseLossBackward>) Loss 36\n",
            "tensor([[-0.3666],\n",
            "        [ 0.9955],\n",
            "        [-0.6820]], requires_grad=True) init_w\n",
            "tensor([[-0.3666],\n",
            "        [ 0.9955],\n",
            "        [-0.6820]], requires_grad=True) forward_w\n",
            "tensor(0.8782, grad_fn=<MseLossBackward>) Loss 37\n",
            "tensor([[-0.3733],\n",
            "        [ 1.0004],\n",
            "        [-0.6672]], requires_grad=True) init_w\n",
            "tensor([[-0.3733],\n",
            "        [ 1.0004],\n",
            "        [-0.6672]], requires_grad=True) forward_w\n",
            "tensor(0.8498, grad_fn=<MseLossBackward>) Loss 38\n",
            "tensor([[-0.3798],\n",
            "        [ 1.0051],\n",
            "        [-0.6527]], requires_grad=True) init_w\n",
            "tensor([[-0.3798],\n",
            "        [ 1.0051],\n",
            "        [-0.6527]], requires_grad=True) forward_w\n",
            "tensor(0.8223, grad_fn=<MseLossBackward>) Loss 39\n",
            "tensor([[-0.3863],\n",
            "        [ 1.0098],\n",
            "        [-0.6384]], requires_grad=True) init_w\n",
            "tensor([[-0.3863],\n",
            "        [ 1.0098],\n",
            "        [-0.6384]], requires_grad=True) forward_w\n",
            "tensor(0.7957, grad_fn=<MseLossBackward>) Loss 40\n",
            "tensor([[-0.3927],\n",
            "        [ 1.0145],\n",
            "        [-0.6243]], requires_grad=True) init_w\n",
            "tensor([[-0.3927],\n",
            "        [ 1.0145],\n",
            "        [-0.6243]], requires_grad=True) forward_w\n",
            "tensor(0.7700, grad_fn=<MseLossBackward>) Loss 41\n",
            "tensor([[-0.3989],\n",
            "        [ 1.0190],\n",
            "        [-0.6105]], requires_grad=True) init_w\n",
            "tensor([[-0.3989],\n",
            "        [ 1.0190],\n",
            "        [-0.6105]], requires_grad=True) forward_w\n",
            "tensor(0.7451, grad_fn=<MseLossBackward>) Loss 42\n",
            "tensor([[-0.4051],\n",
            "        [ 1.0235],\n",
            "        [-0.5969]], requires_grad=True) init_w\n",
            "tensor([[-0.4051],\n",
            "        [ 1.0235],\n",
            "        [-0.5969]], requires_grad=True) forward_w\n",
            "tensor(0.7210, grad_fn=<MseLossBackward>) Loss 43\n",
            "tensor([[-0.4111],\n",
            "        [ 1.0279],\n",
            "        [-0.5835]], requires_grad=True) init_w\n",
            "tensor([[-0.4111],\n",
            "        [ 1.0279],\n",
            "        [-0.5835]], requires_grad=True) forward_w\n",
            "tensor(0.6977, grad_fn=<MseLossBackward>) Loss 44\n",
            "tensor([[-0.4171],\n",
            "        [ 1.0322],\n",
            "        [-0.5703]], requires_grad=True) init_w\n",
            "tensor([[-0.4171],\n",
            "        [ 1.0322],\n",
            "        [-0.5703]], requires_grad=True) forward_w\n",
            "tensor(0.6751, grad_fn=<MseLossBackward>) Loss 45\n",
            "tensor([[-0.4229],\n",
            "        [ 1.0365],\n",
            "        [-0.5574]], requires_grad=True) init_w\n",
            "tensor([[-0.4229],\n",
            "        [ 1.0365],\n",
            "        [-0.5574]], requires_grad=True) forward_w\n",
            "tensor(0.6533, grad_fn=<MseLossBackward>) Loss 46\n",
            "tensor([[-0.4287],\n",
            "        [ 1.0407],\n",
            "        [-0.5446]], requires_grad=True) init_w\n",
            "tensor([[-0.4287],\n",
            "        [ 1.0407],\n",
            "        [-0.5446]], requires_grad=True) forward_w\n",
            "tensor(0.6321, grad_fn=<MseLossBackward>) Loss 47\n",
            "tensor([[-0.4343],\n",
            "        [ 1.0448],\n",
            "        [-0.5321]], requires_grad=True) init_w\n",
            "tensor([[-0.4343],\n",
            "        [ 1.0448],\n",
            "        [-0.5321]], requires_grad=True) forward_w\n",
            "tensor(0.6117, grad_fn=<MseLossBackward>) Loss 48\n",
            "tensor([[-0.4399],\n",
            "        [ 1.0488],\n",
            "        [-0.5198]], requires_grad=True) init_w\n",
            "tensor([[-0.4399],\n",
            "        [ 1.0488],\n",
            "        [-0.5198]], requires_grad=True) forward_w\n",
            "tensor(0.5919, grad_fn=<MseLossBackward>) Loss 49\n",
            "tensor([[-0.4454],\n",
            "        [ 1.0528],\n",
            "        [-0.5077]], requires_grad=True) init_w\n",
            "tensor([[-0.4454],\n",
            "        [ 1.0528],\n",
            "        [-0.5077]], requires_grad=True) forward_w\n",
            "tensor(0.5727, grad_fn=<MseLossBackward>) Loss 50\n",
            "tensor([[-0.4508],\n",
            "        [ 1.0567],\n",
            "        [-0.4957]], requires_grad=True) init_w\n",
            "tensor([[-0.4508],\n",
            "        [ 1.0567],\n",
            "        [-0.4957]], requires_grad=True) forward_w\n",
            "tensor(0.5542, grad_fn=<MseLossBackward>) Loss 51\n",
            "tensor([[-0.4561],\n",
            "        [ 1.0606],\n",
            "        [-0.4840]], requires_grad=True) init_w\n",
            "tensor([[-0.4561],\n",
            "        [ 1.0606],\n",
            "        [-0.4840]], requires_grad=True) forward_w\n",
            "tensor(0.5363, grad_fn=<MseLossBackward>) Loss 52\n",
            "tensor([[-0.4613],\n",
            "        [ 1.0644],\n",
            "        [-0.4724]], requires_grad=True) init_w\n",
            "tensor([[-0.4613],\n",
            "        [ 1.0644],\n",
            "        [-0.4724]], requires_grad=True) forward_w\n",
            "tensor(0.5189, grad_fn=<MseLossBackward>) Loss 53\n",
            "tensor([[-0.4664],\n",
            "        [ 1.0681],\n",
            "        [-0.4611]], requires_grad=True) init_w\n",
            "tensor([[-0.4664],\n",
            "        [ 1.0681],\n",
            "        [-0.4611]], requires_grad=True) forward_w\n",
            "tensor(0.5022, grad_fn=<MseLossBackward>) Loss 54\n",
            "tensor([[-0.4715],\n",
            "        [ 1.0718],\n",
            "        [-0.4499]], requires_grad=True) init_w\n",
            "tensor([[-0.4715],\n",
            "        [ 1.0718],\n",
            "        [-0.4499]], requires_grad=True) forward_w\n",
            "tensor(0.4859, grad_fn=<MseLossBackward>) Loss 55\n",
            "tensor([[-0.4765],\n",
            "        [ 1.0754],\n",
            "        [-0.4389]], requires_grad=True) init_w\n",
            "tensor([[-0.4765],\n",
            "        [ 1.0754],\n",
            "        [-0.4389]], requires_grad=True) forward_w\n",
            "tensor(0.4702, grad_fn=<MseLossBackward>) Loss 56\n",
            "tensor([[-0.4813],\n",
            "        [ 1.0790],\n",
            "        [-0.4281]], requires_grad=True) init_w\n",
            "tensor([[-0.4813],\n",
            "        [ 1.0790],\n",
            "        [-0.4281]], requires_grad=True) forward_w\n",
            "tensor(0.4550, grad_fn=<MseLossBackward>) Loss 57\n",
            "tensor([[-0.4861],\n",
            "        [ 1.0825],\n",
            "        [-0.4175]], requires_grad=True) init_w\n",
            "tensor([[-0.4861],\n",
            "        [ 1.0825],\n",
            "        [-0.4175]], requires_grad=True) forward_w\n",
            "tensor(0.4403, grad_fn=<MseLossBackward>) Loss 58\n",
            "tensor([[-0.4909],\n",
            "        [ 1.0859],\n",
            "        [-0.4070]], requires_grad=True) init_w\n",
            "tensor([[-0.4909],\n",
            "        [ 1.0859],\n",
            "        [-0.4070]], requires_grad=True) forward_w\n",
            "tensor(0.4260, grad_fn=<MseLossBackward>) Loss 59\n",
            "tensor([[-0.4955],\n",
            "        [ 1.0893],\n",
            "        [-0.3967]], requires_grad=True) init_w\n",
            "tensor([[-0.4955],\n",
            "        [ 1.0893],\n",
            "        [-0.3967]], requires_grad=True) forward_w\n",
            "tensor(0.4122, grad_fn=<MseLossBackward>) Loss 60\n",
            "tensor([[-0.5001],\n",
            "        [ 1.0926],\n",
            "        [-0.3866]], requires_grad=True) init_w\n",
            "tensor([[-0.5001],\n",
            "        [ 1.0926],\n",
            "        [-0.3866]], requires_grad=True) forward_w\n",
            "tensor(0.3989, grad_fn=<MseLossBackward>) Loss 61\n",
            "tensor([[-0.5046],\n",
            "        [ 1.0959],\n",
            "        [-0.3767]], requires_grad=True) init_w\n",
            "tensor([[-0.5046],\n",
            "        [ 1.0959],\n",
            "        [-0.3767]], requires_grad=True) forward_w\n",
            "tensor(0.3860, grad_fn=<MseLossBackward>) Loss 62\n",
            "tensor([[-0.5090],\n",
            "        [ 1.0991],\n",
            "        [-0.3669]], requires_grad=True) init_w\n",
            "tensor([[-0.5090],\n",
            "        [ 1.0991],\n",
            "        [-0.3669]], requires_grad=True) forward_w\n",
            "tensor(0.3735, grad_fn=<MseLossBackward>) Loss 63\n",
            "tensor([[-0.5134],\n",
            "        [ 1.1023],\n",
            "        [-0.3572]], requires_grad=True) init_w\n",
            "tensor([[-0.5134],\n",
            "        [ 1.1023],\n",
            "        [-0.3572]], requires_grad=True) forward_w\n",
            "tensor(0.3614, grad_fn=<MseLossBackward>) Loss 64\n",
            "tensor([[-0.5177],\n",
            "        [ 1.1054],\n",
            "        [-0.3478]], requires_grad=True) init_w\n",
            "tensor([[-0.5177],\n",
            "        [ 1.1054],\n",
            "        [-0.3478]], requires_grad=True) forward_w\n",
            "tensor(0.3497, grad_fn=<MseLossBackward>) Loss 65\n",
            "tensor([[-0.5219],\n",
            "        [ 1.1085],\n",
            "        [-0.3384]], requires_grad=True) init_w\n",
            "tensor([[-0.5219],\n",
            "        [ 1.1085],\n",
            "        [-0.3384]], requires_grad=True) forward_w\n",
            "tensor(0.3384, grad_fn=<MseLossBackward>) Loss 66\n",
            "tensor([[-0.5260],\n",
            "        [ 1.1115],\n",
            "        [-0.3293]], requires_grad=True) init_w\n",
            "tensor([[-0.5260],\n",
            "        [ 1.1115],\n",
            "        [-0.3293]], requires_grad=True) forward_w\n",
            "tensor(0.3275, grad_fn=<MseLossBackward>) Loss 67\n",
            "tensor([[-0.5301],\n",
            "        [ 1.1145],\n",
            "        [-0.3202]], requires_grad=True) init_w\n",
            "tensor([[-0.5301],\n",
            "        [ 1.1145],\n",
            "        [-0.3202]], requires_grad=True) forward_w\n",
            "tensor(0.3169, grad_fn=<MseLossBackward>) Loss 68\n",
            "tensor([[-0.5341],\n",
            "        [ 1.1174],\n",
            "        [-0.3114]], requires_grad=True) init_w\n",
            "tensor([[-0.5341],\n",
            "        [ 1.1174],\n",
            "        [-0.3114]], requires_grad=True) forward_w\n",
            "tensor(0.3066, grad_fn=<MseLossBackward>) Loss 69\n",
            "tensor([[-0.5381],\n",
            "        [ 1.1202],\n",
            "        [-0.3026]], requires_grad=True) init_w\n",
            "tensor([[-0.5381],\n",
            "        [ 1.1202],\n",
            "        [-0.3026]], requires_grad=True) forward_w\n",
            "tensor(0.2967, grad_fn=<MseLossBackward>) Loss 70\n",
            "tensor([[-0.5419],\n",
            "        [ 1.1231],\n",
            "        [-0.2940]], requires_grad=True) init_w\n",
            "tensor([[-0.5419],\n",
            "        [ 1.1231],\n",
            "        [-0.2940]], requires_grad=True) forward_w\n",
            "tensor(0.2871, grad_fn=<MseLossBackward>) Loss 71\n",
            "tensor([[-0.5458],\n",
            "        [ 1.1258],\n",
            "        [-0.2856]], requires_grad=True) init_w\n",
            "tensor([[-0.5458],\n",
            "        [ 1.1258],\n",
            "        [-0.2856]], requires_grad=True) forward_w\n",
            "tensor(0.2778, grad_fn=<MseLossBackward>) Loss 72\n",
            "tensor([[-0.5495],\n",
            "        [ 1.1286],\n",
            "        [-0.2773]], requires_grad=True) init_w\n",
            "tensor([[-0.5495],\n",
            "        [ 1.1286],\n",
            "        [-0.2773]], requires_grad=True) forward_w\n",
            "tensor(0.2688, grad_fn=<MseLossBackward>) Loss 73\n",
            "tensor([[-0.5532],\n",
            "        [ 1.1313],\n",
            "        [-0.2691]], requires_grad=True) init_w\n",
            "tensor([[-0.5532],\n",
            "        [ 1.1313],\n",
            "        [-0.2691]], requires_grad=True) forward_w\n",
            "tensor(0.2602, grad_fn=<MseLossBackward>) Loss 74\n",
            "tensor([[-0.5568],\n",
            "        [ 1.1339],\n",
            "        [-0.2611]], requires_grad=True) init_w\n",
            "tensor([[-0.5568],\n",
            "        [ 1.1339],\n",
            "        [-0.2611]], requires_grad=True) forward_w\n",
            "tensor(0.2517, grad_fn=<MseLossBackward>) Loss 75\n",
            "tensor([[-0.5604],\n",
            "        [ 1.1365],\n",
            "        [-0.2532]], requires_grad=True) init_w\n",
            "tensor([[-0.5604],\n",
            "        [ 1.1365],\n",
            "        [-0.2532]], requires_grad=True) forward_w\n",
            "tensor(0.2436, grad_fn=<MseLossBackward>) Loss 76\n",
            "tensor([[-0.5639],\n",
            "        [ 1.1391],\n",
            "        [-0.2454]], requires_grad=True) init_w\n",
            "tensor([[-0.5639],\n",
            "        [ 1.1391],\n",
            "        [-0.2454]], requires_grad=True) forward_w\n",
            "tensor(0.2357, grad_fn=<MseLossBackward>) Loss 77\n",
            "tensor([[-0.5674],\n",
            "        [ 1.1416],\n",
            "        [-0.2377]], requires_grad=True) init_w\n",
            "tensor([[-0.5674],\n",
            "        [ 1.1416],\n",
            "        [-0.2377]], requires_grad=True) forward_w\n",
            "tensor(0.2281, grad_fn=<MseLossBackward>) Loss 78\n",
            "tensor([[-0.5708],\n",
            "        [ 1.1441],\n",
            "        [-0.2302]], requires_grad=True) init_w\n",
            "tensor([[-0.5708],\n",
            "        [ 1.1441],\n",
            "        [-0.2302]], requires_grad=True) forward_w\n",
            "tensor(0.2207, grad_fn=<MseLossBackward>) Loss 79\n",
            "tensor([[-0.5741],\n",
            "        [ 1.1465],\n",
            "        [-0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.5741],\n",
            "        [ 1.1465],\n",
            "        [-0.2228]], requires_grad=True) forward_w\n",
            "tensor(0.2136, grad_fn=<MseLossBackward>) Loss 80\n",
            "tensor([[-0.5774],\n",
            "        [ 1.1489],\n",
            "        [-0.2155]], requires_grad=True) init_w\n",
            "tensor([[-0.5774],\n",
            "        [ 1.1489],\n",
            "        [-0.2155]], requires_grad=True) forward_w\n",
            "tensor(0.2067, grad_fn=<MseLossBackward>) Loss 81\n",
            "tensor([[-0.5807],\n",
            "        [ 1.1512],\n",
            "        [-0.2083]], requires_grad=True) init_w\n",
            "tensor([[-0.5807],\n",
            "        [ 1.1512],\n",
            "        [-0.2083]], requires_grad=True) forward_w\n",
            "tensor(0.2000, grad_fn=<MseLossBackward>) Loss 82\n",
            "tensor([[-0.5839],\n",
            "        [ 1.1536],\n",
            "        [-0.2013]], requires_grad=True) init_w\n",
            "tensor([[-0.5839],\n",
            "        [ 1.1536],\n",
            "        [-0.2013]], requires_grad=True) forward_w\n",
            "tensor(0.1935, grad_fn=<MseLossBackward>) Loss 83\n",
            "tensor([[-0.5870],\n",
            "        [ 1.1558],\n",
            "        [-0.1944]], requires_grad=True) init_w\n",
            "tensor([[-0.5870],\n",
            "        [ 1.1558],\n",
            "        [-0.1944]], requires_grad=True) forward_w\n",
            "tensor(0.1872, grad_fn=<MseLossBackward>) Loss 84\n",
            "tensor([[-0.5901],\n",
            "        [ 1.1581],\n",
            "        [-0.1875]], requires_grad=True) init_w\n",
            "tensor([[-0.5901],\n",
            "        [ 1.1581],\n",
            "        [-0.1875]], requires_grad=True) forward_w\n",
            "tensor(0.1812, grad_fn=<MseLossBackward>) Loss 85\n",
            "tensor([[-0.5931],\n",
            "        [ 1.1603],\n",
            "        [-0.1808]], requires_grad=True) init_w\n",
            "tensor([[-0.5931],\n",
            "        [ 1.1603],\n",
            "        [-0.1808]], requires_grad=True) forward_w\n",
            "tensor(0.1753, grad_fn=<MseLossBackward>) Loss 86\n",
            "tensor([[-0.5961],\n",
            "        [ 1.1625],\n",
            "        [-0.1742]], requires_grad=True) init_w\n",
            "tensor([[-0.5961],\n",
            "        [ 1.1625],\n",
            "        [-0.1742]], requires_grad=True) forward_w\n",
            "tensor(0.1697, grad_fn=<MseLossBackward>) Loss 87\n",
            "tensor([[-0.5990],\n",
            "        [ 1.1646],\n",
            "        [-0.1677]], requires_grad=True) init_w\n",
            "tensor([[-0.5990],\n",
            "        [ 1.1646],\n",
            "        [-0.1677]], requires_grad=True) forward_w\n",
            "tensor(0.1642, grad_fn=<MseLossBackward>) Loss 88\n",
            "tensor([[-0.6019],\n",
            "        [ 1.1667],\n",
            "        [-0.1614]], requires_grad=True) init_w\n",
            "tensor([[-0.6019],\n",
            "        [ 1.1667],\n",
            "        [-0.1614]], requires_grad=True) forward_w\n",
            "tensor(0.1589, grad_fn=<MseLossBackward>) Loss 89\n",
            "tensor([[-0.6047],\n",
            "        [ 1.1688],\n",
            "        [-0.1551]], requires_grad=True) init_w\n",
            "tensor([[-0.6047],\n",
            "        [ 1.1688],\n",
            "        [-0.1551]], requires_grad=True) forward_w\n",
            "tensor(0.1537, grad_fn=<MseLossBackward>) Loss 90\n",
            "tensor([[-0.6075],\n",
            "        [ 1.1708],\n",
            "        [-0.1489]], requires_grad=True) init_w\n",
            "tensor([[-0.6075],\n",
            "        [ 1.1708],\n",
            "        [-0.1489]], requires_grad=True) forward_w\n",
            "tensor(0.1487, grad_fn=<MseLossBackward>) Loss 91\n",
            "tensor([[-0.6103],\n",
            "        [ 1.1728],\n",
            "        [-0.1428]], requires_grad=True) init_w\n",
            "tensor([[-0.6103],\n",
            "        [ 1.1728],\n",
            "        [-0.1428]], requires_grad=True) forward_w\n",
            "tensor(0.1439, grad_fn=<MseLossBackward>) Loss 92\n",
            "tensor([[-0.6130],\n",
            "        [ 1.1748],\n",
            "        [-0.1368]], requires_grad=True) init_w\n",
            "tensor([[-0.6130],\n",
            "        [ 1.1748],\n",
            "        [-0.1368]], requires_grad=True) forward_w\n",
            "tensor(0.1393, grad_fn=<MseLossBackward>) Loss 93\n",
            "tensor([[-0.6156],\n",
            "        [ 1.1767],\n",
            "        [-0.1309]], requires_grad=True) init_w\n",
            "tensor([[-0.6156],\n",
            "        [ 1.1767],\n",
            "        [-0.1309]], requires_grad=True) forward_w\n",
            "tensor(0.1348, grad_fn=<MseLossBackward>) Loss 94\n",
            "tensor([[-0.6183],\n",
            "        [ 1.1786],\n",
            "        [-0.1252]], requires_grad=True) init_w\n",
            "tensor([[-0.6183],\n",
            "        [ 1.1786],\n",
            "        [-0.1252]], requires_grad=True) forward_w\n",
            "tensor(0.1304, grad_fn=<MseLossBackward>) Loss 95\n",
            "tensor([[-0.6208],\n",
            "        [ 1.1805],\n",
            "        [-0.1195]], requires_grad=True) init_w\n",
            "tensor([[-0.6208],\n",
            "        [ 1.1805],\n",
            "        [-0.1195]], requires_grad=True) forward_w\n",
            "tensor(0.1262, grad_fn=<MseLossBackward>) Loss 96\n",
            "tensor([[-0.6234],\n",
            "        [ 1.1823],\n",
            "        [-0.1139]], requires_grad=True) init_w\n",
            "tensor([[-0.6234],\n",
            "        [ 1.1823],\n",
            "        [-0.1139]], requires_grad=True) forward_w\n",
            "tensor(0.1221, grad_fn=<MseLossBackward>) Loss 97\n",
            "tensor([[-0.6259],\n",
            "        [ 1.1841],\n",
            "        [-0.1084]], requires_grad=True) init_w\n",
            "tensor([[-0.6259],\n",
            "        [ 1.1841],\n",
            "        [-0.1084]], requires_grad=True) forward_w\n",
            "tensor(0.1182, grad_fn=<MseLossBackward>) Loss 98\n",
            "tensor([[-0.6283],\n",
            "        [ 1.1859],\n",
            "        [-0.1029]], requires_grad=True) init_w\n",
            "tensor([[-0.6283],\n",
            "        [ 1.1859],\n",
            "        [-0.1029]], requires_grad=True) forward_w\n",
            "tensor(0.1143, grad_fn=<MseLossBackward>) Loss 99\n",
            "tensor([[-0.6307],\n",
            "        [ 1.1877],\n",
            "        [-0.0976]], requires_grad=True) init_w\n",
            "tensor([[-0.6307],\n",
            "        [ 1.1877],\n",
            "        [-0.0976]], requires_grad=True) forward_w\n",
            "tensor(0.1106, grad_fn=<MseLossBackward>) Loss 100\n",
            "tensor([[-0.6331],\n",
            "        [ 1.1894],\n",
            "        [-0.0924]], requires_grad=True) init_w\n",
            "tensor([[-0.6331],\n",
            "        [ 1.1894],\n",
            "        [-0.0924]], requires_grad=True) forward_w\n",
            "tensor(0.1071, grad_fn=<MseLossBackward>) Loss 101\n",
            "tensor([[-0.6354],\n",
            "        [ 1.1911],\n",
            "        [-0.0872]], requires_grad=True) init_w\n",
            "tensor([[-0.6354],\n",
            "        [ 1.1911],\n",
            "        [-0.0872]], requires_grad=True) forward_w\n",
            "tensor(0.1036, grad_fn=<MseLossBackward>) Loss 102\n",
            "tensor([[-0.6377],\n",
            "        [ 1.1928],\n",
            "        [-0.0821]], requires_grad=True) init_w\n",
            "tensor([[-0.6377],\n",
            "        [ 1.1928],\n",
            "        [-0.0821]], requires_grad=True) forward_w\n",
            "tensor(0.1003, grad_fn=<MseLossBackward>) Loss 103\n",
            "tensor([[-0.6400],\n",
            "        [ 1.1944],\n",
            "        [-0.0771]], requires_grad=True) init_w\n",
            "tensor([[-0.6400],\n",
            "        [ 1.1944],\n",
            "        [-0.0771]], requires_grad=True) forward_w\n",
            "tensor(0.0970, grad_fn=<MseLossBackward>) Loss 104\n",
            "tensor([[-0.6422],\n",
            "        [ 1.1960],\n",
            "        [-0.0722]], requires_grad=True) init_w\n",
            "tensor([[-0.6422],\n",
            "        [ 1.1960],\n",
            "        [-0.0722]], requires_grad=True) forward_w\n",
            "tensor(0.0939, grad_fn=<MseLossBackward>) Loss 105\n",
            "tensor([[-0.6444],\n",
            "        [ 1.1976],\n",
            "        [-0.0674]], requires_grad=True) init_w\n",
            "tensor([[-0.6444],\n",
            "        [ 1.1976],\n",
            "        [-0.0674]], requires_grad=True) forward_w\n",
            "tensor(0.0908, grad_fn=<MseLossBackward>) Loss 106\n",
            "tensor([[-0.6465],\n",
            "        [ 1.1992],\n",
            "        [-0.0626]], requires_grad=True) init_w\n",
            "tensor([[-0.6465],\n",
            "        [ 1.1992],\n",
            "        [-0.0626]], requires_grad=True) forward_w\n",
            "tensor(0.0879, grad_fn=<MseLossBackward>) Loss 107\n",
            "tensor([[-0.6486],\n",
            "        [ 1.2007],\n",
            "        [-0.0580]], requires_grad=True) init_w\n",
            "tensor([[-0.6486],\n",
            "        [ 1.2007],\n",
            "        [-0.0580]], requires_grad=True) forward_w\n",
            "tensor(0.0851, grad_fn=<MseLossBackward>) Loss 108\n",
            "tensor([[-0.6507],\n",
            "        [ 1.2022],\n",
            "        [-0.0534]], requires_grad=True) init_w\n",
            "tensor([[-0.6507],\n",
            "        [ 1.2022],\n",
            "        [-0.0534]], requires_grad=True) forward_w\n",
            "tensor(0.0823, grad_fn=<MseLossBackward>) Loss 109\n",
            "tensor([[-0.6527],\n",
            "        [ 1.2037],\n",
            "        [-0.0489]], requires_grad=True) init_w\n",
            "tensor([[-0.6527],\n",
            "        [ 1.2037],\n",
            "        [-0.0489]], requires_grad=True) forward_w\n",
            "tensor(0.0796, grad_fn=<MseLossBackward>) Loss 110\n",
            "tensor([[-0.6548],\n",
            "        [ 1.2052],\n",
            "        [-0.0444]], requires_grad=True) init_w\n",
            "tensor([[-0.6548],\n",
            "        [ 1.2052],\n",
            "        [-0.0444]], requires_grad=True) forward_w\n",
            "tensor(0.0771, grad_fn=<MseLossBackward>) Loss 111\n",
            "tensor([[-0.6567],\n",
            "        [ 1.2066],\n",
            "        [-0.0400]], requires_grad=True) init_w\n",
            "tensor([[-0.6567],\n",
            "        [ 1.2066],\n",
            "        [-0.0400]], requires_grad=True) forward_w\n",
            "tensor(0.0746, grad_fn=<MseLossBackward>) Loss 112\n",
            "tensor([[-0.6587],\n",
            "        [ 1.2080],\n",
            "        [-0.0357]], requires_grad=True) init_w\n",
            "tensor([[-0.6587],\n",
            "        [ 1.2080],\n",
            "        [-0.0357]], requires_grad=True) forward_w\n",
            "tensor(0.0722, grad_fn=<MseLossBackward>) Loss 113\n",
            "tensor([[-0.6606],\n",
            "        [ 1.2094],\n",
            "        [-0.0315]], requires_grad=True) init_w\n",
            "tensor([[-0.6606],\n",
            "        [ 1.2094],\n",
            "        [-0.0315]], requires_grad=True) forward_w\n",
            "tensor(0.0698, grad_fn=<MseLossBackward>) Loss 114\n",
            "tensor([[-0.6625],\n",
            "        [ 1.2108],\n",
            "        [-0.0273]], requires_grad=True) init_w\n",
            "tensor([[-0.6625],\n",
            "        [ 1.2108],\n",
            "        [-0.0273]], requires_grad=True) forward_w\n",
            "tensor(0.0676, grad_fn=<MseLossBackward>) Loss 115\n",
            "tensor([[-0.6643],\n",
            "        [ 1.2121],\n",
            "        [-0.0232]], requires_grad=True) init_w\n",
            "tensor([[-0.6643],\n",
            "        [ 1.2121],\n",
            "        [-0.0232]], requires_grad=True) forward_w\n",
            "tensor(0.0654, grad_fn=<MseLossBackward>) Loss 116\n",
            "tensor([[-0.6662],\n",
            "        [ 1.2134],\n",
            "        [-0.0192]], requires_grad=True) init_w\n",
            "tensor([[-0.6662],\n",
            "        [ 1.2134],\n",
            "        [-0.0192]], requires_grad=True) forward_w\n",
            "tensor(0.0633, grad_fn=<MseLossBackward>) Loss 117\n",
            "tensor([[-0.6679],\n",
            "        [ 1.2148],\n",
            "        [-0.0152]], requires_grad=True) init_w\n",
            "tensor([[-0.6679],\n",
            "        [ 1.2148],\n",
            "        [-0.0152]], requires_grad=True) forward_w\n",
            "tensor(0.0612, grad_fn=<MseLossBackward>) Loss 118\n",
            "tensor([[-0.6697],\n",
            "        [ 1.2160],\n",
            "        [-0.0113]], requires_grad=True) init_w\n",
            "tensor([[-0.6697],\n",
            "        [ 1.2160],\n",
            "        [-0.0113]], requires_grad=True) forward_w\n",
            "tensor(0.0592, grad_fn=<MseLossBackward>) Loss 119\n",
            "tensor([[-0.6714],\n",
            "        [ 1.2173],\n",
            "        [-0.0075]], requires_grad=True) init_w\n",
            "tensor([[-0.6714],\n",
            "        [ 1.2173],\n",
            "        [-0.0075]], requires_grad=True) forward_w\n",
            "tensor(0.0573, grad_fn=<MseLossBackward>) Loss 120\n",
            "tensor([[-0.6731],\n",
            "        [ 1.2185],\n",
            "        [-0.0037]], requires_grad=True) init_w\n",
            "tensor([[-0.6731],\n",
            "        [ 1.2185],\n",
            "        [-0.0037]], requires_grad=True) forward_w\n",
            "tensor(0.0555, grad_fn=<MseLossBackward>) Loss 121\n",
            "tensor([[-6.7482e-01],\n",
            "        [ 1.2198e+00],\n",
            "        [-5.8788e-06]], requires_grad=True) init_w\n",
            "tensor([[-6.7482e-01],\n",
            "        [ 1.2198e+00],\n",
            "        [-5.8788e-06]], requires_grad=True) forward_w\n",
            "tensor(0.0537, grad_fn=<MseLossBackward>) Loss 122\n",
            "tensor([[-0.6765],\n",
            "        [ 1.2210],\n",
            "        [ 0.0036]], requires_grad=True) init_w\n",
            "tensor([[-0.6765],\n",
            "        [ 1.2210],\n",
            "        [ 0.0036]], requires_grad=True) forward_w\n",
            "tensor(0.0519, grad_fn=<MseLossBackward>) Loss 123\n",
            "tensor([[-0.6781],\n",
            "        [ 1.2221],\n",
            "        [ 0.0072]], requires_grad=True) init_w\n",
            "tensor([[-0.6781],\n",
            "        [ 1.2221],\n",
            "        [ 0.0072]], requires_grad=True) forward_w\n",
            "tensor(0.0503, grad_fn=<MseLossBackward>) Loss 124\n",
            "tensor([[-0.6797],\n",
            "        [ 1.2233],\n",
            "        [ 0.0108]], requires_grad=True) init_w\n",
            "tensor([[-0.6797],\n",
            "        [ 1.2233],\n",
            "        [ 0.0108]], requires_grad=True) forward_w\n",
            "tensor(0.0486, grad_fn=<MseLossBackward>) Loss 125\n",
            "tensor([[-0.6813],\n",
            "        [ 1.2244],\n",
            "        [ 0.0142]], requires_grad=True) init_w\n",
            "tensor([[-0.6813],\n",
            "        [ 1.2244],\n",
            "        [ 0.0142]], requires_grad=True) forward_w\n",
            "tensor(0.0471, grad_fn=<MseLossBackward>) Loss 126\n",
            "tensor([[-0.6828],\n",
            "        [ 1.2256],\n",
            "        [ 0.0177]], requires_grad=True) init_w\n",
            "tensor([[-0.6828],\n",
            "        [ 1.2256],\n",
            "        [ 0.0177]], requires_grad=True) forward_w\n",
            "tensor(0.0455, grad_fn=<MseLossBackward>) Loss 127\n",
            "tensor([[-0.6843],\n",
            "        [ 1.2267],\n",
            "        [ 0.0210]], requires_grad=True) init_w\n",
            "tensor([[-0.6843],\n",
            "        [ 1.2267],\n",
            "        [ 0.0210]], requires_grad=True) forward_w\n",
            "tensor(0.0441, grad_fn=<MseLossBackward>) Loss 128\n",
            "tensor([[-0.6858],\n",
            "        [ 1.2278],\n",
            "        [ 0.0243]], requires_grad=True) init_w\n",
            "tensor([[-0.6858],\n",
            "        [ 1.2278],\n",
            "        [ 0.0243]], requires_grad=True) forward_w\n",
            "tensor(0.0426, grad_fn=<MseLossBackward>) Loss 129\n",
            "tensor([[-0.6873],\n",
            "        [ 1.2288],\n",
            "        [ 0.0276]], requires_grad=True) init_w\n",
            "tensor([[-0.6873],\n",
            "        [ 1.2288],\n",
            "        [ 0.0276]], requires_grad=True) forward_w\n",
            "tensor(0.0413, grad_fn=<MseLossBackward>) Loss 130\n",
            "tensor([[-0.6887],\n",
            "        [ 1.2299],\n",
            "        [ 0.0308]], requires_grad=True) init_w\n",
            "tensor([[-0.6887],\n",
            "        [ 1.2299],\n",
            "        [ 0.0308]], requires_grad=True) forward_w\n",
            "tensor(0.0399, grad_fn=<MseLossBackward>) Loss 131\n",
            "tensor([[-0.6902],\n",
            "        [ 1.2309],\n",
            "        [ 0.0339]], requires_grad=True) init_w\n",
            "tensor([[-0.6902],\n",
            "        [ 1.2309],\n",
            "        [ 0.0339]], requires_grad=True) forward_w\n",
            "tensor(0.0386, grad_fn=<MseLossBackward>) Loss 132\n",
            "tensor([[-0.6916],\n",
            "        [ 1.2319],\n",
            "        [ 0.0370]], requires_grad=True) init_w\n",
            "tensor([[-0.6916],\n",
            "        [ 1.2319],\n",
            "        [ 0.0370]], requires_grad=True) forward_w\n",
            "tensor(0.0374, grad_fn=<MseLossBackward>) Loss 133\n",
            "tensor([[-0.6929],\n",
            "        [ 1.2329],\n",
            "        [ 0.0401]], requires_grad=True) init_w\n",
            "tensor([[-0.6929],\n",
            "        [ 1.2329],\n",
            "        [ 0.0401]], requires_grad=True) forward_w\n",
            "tensor(0.0362, grad_fn=<MseLossBackward>) Loss 134\n",
            "tensor([[-0.6943],\n",
            "        [ 1.2339],\n",
            "        [ 0.0431]], requires_grad=True) init_w\n",
            "tensor([[-0.6943],\n",
            "        [ 1.2339],\n",
            "        [ 0.0431]], requires_grad=True) forward_w\n",
            "tensor(0.0350, grad_fn=<MseLossBackward>) Loss 135\n",
            "tensor([[-0.6956],\n",
            "        [ 1.2349],\n",
            "        [ 0.0460]], requires_grad=True) init_w\n",
            "tensor([[-0.6956],\n",
            "        [ 1.2349],\n",
            "        [ 0.0460]], requires_grad=True) forward_w\n",
            "tensor(0.0339, grad_fn=<MseLossBackward>) Loss 136\n",
            "tensor([[-0.6969],\n",
            "        [ 1.2359],\n",
            "        [ 0.0489]], requires_grad=True) init_w\n",
            "tensor([[-0.6969],\n",
            "        [ 1.2359],\n",
            "        [ 0.0489]], requires_grad=True) forward_w\n",
            "tensor(0.0328, grad_fn=<MseLossBackward>) Loss 137\n",
            "tensor([[-0.6982],\n",
            "        [ 1.2368],\n",
            "        [ 0.0518]], requires_grad=True) init_w\n",
            "tensor([[-0.6982],\n",
            "        [ 1.2368],\n",
            "        [ 0.0518]], requires_grad=True) forward_w\n",
            "tensor(0.0317, grad_fn=<MseLossBackward>) Loss 138\n",
            "tensor([[-0.6995],\n",
            "        [ 1.2377],\n",
            "        [ 0.0546]], requires_grad=True) init_w\n",
            "tensor([[-0.6995],\n",
            "        [ 1.2377],\n",
            "        [ 0.0546]], requires_grad=True) forward_w\n",
            "tensor(0.0307, grad_fn=<MseLossBackward>) Loss 139\n",
            "tensor([[-0.7008],\n",
            "        [ 1.2386],\n",
            "        [ 0.0574]], requires_grad=True) init_w\n",
            "tensor([[-0.7008],\n",
            "        [ 1.2386],\n",
            "        [ 0.0574]], requires_grad=True) forward_w\n",
            "tensor(0.0297, grad_fn=<MseLossBackward>) Loss 140\n",
            "tensor([[-0.7020],\n",
            "        [ 1.2395],\n",
            "        [ 0.0601]], requires_grad=True) init_w\n",
            "tensor([[-0.7020],\n",
            "        [ 1.2395],\n",
            "        [ 0.0601]], requires_grad=True) forward_w\n",
            "tensor(0.0287, grad_fn=<MseLossBackward>) Loss 141\n",
            "tensor([[-0.7032],\n",
            "        [ 1.2404],\n",
            "        [ 0.0628]], requires_grad=True) init_w\n",
            "tensor([[-0.7032],\n",
            "        [ 1.2404],\n",
            "        [ 0.0628]], requires_grad=True) forward_w\n",
            "tensor(0.0278, grad_fn=<MseLossBackward>) Loss 142\n",
            "tensor([[-0.7044],\n",
            "        [ 1.2413],\n",
            "        [ 0.0654]], requires_grad=True) init_w\n",
            "tensor([[-0.7044],\n",
            "        [ 1.2413],\n",
            "        [ 0.0654]], requires_grad=True) forward_w\n",
            "tensor(0.0269, grad_fn=<MseLossBackward>) Loss 143\n",
            "tensor([[-0.7055],\n",
            "        [ 1.2421],\n",
            "        [ 0.0680]], requires_grad=True) init_w\n",
            "tensor([[-0.7055],\n",
            "        [ 1.2421],\n",
            "        [ 0.0680]], requires_grad=True) forward_w\n",
            "tensor(0.0260, grad_fn=<MseLossBackward>) Loss 144\n",
            "tensor([[-0.7067],\n",
            "        [ 1.2429],\n",
            "        [ 0.0705]], requires_grad=True) init_w\n",
            "tensor([[-0.7067],\n",
            "        [ 1.2429],\n",
            "        [ 0.0705]], requires_grad=True) forward_w\n",
            "tensor(0.0252, grad_fn=<MseLossBackward>) Loss 145\n",
            "tensor([[-0.7078],\n",
            "        [ 1.2438],\n",
            "        [ 0.0730]], requires_grad=True) init_w\n",
            "tensor([[-0.7078],\n",
            "        [ 1.2438],\n",
            "        [ 0.0730]], requires_grad=True) forward_w\n",
            "tensor(0.0244, grad_fn=<MseLossBackward>) Loss 146\n",
            "tensor([[-0.7089],\n",
            "        [ 1.2446],\n",
            "        [ 0.0755]], requires_grad=True) init_w\n",
            "tensor([[-0.7089],\n",
            "        [ 1.2446],\n",
            "        [ 0.0755]], requires_grad=True) forward_w\n",
            "tensor(0.0236, grad_fn=<MseLossBackward>) Loss 147\n",
            "tensor([[-0.7100],\n",
            "        [ 1.2454],\n",
            "        [ 0.0779]], requires_grad=True) init_w\n",
            "tensor([[-0.7100],\n",
            "        [ 1.2454],\n",
            "        [ 0.0779]], requires_grad=True) forward_w\n",
            "tensor(0.0228, grad_fn=<MseLossBackward>) Loss 148\n",
            "tensor([[-0.7111],\n",
            "        [ 1.2462],\n",
            "        [ 0.0803]], requires_grad=True) init_w\n",
            "tensor([[-0.7111],\n",
            "        [ 1.2462],\n",
            "        [ 0.0803]], requires_grad=True) forward_w\n",
            "tensor(0.0221, grad_fn=<MseLossBackward>) Loss 149\n",
            "tensor([[-0.7122],\n",
            "        [ 1.2469],\n",
            "        [ 0.0826]], requires_grad=True) init_w\n",
            "tensor([[-0.7122],\n",
            "        [ 1.2469],\n",
            "        [ 0.0826]], requires_grad=True) forward_w\n",
            "tensor(0.0214, grad_fn=<MseLossBackward>) Loss 150\n",
            "tensor([[-0.7132],\n",
            "        [ 1.2477],\n",
            "        [ 0.0849]], requires_grad=True) init_w\n",
            "tensor([[-0.7132],\n",
            "        [ 1.2477],\n",
            "        [ 0.0849]], requires_grad=True) forward_w\n",
            "tensor(0.0207, grad_fn=<MseLossBackward>) Loss 151\n",
            "tensor([[-0.7142],\n",
            "        [ 1.2484],\n",
            "        [ 0.0872]], requires_grad=True) init_w\n",
            "tensor([[-0.7142],\n",
            "        [ 1.2484],\n",
            "        [ 0.0872]], requires_grad=True) forward_w\n",
            "tensor(0.0200, grad_fn=<MseLossBackward>) Loss 152\n",
            "tensor([[-0.7152],\n",
            "        [ 1.2492],\n",
            "        [ 0.0894]], requires_grad=True) init_w\n",
            "tensor([[-0.7152],\n",
            "        [ 1.2492],\n",
            "        [ 0.0894]], requires_grad=True) forward_w\n",
            "tensor(0.0194, grad_fn=<MseLossBackward>) Loss 153\n",
            "tensor([[-0.7162],\n",
            "        [ 1.2499],\n",
            "        [ 0.0916]], requires_grad=True) init_w\n",
            "tensor([[-0.7162],\n",
            "        [ 1.2499],\n",
            "        [ 0.0916]], requires_grad=True) forward_w\n",
            "tensor(0.0187, grad_fn=<MseLossBackward>) Loss 154\n",
            "tensor([[-0.7172],\n",
            "        [ 1.2506],\n",
            "        [ 0.0938]], requires_grad=True) init_w\n",
            "tensor([[-0.7172],\n",
            "        [ 1.2506],\n",
            "        [ 0.0938]], requires_grad=True) forward_w\n",
            "tensor(0.0181, grad_fn=<MseLossBackward>) Loss 155\n",
            "tensor([[-0.7182],\n",
            "        [ 1.2513],\n",
            "        [ 0.0959]], requires_grad=True) init_w\n",
            "tensor([[-0.7182],\n",
            "        [ 1.2513],\n",
            "        [ 0.0959]], requires_grad=True) forward_w\n",
            "tensor(0.0175, grad_fn=<MseLossBackward>) Loss 156\n",
            "tensor([[-0.7191],\n",
            "        [ 1.2520],\n",
            "        [ 0.0980]], requires_grad=True) init_w\n",
            "tensor([[-0.7191],\n",
            "        [ 1.2520],\n",
            "        [ 0.0980]], requires_grad=True) forward_w\n",
            "tensor(0.0170, grad_fn=<MseLossBackward>) Loss 157\n",
            "tensor([[-0.7200],\n",
            "        [ 1.2527],\n",
            "        [ 0.1000]], requires_grad=True) init_w\n",
            "tensor([[-0.7200],\n",
            "        [ 1.2527],\n",
            "        [ 0.1000]], requires_grad=True) forward_w\n",
            "tensor(0.0164, grad_fn=<MseLossBackward>) Loss 158\n",
            "tensor([[-0.7210],\n",
            "        [ 1.2533],\n",
            "        [ 0.1021]], requires_grad=True) init_w\n",
            "tensor([[-0.7210],\n",
            "        [ 1.2533],\n",
            "        [ 0.1021]], requires_grad=True) forward_w\n",
            "tensor(0.0159, grad_fn=<MseLossBackward>) Loss 159\n",
            "tensor([[-0.7219],\n",
            "        [ 1.2540],\n",
            "        [ 0.1041]], requires_grad=True) init_w\n",
            "tensor([[-0.7219],\n",
            "        [ 1.2540],\n",
            "        [ 0.1041]], requires_grad=True) forward_w\n",
            "tensor(0.0154, grad_fn=<MseLossBackward>) Loss 160\n",
            "tensor([[-0.7227],\n",
            "        [ 1.2546],\n",
            "        [ 0.1060]], requires_grad=True) init_w\n",
            "tensor([[-0.7227],\n",
            "        [ 1.2546],\n",
            "        [ 0.1060]], requires_grad=True) forward_w\n",
            "tensor(0.0149, grad_fn=<MseLossBackward>) Loss 161\n",
            "tensor([[-0.7236],\n",
            "        [ 1.2552],\n",
            "        [ 0.1079]], requires_grad=True) init_w\n",
            "tensor([[-0.7236],\n",
            "        [ 1.2552],\n",
            "        [ 0.1079]], requires_grad=True) forward_w\n",
            "tensor(0.0144, grad_fn=<MseLossBackward>) Loss 162\n",
            "tensor([[-0.7245],\n",
            "        [ 1.2559],\n",
            "        [ 0.1098]], requires_grad=True) init_w\n",
            "tensor([[-0.7245],\n",
            "        [ 1.2559],\n",
            "        [ 0.1098]], requires_grad=True) forward_w\n",
            "tensor(0.0139, grad_fn=<MseLossBackward>) Loss 163\n",
            "tensor([[-0.7253],\n",
            "        [ 1.2565],\n",
            "        [ 0.1117]], requires_grad=True) init_w\n",
            "tensor([[-0.7253],\n",
            "        [ 1.2565],\n",
            "        [ 0.1117]], requires_grad=True) forward_w\n",
            "tensor(0.0135, grad_fn=<MseLossBackward>) Loss 164\n",
            "tensor([[-0.7261],\n",
            "        [ 1.2571],\n",
            "        [ 0.1135]], requires_grad=True) init_w\n",
            "tensor([[-0.7261],\n",
            "        [ 1.2571],\n",
            "        [ 0.1135]], requires_grad=True) forward_w\n",
            "tensor(0.0131, grad_fn=<MseLossBackward>) Loss 165\n",
            "tensor([[-0.7269],\n",
            "        [ 1.2577],\n",
            "        [ 0.1153]], requires_grad=True) init_w\n",
            "tensor([[-0.7269],\n",
            "        [ 1.2577],\n",
            "        [ 0.1153]], requires_grad=True) forward_w\n",
            "tensor(0.0126, grad_fn=<MseLossBackward>) Loss 166\n",
            "tensor([[-0.7277],\n",
            "        [ 1.2583],\n",
            "        [ 0.1171]], requires_grad=True) init_w\n",
            "tensor([[-0.7277],\n",
            "        [ 1.2583],\n",
            "        [ 0.1171]], requires_grad=True) forward_w\n",
            "tensor(0.0122, grad_fn=<MseLossBackward>) Loss 167\n",
            "tensor([[-0.7285],\n",
            "        [ 1.2588],\n",
            "        [ 0.1188]], requires_grad=True) init_w\n",
            "tensor([[-0.7285],\n",
            "        [ 1.2588],\n",
            "        [ 0.1188]], requires_grad=True) forward_w\n",
            "tensor(0.0118, grad_fn=<MseLossBackward>) Loss 168\n",
            "tensor([[-0.7293],\n",
            "        [ 1.2594],\n",
            "        [ 0.1205]], requires_grad=True) init_w\n",
            "tensor([[-0.7293],\n",
            "        [ 1.2594],\n",
            "        [ 0.1205]], requires_grad=True) forward_w\n",
            "tensor(0.0114, grad_fn=<MseLossBackward>) Loss 169\n",
            "tensor([[-0.7301],\n",
            "        [ 1.2600],\n",
            "        [ 0.1222]], requires_grad=True) init_w\n",
            "tensor([[-0.7301],\n",
            "        [ 1.2600],\n",
            "        [ 0.1222]], requires_grad=True) forward_w\n",
            "tensor(0.0111, grad_fn=<MseLossBackward>) Loss 170\n",
            "tensor([[-0.7308],\n",
            "        [ 1.2605],\n",
            "        [ 0.1239]], requires_grad=True) init_w\n",
            "tensor([[-0.7308],\n",
            "        [ 1.2605],\n",
            "        [ 0.1239]], requires_grad=True) forward_w\n",
            "tensor(0.0107, grad_fn=<MseLossBackward>) Loss 171\n",
            "tensor([[-0.7316],\n",
            "        [ 1.2610],\n",
            "        [ 0.1255]], requires_grad=True) init_w\n",
            "tensor([[-0.7316],\n",
            "        [ 1.2610],\n",
            "        [ 0.1255]], requires_grad=True) forward_w\n",
            "tensor(0.0104, grad_fn=<MseLossBackward>) Loss 172\n",
            "tensor([[-0.7323],\n",
            "        [ 1.2616],\n",
            "        [ 0.1271]], requires_grad=True) init_w\n",
            "tensor([[-0.7323],\n",
            "        [ 1.2616],\n",
            "        [ 0.1271]], requires_grad=True) forward_w\n",
            "tensor(0.0100, grad_fn=<MseLossBackward>) Loss 173\n",
            "tensor([[-0.7330],\n",
            "        [ 1.2621],\n",
            "        [ 0.1287]], requires_grad=True) init_w\n",
            "tensor([[-0.7330],\n",
            "        [ 1.2621],\n",
            "        [ 0.1287]], requires_grad=True) forward_w\n",
            "tensor(0.0097, grad_fn=<MseLossBackward>) Loss 174\n",
            "tensor([[-0.7337],\n",
            "        [ 1.2626],\n",
            "        [ 0.1303]], requires_grad=True) init_w\n",
            "tensor([[-0.7337],\n",
            "        [ 1.2626],\n",
            "        [ 0.1303]], requires_grad=True) forward_w\n",
            "tensor(0.0094, grad_fn=<MseLossBackward>) Loss 175\n",
            "tensor([[-0.7344],\n",
            "        [ 1.2631],\n",
            "        [ 0.1318]], requires_grad=True) init_w\n",
            "tensor([[-0.7344],\n",
            "        [ 1.2631],\n",
            "        [ 0.1318]], requires_grad=True) forward_w\n",
            "tensor(0.0091, grad_fn=<MseLossBackward>) Loss 176\n",
            "tensor([[-0.7351],\n",
            "        [ 1.2636],\n",
            "        [ 0.1333]], requires_grad=True) init_w\n",
            "tensor([[-0.7351],\n",
            "        [ 1.2636],\n",
            "        [ 0.1333]], requires_grad=True) forward_w\n",
            "tensor(0.0088, grad_fn=<MseLossBackward>) Loss 177\n",
            "tensor([[-0.7357],\n",
            "        [ 1.2641],\n",
            "        [ 0.1348]], requires_grad=True) init_w\n",
            "tensor([[-0.7357],\n",
            "        [ 1.2641],\n",
            "        [ 0.1348]], requires_grad=True) forward_w\n",
            "tensor(0.0085, grad_fn=<MseLossBackward>) Loss 178\n",
            "tensor([[-0.7364],\n",
            "        [ 1.2646],\n",
            "        [ 0.1362]], requires_grad=True) init_w\n",
            "tensor([[-0.7364],\n",
            "        [ 1.2646],\n",
            "        [ 0.1362]], requires_grad=True) forward_w\n",
            "tensor(0.0082, grad_fn=<MseLossBackward>) Loss 179\n",
            "tensor([[-0.7370],\n",
            "        [ 1.2650],\n",
            "        [ 0.1377]], requires_grad=True) init_w\n",
            "tensor([[-0.7370],\n",
            "        [ 1.2650],\n",
            "        [ 0.1377]], requires_grad=True) forward_w\n",
            "tensor(0.0080, grad_fn=<MseLossBackward>) Loss 180\n",
            "tensor([[-0.7377],\n",
            "        [ 1.2655],\n",
            "        [ 0.1391]], requires_grad=True) init_w\n",
            "tensor([[-0.7377],\n",
            "        [ 1.2655],\n",
            "        [ 0.1391]], requires_grad=True) forward_w\n",
            "tensor(0.0077, grad_fn=<MseLossBackward>) Loss 181\n",
            "tensor([[-0.7383],\n",
            "        [ 1.2659],\n",
            "        [ 0.1404]], requires_grad=True) init_w\n",
            "tensor([[-0.7383],\n",
            "        [ 1.2659],\n",
            "        [ 0.1404]], requires_grad=True) forward_w\n",
            "tensor(0.0075, grad_fn=<MseLossBackward>) Loss 182\n",
            "tensor([[-0.7389],\n",
            "        [ 1.2664],\n",
            "        [ 0.1418]], requires_grad=True) init_w\n",
            "tensor([[-0.7389],\n",
            "        [ 1.2664],\n",
            "        [ 0.1418]], requires_grad=True) forward_w\n",
            "tensor(0.0072, grad_fn=<MseLossBackward>) Loss 183\n",
            "tensor([[-0.7395],\n",
            "        [ 1.2668],\n",
            "        [ 0.1431]], requires_grad=True) init_w\n",
            "tensor([[-0.7395],\n",
            "        [ 1.2668],\n",
            "        [ 0.1431]], requires_grad=True) forward_w\n",
            "tensor(0.0070, grad_fn=<MseLossBackward>) Loss 184\n",
            "tensor([[-0.7401],\n",
            "        [ 1.2673],\n",
            "        [ 0.1445]], requires_grad=True) init_w\n",
            "tensor([[-0.7401],\n",
            "        [ 1.2673],\n",
            "        [ 0.1445]], requires_grad=True) forward_w\n",
            "tensor(0.0068, grad_fn=<MseLossBackward>) Loss 185\n",
            "tensor([[-0.7407],\n",
            "        [ 1.2677],\n",
            "        [ 0.1458]], requires_grad=True) init_w\n",
            "tensor([[-0.7407],\n",
            "        [ 1.2677],\n",
            "        [ 0.1458]], requires_grad=True) forward_w\n",
            "tensor(0.0065, grad_fn=<MseLossBackward>) Loss 186\n",
            "tensor([[-0.7413],\n",
            "        [ 1.2681],\n",
            "        [ 0.1470]], requires_grad=True) init_w\n",
            "tensor([[-0.7413],\n",
            "        [ 1.2681],\n",
            "        [ 0.1470]], requires_grad=True) forward_w\n",
            "tensor(0.0063, grad_fn=<MseLossBackward>) Loss 187\n",
            "tensor([[-0.7418],\n",
            "        [ 1.2685],\n",
            "        [ 0.1483]], requires_grad=True) init_w\n",
            "tensor([[-0.7418],\n",
            "        [ 1.2685],\n",
            "        [ 0.1483]], requires_grad=True) forward_w\n",
            "tensor(0.0061, grad_fn=<MseLossBackward>) Loss 188\n",
            "tensor([[-0.7424],\n",
            "        [ 1.2689],\n",
            "        [ 0.1495]], requires_grad=True) init_w\n",
            "tensor([[-0.7424],\n",
            "        [ 1.2689],\n",
            "        [ 0.1495]], requires_grad=True) forward_w\n",
            "tensor(0.0059, grad_fn=<MseLossBackward>) Loss 189\n",
            "tensor([[-0.7430],\n",
            "        [ 1.2693],\n",
            "        [ 0.1507]], requires_grad=True) init_w\n",
            "tensor([[-0.7430],\n",
            "        [ 1.2693],\n",
            "        [ 0.1507]], requires_grad=True) forward_w\n",
            "tensor(0.0057, grad_fn=<MseLossBackward>) Loss 190\n",
            "tensor([[-0.7435],\n",
            "        [ 1.2697],\n",
            "        [ 0.1519]], requires_grad=True) init_w\n",
            "tensor([[-0.7435],\n",
            "        [ 1.2697],\n",
            "        [ 0.1519]], requires_grad=True) forward_w\n",
            "tensor(0.0056, grad_fn=<MseLossBackward>) Loss 191\n",
            "tensor([[-0.7440],\n",
            "        [ 1.2701],\n",
            "        [ 0.1531]], requires_grad=True) init_w\n",
            "tensor([[-0.7440],\n",
            "        [ 1.2701],\n",
            "        [ 0.1531]], requires_grad=True) forward_w\n",
            "tensor(0.0054, grad_fn=<MseLossBackward>) Loss 192\n",
            "tensor([[-0.7445],\n",
            "        [ 1.2705],\n",
            "        [ 0.1543]], requires_grad=True) init_w\n",
            "tensor([[-0.7445],\n",
            "        [ 1.2705],\n",
            "        [ 0.1543]], requires_grad=True) forward_w\n",
            "tensor(0.0052, grad_fn=<MseLossBackward>) Loss 193\n",
            "tensor([[-0.7451],\n",
            "        [ 1.2709],\n",
            "        [ 0.1554]], requires_grad=True) init_w\n",
            "tensor([[-0.7451],\n",
            "        [ 1.2709],\n",
            "        [ 0.1554]], requires_grad=True) forward_w\n",
            "tensor(0.0050, grad_fn=<MseLossBackward>) Loss 194\n",
            "tensor([[-0.7456],\n",
            "        [ 1.2712],\n",
            "        [ 0.1565]], requires_grad=True) init_w\n",
            "tensor([[-0.7456],\n",
            "        [ 1.2712],\n",
            "        [ 0.1565]], requires_grad=True) forward_w\n",
            "tensor(0.0049, grad_fn=<MseLossBackward>) Loss 195\n",
            "tensor([[-0.7461],\n",
            "        [ 1.2716],\n",
            "        [ 0.1576]], requires_grad=True) init_w\n",
            "tensor([[-0.7461],\n",
            "        [ 1.2716],\n",
            "        [ 0.1576]], requires_grad=True) forward_w\n",
            "tensor(0.0047, grad_fn=<MseLossBackward>) Loss 196\n",
            "tensor([[-0.7465],\n",
            "        [ 1.2719],\n",
            "        [ 0.1587]], requires_grad=True) init_w\n",
            "tensor([[-0.7465],\n",
            "        [ 1.2719],\n",
            "        [ 0.1587]], requires_grad=True) forward_w\n",
            "tensor(0.0046, grad_fn=<MseLossBackward>) Loss 197\n",
            "tensor([[-0.7470],\n",
            "        [ 1.2723],\n",
            "        [ 0.1598]], requires_grad=True) init_w\n",
            "tensor([[-0.7470],\n",
            "        [ 1.2723],\n",
            "        [ 0.1598]], requires_grad=True) forward_w\n",
            "tensor(0.0044, grad_fn=<MseLossBackward>) Loss 198\n",
            "tensor([[-0.7475],\n",
            "        [ 1.2726],\n",
            "        [ 0.1608]], requires_grad=True) init_w\n",
            "tensor([[-0.7475],\n",
            "        [ 1.2726],\n",
            "        [ 0.1608]], requires_grad=True) forward_w\n",
            "tensor(0.0043, grad_fn=<MseLossBackward>) Loss 199\n",
            "tensor([[-0.7480],\n",
            "        [ 1.2730],\n",
            "        [ 0.1618]], requires_grad=True) init_w\n",
            "tensor([[-0.7480],\n",
            "        [ 1.2730],\n",
            "        [ 0.1618]], requires_grad=True) forward_w\n",
            "tensor(0.0041, grad_fn=<MseLossBackward>) Loss 200\n",
            "tensor([[-0.7484],\n",
            "        [ 1.2733],\n",
            "        [ 0.1629]], requires_grad=True) init_w\n",
            "tensor([[-0.7484],\n",
            "        [ 1.2733],\n",
            "        [ 0.1629]], requires_grad=True) forward_w\n",
            "tensor(0.0040, grad_fn=<MseLossBackward>) Loss 201\n",
            "tensor([[-0.7489],\n",
            "        [ 1.2736],\n",
            "        [ 0.1638]], requires_grad=True) init_w\n",
            "tensor([[-0.7489],\n",
            "        [ 1.2736],\n",
            "        [ 0.1638]], requires_grad=True) forward_w\n",
            "tensor(0.0039, grad_fn=<MseLossBackward>) Loss 202\n",
            "tensor([[-0.7493],\n",
            "        [ 1.2740],\n",
            "        [ 0.1648]], requires_grad=True) init_w\n",
            "tensor([[-0.7493],\n",
            "        [ 1.2740],\n",
            "        [ 0.1648]], requires_grad=True) forward_w\n",
            "tensor(0.0037, grad_fn=<MseLossBackward>) Loss 203\n",
            "tensor([[-0.7498],\n",
            "        [ 1.2743],\n",
            "        [ 0.1658]], requires_grad=True) init_w\n",
            "tensor([[-0.7498],\n",
            "        [ 1.2743],\n",
            "        [ 0.1658]], requires_grad=True) forward_w\n",
            "tensor(0.0036, grad_fn=<MseLossBackward>) Loss 204\n",
            "tensor([[-0.7502],\n",
            "        [ 1.2746],\n",
            "        [ 0.1667]], requires_grad=True) init_w\n",
            "tensor([[-0.7502],\n",
            "        [ 1.2746],\n",
            "        [ 0.1667]], requires_grad=True) forward_w\n",
            "tensor(0.0035, grad_fn=<MseLossBackward>) Loss 205\n",
            "tensor([[-0.7506],\n",
            "        [ 1.2749],\n",
            "        [ 0.1677]], requires_grad=True) init_w\n",
            "tensor([[-0.7506],\n",
            "        [ 1.2749],\n",
            "        [ 0.1677]], requires_grad=True) forward_w\n",
            "tensor(0.0034, grad_fn=<MseLossBackward>) Loss 206\n",
            "tensor([[-0.7510],\n",
            "        [ 1.2752],\n",
            "        [ 0.1686]], requires_grad=True) init_w\n",
            "tensor([[-0.7510],\n",
            "        [ 1.2752],\n",
            "        [ 0.1686]], requires_grad=True) forward_w\n",
            "tensor(0.0033, grad_fn=<MseLossBackward>) Loss 207\n",
            "tensor([[-0.7514],\n",
            "        [ 1.2755],\n",
            "        [ 0.1695]], requires_grad=True) init_w\n",
            "tensor([[-0.7514],\n",
            "        [ 1.2755],\n",
            "        [ 0.1695]], requires_grad=True) forward_w\n",
            "tensor(0.0032, grad_fn=<MseLossBackward>) Loss 208\n",
            "tensor([[-0.7518],\n",
            "        [ 1.2758],\n",
            "        [ 0.1704]], requires_grad=True) init_w\n",
            "tensor([[-0.7518],\n",
            "        [ 1.2758],\n",
            "        [ 0.1704]], requires_grad=True) forward_w\n",
            "tensor(0.0031, grad_fn=<MseLossBackward>) Loss 209\n",
            "tensor([[-0.7522],\n",
            "        [ 1.2761],\n",
            "        [ 0.1713]], requires_grad=True) init_w\n",
            "tensor([[-0.7522],\n",
            "        [ 1.2761],\n",
            "        [ 0.1713]], requires_grad=True) forward_w\n",
            "tensor(0.0030, grad_fn=<MseLossBackward>) Loss 210\n",
            "tensor([[-0.7526],\n",
            "        [ 1.2764],\n",
            "        [ 0.1721]], requires_grad=True) init_w\n",
            "tensor([[-0.7526],\n",
            "        [ 1.2764],\n",
            "        [ 0.1721]], requires_grad=True) forward_w\n",
            "tensor(0.0029, grad_fn=<MseLossBackward>) Loss 211\n",
            "tensor([[-0.7530],\n",
            "        [ 1.2766],\n",
            "        [ 0.1730]], requires_grad=True) init_w\n",
            "tensor([[-0.7530],\n",
            "        [ 1.2766],\n",
            "        [ 0.1730]], requires_grad=True) forward_w\n",
            "tensor(0.0028, grad_fn=<MseLossBackward>) Loss 212\n",
            "tensor([[-0.7534],\n",
            "        [ 1.2769],\n",
            "        [ 0.1738]], requires_grad=True) init_w\n",
            "tensor([[-0.7534],\n",
            "        [ 1.2769],\n",
            "        [ 0.1738]], requires_grad=True) forward_w\n",
            "tensor(0.0027, grad_fn=<MseLossBackward>) Loss 213\n",
            "tensor([[-0.7537],\n",
            "        [ 1.2772],\n",
            "        [ 0.1746]], requires_grad=True) init_w\n",
            "tensor([[-0.7537],\n",
            "        [ 1.2772],\n",
            "        [ 0.1746]], requires_grad=True) forward_w\n",
            "tensor(0.0026, grad_fn=<MseLossBackward>) Loss 214\n",
            "tensor([[-0.7541],\n",
            "        [ 1.2774],\n",
            "        [ 0.1754]], requires_grad=True) init_w\n",
            "tensor([[-0.7541],\n",
            "        [ 1.2774],\n",
            "        [ 0.1754]], requires_grad=True) forward_w\n",
            "tensor(0.0025, grad_fn=<MseLossBackward>) Loss 215\n",
            "tensor([[-0.7545],\n",
            "        [ 1.2777],\n",
            "        [ 0.1762]], requires_grad=True) init_w\n",
            "tensor([[-0.7545],\n",
            "        [ 1.2777],\n",
            "        [ 0.1762]], requires_grad=True) forward_w\n",
            "tensor(0.0024, grad_fn=<MseLossBackward>) Loss 216\n",
            "tensor([[-0.7548],\n",
            "        [ 1.2780],\n",
            "        [ 0.1770]], requires_grad=True) init_w\n",
            "tensor([[-0.7548],\n",
            "        [ 1.2780],\n",
            "        [ 0.1770]], requires_grad=True) forward_w\n",
            "tensor(0.0024, grad_fn=<MseLossBackward>) Loss 217\n",
            "tensor([[-0.7552],\n",
            "        [ 1.2782],\n",
            "        [ 0.1778]], requires_grad=True) init_w\n",
            "tensor([[-0.7552],\n",
            "        [ 1.2782],\n",
            "        [ 0.1778]], requires_grad=True) forward_w\n",
            "tensor(0.0023, grad_fn=<MseLossBackward>) Loss 218\n",
            "tensor([[-0.7555],\n",
            "        [ 1.2785],\n",
            "        [ 0.1785]], requires_grad=True) init_w\n",
            "tensor([[-0.7555],\n",
            "        [ 1.2785],\n",
            "        [ 0.1785]], requires_grad=True) forward_w\n",
            "tensor(0.0022, grad_fn=<MseLossBackward>) Loss 219\n",
            "tensor([[-0.7558],\n",
            "        [ 1.2787],\n",
            "        [ 0.1792]], requires_grad=True) init_w\n",
            "tensor([[-0.7558],\n",
            "        [ 1.2787],\n",
            "        [ 0.1792]], requires_grad=True) forward_w\n",
            "tensor(0.0021, grad_fn=<MseLossBackward>) Loss 220\n",
            "tensor([[-0.7562],\n",
            "        [ 1.2789],\n",
            "        [ 0.1800]], requires_grad=True) init_w\n",
            "tensor([[-0.7562],\n",
            "        [ 1.2789],\n",
            "        [ 0.1800]], requires_grad=True) forward_w\n",
            "tensor(0.0021, grad_fn=<MseLossBackward>) Loss 221\n",
            "tensor([[-0.7565],\n",
            "        [ 1.2792],\n",
            "        [ 0.1807]], requires_grad=True) init_w\n",
            "tensor([[-0.7565],\n",
            "        [ 1.2792],\n",
            "        [ 0.1807]], requires_grad=True) forward_w\n",
            "tensor(0.0020, grad_fn=<MseLossBackward>) Loss 222\n",
            "tensor([[-0.7568],\n",
            "        [ 1.2794],\n",
            "        [ 0.1814]], requires_grad=True) init_w\n",
            "tensor([[-0.7568],\n",
            "        [ 1.2794],\n",
            "        [ 0.1814]], requires_grad=True) forward_w\n",
            "tensor(0.0019, grad_fn=<MseLossBackward>) Loss 223\n",
            "tensor([[-0.7571],\n",
            "        [ 1.2796],\n",
            "        [ 0.1821]], requires_grad=True) init_w\n",
            "tensor([[-0.7571],\n",
            "        [ 1.2796],\n",
            "        [ 0.1821]], requires_grad=True) forward_w\n",
            "tensor(0.0019, grad_fn=<MseLossBackward>) Loss 224\n",
            "tensor([[-0.7574],\n",
            "        [ 1.2799],\n",
            "        [ 0.1828]], requires_grad=True) init_w\n",
            "tensor([[-0.7574],\n",
            "        [ 1.2799],\n",
            "        [ 0.1828]], requires_grad=True) forward_w\n",
            "tensor(0.0018, grad_fn=<MseLossBackward>) Loss 225\n",
            "tensor([[-0.7577],\n",
            "        [ 1.2801],\n",
            "        [ 0.1834]], requires_grad=True) init_w\n",
            "tensor([[-0.7577],\n",
            "        [ 1.2801],\n",
            "        [ 0.1834]], requires_grad=True) forward_w\n",
            "tensor(0.0018, grad_fn=<MseLossBackward>) Loss 226\n",
            "tensor([[-0.7580],\n",
            "        [ 1.2803],\n",
            "        [ 0.1841]], requires_grad=True) init_w\n",
            "tensor([[-0.7580],\n",
            "        [ 1.2803],\n",
            "        [ 0.1841]], requires_grad=True) forward_w\n",
            "tensor(0.0017, grad_fn=<MseLossBackward>) Loss 227\n",
            "tensor([[-0.7583],\n",
            "        [ 1.2805],\n",
            "        [ 0.1848]], requires_grad=True) init_w\n",
            "tensor([[-0.7583],\n",
            "        [ 1.2805],\n",
            "        [ 0.1848]], requires_grad=True) forward_w\n",
            "tensor(0.0016, grad_fn=<MseLossBackward>) Loss 228\n",
            "tensor([[-0.7586],\n",
            "        [ 1.2807],\n",
            "        [ 0.1854]], requires_grad=True) init_w\n",
            "tensor([[-0.7586],\n",
            "        [ 1.2807],\n",
            "        [ 0.1854]], requires_grad=True) forward_w\n",
            "tensor(0.0016, grad_fn=<MseLossBackward>) Loss 229\n",
            "tensor([[-0.7589],\n",
            "        [ 1.2809],\n",
            "        [ 0.1860]], requires_grad=True) init_w\n",
            "tensor([[-0.7589],\n",
            "        [ 1.2809],\n",
            "        [ 0.1860]], requires_grad=True) forward_w\n",
            "tensor(0.0015, grad_fn=<MseLossBackward>) Loss 230\n",
            "tensor([[-0.7592],\n",
            "        [ 1.2811],\n",
            "        [ 0.1866]], requires_grad=True) init_w\n",
            "tensor([[-0.7592],\n",
            "        [ 1.2811],\n",
            "        [ 0.1866]], requires_grad=True) forward_w\n",
            "tensor(0.0015, grad_fn=<MseLossBackward>) Loss 231\n",
            "tensor([[-0.7595],\n",
            "        [ 1.2813],\n",
            "        [ 0.1873]], requires_grad=True) init_w\n",
            "tensor([[-0.7595],\n",
            "        [ 1.2813],\n",
            "        [ 0.1873]], requires_grad=True) forward_w\n",
            "tensor(0.0014, grad_fn=<MseLossBackward>) Loss 232\n",
            "tensor([[-0.7597],\n",
            "        [ 1.2815],\n",
            "        [ 0.1878]], requires_grad=True) init_w\n",
            "tensor([[-0.7597],\n",
            "        [ 1.2815],\n",
            "        [ 0.1878]], requires_grad=True) forward_w\n",
            "tensor(0.0014, grad_fn=<MseLossBackward>) Loss 233\n",
            "tensor([[-0.7600],\n",
            "        [ 1.2817],\n",
            "        [ 0.1884]], requires_grad=True) init_w\n",
            "tensor([[-0.7600],\n",
            "        [ 1.2817],\n",
            "        [ 0.1884]], requires_grad=True) forward_w\n",
            "tensor(0.0013, grad_fn=<MseLossBackward>) Loss 234\n",
            "tensor([[-0.7603],\n",
            "        [ 1.2819],\n",
            "        [ 0.1890]], requires_grad=True) init_w\n",
            "tensor([[-0.7603],\n",
            "        [ 1.2819],\n",
            "        [ 0.1890]], requires_grad=True) forward_w\n",
            "tensor(0.0013, grad_fn=<MseLossBackward>) Loss 235\n",
            "tensor([[-0.7605],\n",
            "        [ 1.2821],\n",
            "        [ 0.1896]], requires_grad=True) init_w\n",
            "tensor([[-0.7605],\n",
            "        [ 1.2821],\n",
            "        [ 0.1896]], requires_grad=True) forward_w\n",
            "tensor(0.0013, grad_fn=<MseLossBackward>) Loss 236\n",
            "tensor([[-0.7608],\n",
            "        [ 1.2823],\n",
            "        [ 0.1901]], requires_grad=True) init_w\n",
            "tensor([[-0.7608],\n",
            "        [ 1.2823],\n",
            "        [ 0.1901]], requires_grad=True) forward_w\n",
            "tensor(0.0012, grad_fn=<MseLossBackward>) Loss 237\n",
            "tensor([[-0.7610],\n",
            "        [ 1.2825],\n",
            "        [ 0.1907]], requires_grad=True) init_w\n",
            "tensor([[-0.7610],\n",
            "        [ 1.2825],\n",
            "        [ 0.1907]], requires_grad=True) forward_w\n",
            "tensor(0.0012, grad_fn=<MseLossBackward>) Loss 238\n",
            "tensor([[-0.7613],\n",
            "        [ 1.2826],\n",
            "        [ 0.1912]], requires_grad=True) init_w\n",
            "tensor([[-0.7613],\n",
            "        [ 1.2826],\n",
            "        [ 0.1912]], requires_grad=True) forward_w\n",
            "tensor(0.0011, grad_fn=<MseLossBackward>) Loss 239\n",
            "tensor([[-0.7615],\n",
            "        [ 1.2828],\n",
            "        [ 0.1918]], requires_grad=True) init_w\n",
            "tensor([[-0.7615],\n",
            "        [ 1.2828],\n",
            "        [ 0.1918]], requires_grad=True) forward_w\n",
            "tensor(0.0011, grad_fn=<MseLossBackward>) Loss 240\n",
            "tensor([[-0.7617],\n",
            "        [ 1.2830],\n",
            "        [ 0.1923]], requires_grad=True) init_w\n",
            "tensor([[-0.7617],\n",
            "        [ 1.2830],\n",
            "        [ 0.1923]], requires_grad=True) forward_w\n",
            "tensor(0.0011, grad_fn=<MseLossBackward>) Loss 241\n",
            "tensor([[-0.7620],\n",
            "        [ 1.2832],\n",
            "        [ 0.1928]], requires_grad=True) init_w\n",
            "tensor([[-0.7620],\n",
            "        [ 1.2832],\n",
            "        [ 0.1928]], requires_grad=True) forward_w\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>) Loss 242\n",
            "tensor([[-0.7622],\n",
            "        [ 1.2833],\n",
            "        [ 0.1933]], requires_grad=True) init_w\n",
            "tensor([[-0.7622],\n",
            "        [ 1.2833],\n",
            "        [ 0.1933]], requires_grad=True) forward_w\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>) Loss 243\n",
            "tensor([[-0.7624],\n",
            "        [ 1.2835],\n",
            "        [ 0.1938]], requires_grad=True) init_w\n",
            "tensor([[-0.7624],\n",
            "        [ 1.2835],\n",
            "        [ 0.1938]], requires_grad=True) forward_w\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>) Loss 244\n",
            "tensor([[-0.7626],\n",
            "        [ 1.2837],\n",
            "        [ 0.1943]], requires_grad=True) init_w\n",
            "tensor([[-0.7626],\n",
            "        [ 1.2837],\n",
            "        [ 0.1943]], requires_grad=True) forward_w\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>) Loss 245\n",
            "tensor([[-0.7629],\n",
            "        [ 1.2838],\n",
            "        [ 0.1948]], requires_grad=True) init_w\n",
            "tensor([[-0.7629],\n",
            "        [ 1.2838],\n",
            "        [ 0.1948]], requires_grad=True) forward_w\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>) Loss 246\n",
            "tensor([[-0.7631],\n",
            "        [ 1.2840],\n",
            "        [ 0.1953]], requires_grad=True) init_w\n",
            "tensor([[-0.7631],\n",
            "        [ 1.2840],\n",
            "        [ 0.1953]], requires_grad=True) forward_w\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>) Loss 247\n",
            "tensor([[-0.7633],\n",
            "        [ 1.2841],\n",
            "        [ 0.1957]], requires_grad=True) init_w\n",
            "tensor([[-0.7633],\n",
            "        [ 1.2841],\n",
            "        [ 0.1957]], requires_grad=True) forward_w\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>) Loss 248\n",
            "tensor([[-0.7635],\n",
            "        [ 1.2843],\n",
            "        [ 0.1962]], requires_grad=True) init_w\n",
            "tensor([[-0.7635],\n",
            "        [ 1.2843],\n",
            "        [ 0.1962]], requires_grad=True) forward_w\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>) Loss 249\n",
            "tensor([[-0.7637],\n",
            "        [ 1.2844],\n",
            "        [ 0.1967]], requires_grad=True) init_w\n",
            "tensor([[-0.7637],\n",
            "        [ 1.2844],\n",
            "        [ 0.1967]], requires_grad=True) forward_w\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>) Loss 250\n",
            "tensor([[-0.7639],\n",
            "        [ 1.2846],\n",
            "        [ 0.1971]], requires_grad=True) init_w\n",
            "tensor([[-0.7639],\n",
            "        [ 1.2846],\n",
            "        [ 0.1971]], requires_grad=True) forward_w\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>) Loss 251\n",
            "tensor([[-0.7641],\n",
            "        [ 1.2847],\n",
            "        [ 0.1975]], requires_grad=True) init_w\n",
            "tensor([[-0.7641],\n",
            "        [ 1.2847],\n",
            "        [ 0.1975]], requires_grad=True) forward_w\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>) Loss 252\n",
            "tensor([[-0.7643],\n",
            "        [ 1.2849],\n",
            "        [ 0.1980]], requires_grad=True) init_w\n",
            "tensor([[-0.7643],\n",
            "        [ 1.2849],\n",
            "        [ 0.1980]], requires_grad=True) forward_w\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>) Loss 253\n",
            "tensor([[-0.7645],\n",
            "        [ 1.2850],\n",
            "        [ 0.1984]], requires_grad=True) init_w\n",
            "tensor([[-0.7645],\n",
            "        [ 1.2850],\n",
            "        [ 0.1984]], requires_grad=True) forward_w\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>) Loss 254\n",
            "tensor([[-0.7647],\n",
            "        [ 1.2851],\n",
            "        [ 0.1988]], requires_grad=True) init_w\n",
            "tensor([[-0.7647],\n",
            "        [ 1.2851],\n",
            "        [ 0.1988]], requires_grad=True) forward_w\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>) Loss 255\n",
            "tensor([[-0.7649],\n",
            "        [ 1.2853],\n",
            "        [ 0.1992]], requires_grad=True) init_w\n",
            "tensor([[-0.7649],\n",
            "        [ 1.2853],\n",
            "        [ 0.1992]], requires_grad=True) forward_w\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>) Loss 256\n",
            "tensor([[-0.7650],\n",
            "        [ 1.2854],\n",
            "        [ 0.1996]], requires_grad=True) init_w\n",
            "tensor([[-0.7650],\n",
            "        [ 1.2854],\n",
            "        [ 0.1996]], requires_grad=True) forward_w\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>) Loss 257\n",
            "tensor([[-0.7652],\n",
            "        [ 1.2855],\n",
            "        [ 0.2000]], requires_grad=True) init_w\n",
            "tensor([[-0.7652],\n",
            "        [ 1.2855],\n",
            "        [ 0.2000]], requires_grad=True) forward_w\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>) Loss 258\n",
            "tensor([[-0.7654],\n",
            "        [ 1.2857],\n",
            "        [ 0.2004]], requires_grad=True) init_w\n",
            "tensor([[-0.7654],\n",
            "        [ 1.2857],\n",
            "        [ 0.2004]], requires_grad=True) forward_w\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>) Loss 259\n",
            "tensor([[-0.7656],\n",
            "        [ 1.2858],\n",
            "        [ 0.2008]], requires_grad=True) init_w\n",
            "tensor([[-0.7656],\n",
            "        [ 1.2858],\n",
            "        [ 0.2008]], requires_grad=True) forward_w\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>) Loss 260\n",
            "tensor([[-0.7657],\n",
            "        [ 1.2859],\n",
            "        [ 0.2012]], requires_grad=True) init_w\n",
            "tensor([[-0.7657],\n",
            "        [ 1.2859],\n",
            "        [ 0.2012]], requires_grad=True) forward_w\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>) Loss 261\n",
            "tensor([[-0.7659],\n",
            "        [ 1.2860],\n",
            "        [ 0.2015]], requires_grad=True) init_w\n",
            "tensor([[-0.7659],\n",
            "        [ 1.2860],\n",
            "        [ 0.2015]], requires_grad=True) forward_w\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>) Loss 262\n",
            "tensor([[-0.7661],\n",
            "        [ 1.2862],\n",
            "        [ 0.2019]], requires_grad=True) init_w\n",
            "tensor([[-0.7661],\n",
            "        [ 1.2862],\n",
            "        [ 0.2019]], requires_grad=True) forward_w\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>) Loss 263\n",
            "tensor([[-0.7662],\n",
            "        [ 1.2863],\n",
            "        [ 0.2023]], requires_grad=True) init_w\n",
            "tensor([[-0.7662],\n",
            "        [ 1.2863],\n",
            "        [ 0.2023]], requires_grad=True) forward_w\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>) Loss 264\n",
            "tensor([[-0.7664],\n",
            "        [ 1.2864],\n",
            "        [ 0.2026]], requires_grad=True) init_w\n",
            "tensor([[-0.7664],\n",
            "        [ 1.2864],\n",
            "        [ 0.2026]], requires_grad=True) forward_w\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>) Loss 265\n",
            "tensor([[-0.7666],\n",
            "        [ 1.2865],\n",
            "        [ 0.2030]], requires_grad=True) init_w\n",
            "tensor([[-0.7666],\n",
            "        [ 1.2865],\n",
            "        [ 0.2030]], requires_grad=True) forward_w\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>) Loss 266\n",
            "tensor([[-0.7667],\n",
            "        [ 1.2866],\n",
            "        [ 0.2033]], requires_grad=True) init_w\n",
            "tensor([[-0.7667],\n",
            "        [ 1.2866],\n",
            "        [ 0.2033]], requires_grad=True) forward_w\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>) Loss 267\n",
            "tensor([[-0.7669],\n",
            "        [ 1.2867],\n",
            "        [ 0.2036]], requires_grad=True) init_w\n",
            "tensor([[-0.7669],\n",
            "        [ 1.2867],\n",
            "        [ 0.2036]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 268\n",
            "tensor([[-0.7670],\n",
            "        [ 1.2868],\n",
            "        [ 0.2040]], requires_grad=True) init_w\n",
            "tensor([[-0.7670],\n",
            "        [ 1.2868],\n",
            "        [ 0.2040]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 269\n",
            "tensor([[-0.7672],\n",
            "        [ 1.2869],\n",
            "        [ 0.2043]], requires_grad=True) init_w\n",
            "tensor([[-0.7672],\n",
            "        [ 1.2869],\n",
            "        [ 0.2043]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 270\n",
            "tensor([[-0.7673],\n",
            "        [ 1.2870],\n",
            "        [ 0.2046]], requires_grad=True) init_w\n",
            "tensor([[-0.7673],\n",
            "        [ 1.2870],\n",
            "        [ 0.2046]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 271\n",
            "tensor([[-0.7674],\n",
            "        [ 1.2872],\n",
            "        [ 0.2049]], requires_grad=True) init_w\n",
            "tensor([[-0.7674],\n",
            "        [ 1.2872],\n",
            "        [ 0.2049]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 272\n",
            "tensor([[-0.7676],\n",
            "        [ 1.2873],\n",
            "        [ 0.2053]], requires_grad=True) init_w\n",
            "tensor([[-0.7676],\n",
            "        [ 1.2873],\n",
            "        [ 0.2053]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 273\n",
            "tensor([[-0.7677],\n",
            "        [ 1.2874],\n",
            "        [ 0.2056]], requires_grad=True) init_w\n",
            "tensor([[-0.7677],\n",
            "        [ 1.2874],\n",
            "        [ 0.2056]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 274\n",
            "tensor([[-0.7679],\n",
            "        [ 1.2875],\n",
            "        [ 0.2059]], requires_grad=True) init_w\n",
            "tensor([[-0.7679],\n",
            "        [ 1.2875],\n",
            "        [ 0.2059]], requires_grad=True) forward_w\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>) Loss 275\n",
            "tensor([[-0.7680],\n",
            "        [ 1.2875],\n",
            "        [ 0.2062]], requires_grad=True) init_w\n",
            "tensor([[-0.7680],\n",
            "        [ 1.2875],\n",
            "        [ 0.2062]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 276\n",
            "tensor([[-0.7681],\n",
            "        [ 1.2876],\n",
            "        [ 0.2064]], requires_grad=True) init_w\n",
            "tensor([[-0.7681],\n",
            "        [ 1.2876],\n",
            "        [ 0.2064]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 277\n",
            "tensor([[-0.7683],\n",
            "        [ 1.2877],\n",
            "        [ 0.2067]], requires_grad=True) init_w\n",
            "tensor([[-0.7683],\n",
            "        [ 1.2877],\n",
            "        [ 0.2067]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 278\n",
            "tensor([[-0.7684],\n",
            "        [ 1.2878],\n",
            "        [ 0.2070]], requires_grad=True) init_w\n",
            "tensor([[-0.7684],\n",
            "        [ 1.2878],\n",
            "        [ 0.2070]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 279\n",
            "tensor([[-0.7685],\n",
            "        [ 1.2879],\n",
            "        [ 0.2073]], requires_grad=True) init_w\n",
            "tensor([[-0.7685],\n",
            "        [ 1.2879],\n",
            "        [ 0.2073]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 280\n",
            "tensor([[-0.7686],\n",
            "        [ 1.2880],\n",
            "        [ 0.2076]], requires_grad=True) init_w\n",
            "tensor([[-0.7686],\n",
            "        [ 1.2880],\n",
            "        [ 0.2076]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 281\n",
            "tensor([[-0.7688],\n",
            "        [ 1.2881],\n",
            "        [ 0.2078]], requires_grad=True) init_w\n",
            "tensor([[-0.7688],\n",
            "        [ 1.2881],\n",
            "        [ 0.2078]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 282\n",
            "tensor([[-0.7689],\n",
            "        [ 1.2882],\n",
            "        [ 0.2081]], requires_grad=True) init_w\n",
            "tensor([[-0.7689],\n",
            "        [ 1.2882],\n",
            "        [ 0.2081]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 283\n",
            "tensor([[-0.7690],\n",
            "        [ 1.2883],\n",
            "        [ 0.2083]], requires_grad=True) init_w\n",
            "tensor([[-0.7690],\n",
            "        [ 1.2883],\n",
            "        [ 0.2083]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 284\n",
            "tensor([[-0.7691],\n",
            "        [ 1.2884],\n",
            "        [ 0.2086]], requires_grad=True) init_w\n",
            "tensor([[-0.7691],\n",
            "        [ 1.2884],\n",
            "        [ 0.2086]], requires_grad=True) forward_w\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>) Loss 285\n",
            "tensor([[-0.7692],\n",
            "        [ 1.2884],\n",
            "        [ 0.2089]], requires_grad=True) init_w\n",
            "tensor([[-0.7692],\n",
            "        [ 1.2884],\n",
            "        [ 0.2089]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 286\n",
            "tensor([[-0.7693],\n",
            "        [ 1.2885],\n",
            "        [ 0.2091]], requires_grad=True) init_w\n",
            "tensor([[-0.7693],\n",
            "        [ 1.2885],\n",
            "        [ 0.2091]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 287\n",
            "tensor([[-0.7694],\n",
            "        [ 1.2886],\n",
            "        [ 0.2093]], requires_grad=True) init_w\n",
            "tensor([[-0.7694],\n",
            "        [ 1.2886],\n",
            "        [ 0.2093]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 288\n",
            "tensor([[-0.7695],\n",
            "        [ 1.2887],\n",
            "        [ 0.2096]], requires_grad=True) init_w\n",
            "tensor([[-0.7695],\n",
            "        [ 1.2887],\n",
            "        [ 0.2096]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 289\n",
            "tensor([[-0.7696],\n",
            "        [ 1.2888],\n",
            "        [ 0.2098]], requires_grad=True) init_w\n",
            "tensor([[-0.7696],\n",
            "        [ 1.2888],\n",
            "        [ 0.2098]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 290\n",
            "tensor([[-0.7698],\n",
            "        [ 1.2888],\n",
            "        [ 0.2100]], requires_grad=True) init_w\n",
            "tensor([[-0.7698],\n",
            "        [ 1.2888],\n",
            "        [ 0.2100]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 291\n",
            "tensor([[-0.7699],\n",
            "        [ 1.2889],\n",
            "        [ 0.2103]], requires_grad=True) init_w\n",
            "tensor([[-0.7699],\n",
            "        [ 1.2889],\n",
            "        [ 0.2103]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 292\n",
            "tensor([[-0.7700],\n",
            "        [ 1.2890],\n",
            "        [ 0.2105]], requires_grad=True) init_w\n",
            "tensor([[-0.7700],\n",
            "        [ 1.2890],\n",
            "        [ 0.2105]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 293\n",
            "tensor([[-0.7701],\n",
            "        [ 1.2890],\n",
            "        [ 0.2107]], requires_grad=True) init_w\n",
            "tensor([[-0.7701],\n",
            "        [ 1.2890],\n",
            "        [ 0.2107]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 294\n",
            "tensor([[-0.7702],\n",
            "        [ 1.2891],\n",
            "        [ 0.2109]], requires_grad=True) init_w\n",
            "tensor([[-0.7702],\n",
            "        [ 1.2891],\n",
            "        [ 0.2109]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 295\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2892],\n",
            "        [ 0.2111]], requires_grad=True) init_w\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2892],\n",
            "        [ 0.2111]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 296\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2893],\n",
            "        [ 0.2114]], requires_grad=True) init_w\n",
            "tensor([[-0.7703],\n",
            "        [ 1.2893],\n",
            "        [ 0.2114]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 297\n",
            "tensor([[-0.7704],\n",
            "        [ 1.2893],\n",
            "        [ 0.2116]], requires_grad=True) init_w\n",
            "tensor([[-0.7704],\n",
            "        [ 1.2893],\n",
            "        [ 0.2116]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 298\n",
            "tensor([[-0.7705],\n",
            "        [ 1.2894],\n",
            "        [ 0.2118]], requires_grad=True) init_w\n",
            "tensor([[-0.7705],\n",
            "        [ 1.2894],\n",
            "        [ 0.2118]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 299\n",
            "tensor([[-0.7706],\n",
            "        [ 1.2895],\n",
            "        [ 0.2120]], requires_grad=True) init_w\n",
            "tensor([[-0.7706],\n",
            "        [ 1.2895],\n",
            "        [ 0.2120]], requires_grad=True) forward_w\n",
            "tensor(0.0002, grad_fn=<MseLossBackward>) Loss 300\n",
            "tensor([[-0.7707],\n",
            "        [ 1.2895],\n",
            "        [ 0.2122]], requires_grad=True) init_w\n",
            "tensor([[-0.7707],\n",
            "        [ 1.2895],\n",
            "        [ 0.2122]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 301\n",
            "tensor([[-0.7708],\n",
            "        [ 1.2896],\n",
            "        [ 0.2123]], requires_grad=True) init_w\n",
            "tensor([[-0.7708],\n",
            "        [ 1.2896],\n",
            "        [ 0.2123]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 302\n",
            "tensor([[-0.7709],\n",
            "        [ 1.2896],\n",
            "        [ 0.2125]], requires_grad=True) init_w\n",
            "tensor([[-0.7709],\n",
            "        [ 1.2896],\n",
            "        [ 0.2125]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 303\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2897],\n",
            "        [ 0.2127]], requires_grad=True) init_w\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2897],\n",
            "        [ 0.2127]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 304\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2898],\n",
            "        [ 0.2129]], requires_grad=True) init_w\n",
            "tensor([[-0.7710],\n",
            "        [ 1.2898],\n",
            "        [ 0.2129]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 305\n",
            "tensor([[-0.7711],\n",
            "        [ 1.2898],\n",
            "        [ 0.2131]], requires_grad=True) init_w\n",
            "tensor([[-0.7711],\n",
            "        [ 1.2898],\n",
            "        [ 0.2131]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 306\n",
            "tensor([[-0.7712],\n",
            "        [ 1.2899],\n",
            "        [ 0.2133]], requires_grad=True) init_w\n",
            "tensor([[-0.7712],\n",
            "        [ 1.2899],\n",
            "        [ 0.2133]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 307\n",
            "tensor([[-0.7713],\n",
            "        [ 1.2899],\n",
            "        [ 0.2134]], requires_grad=True) init_w\n",
            "tensor([[-0.7713],\n",
            "        [ 1.2899],\n",
            "        [ 0.2134]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 308\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2900],\n",
            "        [ 0.2136]], requires_grad=True) init_w\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2900],\n",
            "        [ 0.2136]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 309\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2901],\n",
            "        [ 0.2138]], requires_grad=True) init_w\n",
            "tensor([[-0.7714],\n",
            "        [ 1.2901],\n",
            "        [ 0.2138]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 310\n",
            "tensor([[-0.7715],\n",
            "        [ 1.2901],\n",
            "        [ 0.2139]], requires_grad=True) init_w\n",
            "tensor([[-0.7715],\n",
            "        [ 1.2901],\n",
            "        [ 0.2139]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 311\n",
            "tensor([[-0.7716],\n",
            "        [ 1.2902],\n",
            "        [ 0.2141]], requires_grad=True) init_w\n",
            "tensor([[-0.7716],\n",
            "        [ 1.2902],\n",
            "        [ 0.2141]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 312\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2902],\n",
            "        [ 0.2143]], requires_grad=True) init_w\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2902],\n",
            "        [ 0.2143]], requires_grad=True) forward_w\n",
            "tensor(0.0001, grad_fn=<MseLossBackward>) Loss 313\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2903],\n",
            "        [ 0.2144]], requires_grad=True) init_w\n",
            "tensor([[-0.7717],\n",
            "        [ 1.2903],\n",
            "        [ 0.2144]], requires_grad=True) forward_w\n",
            "tensor(9.7245e-05, grad_fn=<MseLossBackward>) Loss 314\n",
            "tensor([[-0.7718],\n",
            "        [ 1.2903],\n",
            "        [ 0.2146]], requires_grad=True) init_w\n",
            "tensor([[-0.7718],\n",
            "        [ 1.2903],\n",
            "        [ 0.2146]], requires_grad=True) forward_w\n",
            "tensor(9.4099e-05, grad_fn=<MseLossBackward>) Loss 315\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2147]], requires_grad=True) init_w\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2147]], requires_grad=True) forward_w\n",
            "tensor(9.1054e-05, grad_fn=<MseLossBackward>) Loss 316\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2149]], requires_grad=True) init_w\n",
            "tensor([[-0.7719],\n",
            "        [ 1.2904],\n",
            "        [ 0.2149]], requires_grad=True) forward_w\n",
            "tensor(8.8109e-05, grad_fn=<MseLossBackward>) Loss 317\n",
            "tensor([[-0.7720],\n",
            "        [ 1.2905],\n",
            "        [ 0.2150]], requires_grad=True) init_w\n",
            "tensor([[-0.7720],\n",
            "        [ 1.2905],\n",
            "        [ 0.2150]], requires_grad=True) forward_w\n",
            "tensor(8.5259e-05, grad_fn=<MseLossBackward>) Loss 318\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2905],\n",
            "        [ 0.2152]], requires_grad=True) init_w\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2905],\n",
            "        [ 0.2152]], requires_grad=True) forward_w\n",
            "tensor(8.2501e-05, grad_fn=<MseLossBackward>) Loss 319\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2906],\n",
            "        [ 0.2153]], requires_grad=True) init_w\n",
            "tensor([[-0.7721],\n",
            "        [ 1.2906],\n",
            "        [ 0.2153]], requires_grad=True) forward_w\n",
            "tensor(7.9832e-05, grad_fn=<MseLossBackward>) Loss 320\n",
            "tensor([[-0.7722],\n",
            "        [ 1.2906],\n",
            "        [ 0.2155]], requires_grad=True) init_w\n",
            "tensor([[-0.7722],\n",
            "        [ 1.2906],\n",
            "        [ 0.2155]], requires_grad=True) forward_w\n",
            "tensor(7.7250e-05, grad_fn=<MseLossBackward>) Loss 321\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2156]], requires_grad=True) init_w\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2156]], requires_grad=True) forward_w\n",
            "tensor(7.4750e-05, grad_fn=<MseLossBackward>) Loss 322\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2157]], requires_grad=True) init_w\n",
            "tensor([[-0.7723],\n",
            "        [ 1.2907],\n",
            "        [ 0.2157]], requires_grad=True) forward_w\n",
            "tensor(7.2333e-05, grad_fn=<MseLossBackward>) Loss 323\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2907],\n",
            "        [ 0.2159]], requires_grad=True) init_w\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2907],\n",
            "        [ 0.2159]], requires_grad=True) forward_w\n",
            "tensor(6.9993e-05, grad_fn=<MseLossBackward>) Loss 324\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2908],\n",
            "        [ 0.2160]], requires_grad=True) init_w\n",
            "tensor([[-0.7724],\n",
            "        [ 1.2908],\n",
            "        [ 0.2160]], requires_grad=True) forward_w\n",
            "tensor(6.7729e-05, grad_fn=<MseLossBackward>) Loss 325\n",
            "tensor([[-0.7725],\n",
            "        [ 1.2908],\n",
            "        [ 0.2161]], requires_grad=True) init_w\n",
            "tensor([[-0.7725],\n",
            "        [ 1.2908],\n",
            "        [ 0.2161]], requires_grad=True) forward_w\n",
            "tensor(6.5537e-05, grad_fn=<MseLossBackward>) Loss 326\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2163]], requires_grad=True) init_w\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2163]], requires_grad=True) forward_w\n",
            "tensor(6.3417e-05, grad_fn=<MseLossBackward>) Loss 327\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2164]], requires_grad=True) init_w\n",
            "tensor([[-0.7726],\n",
            "        [ 1.2909],\n",
            "        [ 0.2164]], requires_grad=True) forward_w\n",
            "tensor(6.1366e-05, grad_fn=<MseLossBackward>) Loss 328\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2165]], requires_grad=True) init_w\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2165]], requires_grad=True) forward_w\n",
            "tensor(5.9381e-05, grad_fn=<MseLossBackward>) Loss 329\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2166]], requires_grad=True) init_w\n",
            "tensor([[-0.7727],\n",
            "        [ 1.2910],\n",
            "        [ 0.2166]], requires_grad=True) forward_w\n",
            "tensor(5.7460e-05, grad_fn=<MseLossBackward>) Loss 330\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2910],\n",
            "        [ 0.2167]], requires_grad=True) init_w\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2910],\n",
            "        [ 0.2167]], requires_grad=True) forward_w\n",
            "tensor(5.5602e-05, grad_fn=<MseLossBackward>) Loss 331\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2911],\n",
            "        [ 0.2169]], requires_grad=True) init_w\n",
            "tensor([[-0.7728],\n",
            "        [ 1.2911],\n",
            "        [ 0.2169]], requires_grad=True) forward_w\n",
            "tensor(5.3803e-05, grad_fn=<MseLossBackward>) Loss 332\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2170]], requires_grad=True) init_w\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2170]], requires_grad=True) forward_w\n",
            "tensor(5.2063e-05, grad_fn=<MseLossBackward>) Loss 333\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2171]], requires_grad=True) init_w\n",
            "tensor([[-0.7729],\n",
            "        [ 1.2911],\n",
            "        [ 0.2171]], requires_grad=True) forward_w\n",
            "tensor(5.0378e-05, grad_fn=<MseLossBackward>) Loss 334\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2172]], requires_grad=True) init_w\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2172]], requires_grad=True) forward_w\n",
            "tensor(4.8749e-05, grad_fn=<MseLossBackward>) Loss 335\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2173]], requires_grad=True) init_w\n",
            "tensor([[-0.7730],\n",
            "        [ 1.2912],\n",
            "        [ 0.2173]], requires_grad=True) forward_w\n",
            "tensor(4.7172e-05, grad_fn=<MseLossBackward>) Loss 336\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2174]], requires_grad=True) init_w\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2174]], requires_grad=True) forward_w\n",
            "tensor(4.5646e-05, grad_fn=<MseLossBackward>) Loss 337\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2175]], requires_grad=True) init_w\n",
            "tensor([[-0.7731],\n",
            "        [ 1.2913],\n",
            "        [ 0.2175]], requires_grad=True) forward_w\n",
            "tensor(4.4168e-05, grad_fn=<MseLossBackward>) Loss 338\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2913],\n",
            "        [ 0.2176]], requires_grad=True) init_w\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2913],\n",
            "        [ 0.2176]], requires_grad=True) forward_w\n",
            "tensor(4.2740e-05, grad_fn=<MseLossBackward>) Loss 339\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2914],\n",
            "        [ 0.2177]], requires_grad=True) init_w\n",
            "tensor([[-0.7732],\n",
            "        [ 1.2914],\n",
            "        [ 0.2177]], requires_grad=True) forward_w\n",
            "tensor(4.1358e-05, grad_fn=<MseLossBackward>) Loss 340\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2178]], requires_grad=True) init_w\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2178]], requires_grad=True) forward_w\n",
            "tensor(4.0019e-05, grad_fn=<MseLossBackward>) Loss 341\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2179]], requires_grad=True) init_w\n",
            "tensor([[-0.7733],\n",
            "        [ 1.2914],\n",
            "        [ 0.2179]], requires_grad=True) forward_w\n",
            "tensor(3.8725e-05, grad_fn=<MseLossBackward>) Loss 342\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2180]], requires_grad=True) init_w\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2180]], requires_grad=True) forward_w\n",
            "tensor(3.7472e-05, grad_fn=<MseLossBackward>) Loss 343\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2181]], requires_grad=True) init_w\n",
            "tensor([[-0.7734],\n",
            "        [ 1.2915],\n",
            "        [ 0.2181]], requires_grad=True) forward_w\n",
            "tensor(3.6260e-05, grad_fn=<MseLossBackward>) Loss 344\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2915],\n",
            "        [ 0.2182]], requires_grad=True) init_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2915],\n",
            "        [ 0.2182]], requires_grad=True) forward_w\n",
            "tensor(3.5087e-05, grad_fn=<MseLossBackward>) Loss 345\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2183]], requires_grad=True) init_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2183]], requires_grad=True) forward_w\n",
            "tensor(3.3952e-05, grad_fn=<MseLossBackward>) Loss 346\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2184]], requires_grad=True) init_w\n",
            "tensor([[-0.7735],\n",
            "        [ 1.2916],\n",
            "        [ 0.2184]], requires_grad=True) forward_w\n",
            "tensor(3.2854e-05, grad_fn=<MseLossBackward>) Loss 347\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2185]], requires_grad=True) init_w\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2185]], requires_grad=True) forward_w\n",
            "tensor(3.1791e-05, grad_fn=<MseLossBackward>) Loss 348\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2186]], requires_grad=True) init_w\n",
            "tensor([[-0.7736],\n",
            "        [ 1.2916],\n",
            "        [ 0.2186]], requires_grad=True) forward_w\n",
            "tensor(3.0763e-05, grad_fn=<MseLossBackward>) Loss 349\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2187]], requires_grad=True) init_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2187]], requires_grad=True) forward_w\n",
            "tensor(2.9768e-05, grad_fn=<MseLossBackward>) Loss 350\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2188]], requires_grad=True) init_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2188]], requires_grad=True) forward_w\n",
            "tensor(2.8805e-05, grad_fn=<MseLossBackward>) Loss 351\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2189]], requires_grad=True) init_w\n",
            "tensor([[-0.7737],\n",
            "        [ 1.2917],\n",
            "        [ 0.2189]], requires_grad=True) forward_w\n",
            "tensor(2.7873e-05, grad_fn=<MseLossBackward>) Loss 352\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2189]], requires_grad=True) init_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2189]], requires_grad=True) forward_w\n",
            "tensor(2.6971e-05, grad_fn=<MseLossBackward>) Loss 353\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2190]], requires_grad=True) init_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2190]], requires_grad=True) forward_w\n",
            "tensor(2.6099e-05, grad_fn=<MseLossBackward>) Loss 354\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2191]], requires_grad=True) init_w\n",
            "tensor([[-0.7738],\n",
            "        [ 1.2918],\n",
            "        [ 0.2191]], requires_grad=True) forward_w\n",
            "tensor(2.5255e-05, grad_fn=<MseLossBackward>) Loss 355\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2918],\n",
            "        [ 0.2192]], requires_grad=True) init_w\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2918],\n",
            "        [ 0.2192]], requires_grad=True) forward_w\n",
            "tensor(2.4438e-05, grad_fn=<MseLossBackward>) Loss 356\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) init_w\n",
            "tensor([[-0.7739],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) forward_w\n",
            "tensor(2.3647e-05, grad_fn=<MseLossBackward>) Loss 357\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) init_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2193]], requires_grad=True) forward_w\n",
            "tensor(2.2882e-05, grad_fn=<MseLossBackward>) Loss 358\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2194]], requires_grad=True) init_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2194]], requires_grad=True) forward_w\n",
            "tensor(2.2142e-05, grad_fn=<MseLossBackward>) Loss 359\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2195]], requires_grad=True) init_w\n",
            "tensor([[-0.7740],\n",
            "        [ 1.2919],\n",
            "        [ 0.2195]], requires_grad=True) forward_w\n",
            "tensor(2.1425e-05, grad_fn=<MseLossBackward>) Loss 360\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) forward_w\n",
            "tensor(2.0733e-05, grad_fn=<MseLossBackward>) Loss 361\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2196]], requires_grad=True) forward_w\n",
            "tensor(2.0062e-05, grad_fn=<MseLossBackward>) Loss 362\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2197]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2197]], requires_grad=True) forward_w\n",
            "tensor(1.9413e-05, grad_fn=<MseLossBackward>) Loss 363\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) init_w\n",
            "tensor([[-0.7741],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) forward_w\n",
            "tensor(1.8785e-05, grad_fn=<MseLossBackward>) Loss 364\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) init_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2920],\n",
            "        [ 0.2198]], requires_grad=True) forward_w\n",
            "tensor(1.8178e-05, grad_fn=<MseLossBackward>) Loss 365\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2199]], requires_grad=True) init_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2199]], requires_grad=True) forward_w\n",
            "tensor(1.7589e-05, grad_fn=<MseLossBackward>) Loss 366\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) init_w\n",
            "tensor([[-0.7742],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) forward_w\n",
            "tensor(1.7021e-05, grad_fn=<MseLossBackward>) Loss 367\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) init_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2200]], requires_grad=True) forward_w\n",
            "tensor(1.6470e-05, grad_fn=<MseLossBackward>) Loss 368\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2201]], requires_grad=True) init_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2921],\n",
            "        [ 0.2201]], requires_grad=True) forward_w\n",
            "tensor(1.5937e-05, grad_fn=<MseLossBackward>) Loss 369\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) init_w\n",
            "tensor([[-0.7743],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) forward_w\n",
            "tensor(1.5422e-05, grad_fn=<MseLossBackward>) Loss 370\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2202]], requires_grad=True) forward_w\n",
            "tensor(1.4923e-05, grad_fn=<MseLossBackward>) Loss 371\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) forward_w\n",
            "tensor(1.4440e-05, grad_fn=<MseLossBackward>) Loss 372\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2203]], requires_grad=True) forward_w\n",
            "tensor(1.3973e-05, grad_fn=<MseLossBackward>) Loss 373\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2204]], requires_grad=True) init_w\n",
            "tensor([[-0.7744],\n",
            "        [ 1.2922],\n",
            "        [ 0.2204]], requires_grad=True) forward_w\n",
            "tensor(1.3521e-05, grad_fn=<MseLossBackward>) Loss 374\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) forward_w\n",
            "tensor(1.3084e-05, grad_fn=<MseLossBackward>) Loss 375\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2205]], requires_grad=True) forward_w\n",
            "tensor(1.2660e-05, grad_fn=<MseLossBackward>) Loss 376\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) forward_w\n",
            "tensor(1.2251e-05, grad_fn=<MseLossBackward>) Loss 377\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) init_w\n",
            "tensor([[-0.7745],\n",
            "        [ 1.2923],\n",
            "        [ 0.2206]], requires_grad=True) forward_w\n",
            "tensor(1.1855e-05, grad_fn=<MseLossBackward>) Loss 378\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) forward_w\n",
            "tensor(1.1471e-05, grad_fn=<MseLossBackward>) Loss 379\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2923],\n",
            "        [ 0.2207]], requires_grad=True) forward_w\n",
            "tensor(1.1100e-05, grad_fn=<MseLossBackward>) Loss 380\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) forward_w\n",
            "tensor(1.0741e-05, grad_fn=<MseLossBackward>) Loss 381\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) init_w\n",
            "tensor([[-0.7746],\n",
            "        [ 1.2924],\n",
            "        [ 0.2208]], requires_grad=True) forward_w\n",
            "tensor(1.0394e-05, grad_fn=<MseLossBackward>) Loss 382\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) forward_w\n",
            "tensor(1.0058e-05, grad_fn=<MseLossBackward>) Loss 383\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2209]], requires_grad=True) forward_w\n",
            "tensor(9.7323e-06, grad_fn=<MseLossBackward>) Loss 384\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) forward_w\n",
            "tensor(9.4174e-06, grad_fn=<MseLossBackward>) Loss 385\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2924],\n",
            "        [ 0.2210]], requires_grad=True) forward_w\n",
            "tensor(9.1126e-06, grad_fn=<MseLossBackward>) Loss 386\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) init_w\n",
            "tensor([[-0.7747],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) forward_w\n",
            "tensor(8.8181e-06, grad_fn=<MseLossBackward>) Loss 387\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2211]], requires_grad=True) forward_w\n",
            "tensor(8.5326e-06, grad_fn=<MseLossBackward>) Loss 388\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) forward_w\n",
            "tensor(8.2568e-06, grad_fn=<MseLossBackward>) Loss 389\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2212]], requires_grad=True) forward_w\n",
            "tensor(7.9897e-06, grad_fn=<MseLossBackward>) Loss 390\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) forward_w\n",
            "tensor(7.7312e-06, grad_fn=<MseLossBackward>) Loss 391\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) init_w\n",
            "tensor([[-0.7748],\n",
            "        [ 1.2925],\n",
            "        [ 0.2213]], requires_grad=True) forward_w\n",
            "tensor(7.4812e-06, grad_fn=<MseLossBackward>) Loss 392\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2925],\n",
            "        [ 0.2214]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2925],\n",
            "        [ 0.2214]], requires_grad=True) forward_w\n",
            "tensor(7.2390e-06, grad_fn=<MseLossBackward>) Loss 393\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) forward_w\n",
            "tensor(7.0049e-06, grad_fn=<MseLossBackward>) Loss 394\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2214]], requires_grad=True) forward_w\n",
            "tensor(6.7784e-06, grad_fn=<MseLossBackward>) Loss 395\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) forward_w\n",
            "tensor(6.5591e-06, grad_fn=<MseLossBackward>) Loss 396\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) init_w\n",
            "tensor([[-0.7749],\n",
            "        [ 1.2926],\n",
            "        [ 0.2215]], requires_grad=True) forward_w\n",
            "tensor(6.3471e-06, grad_fn=<MseLossBackward>) Loss 397\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) forward_w\n",
            "tensor(6.1418e-06, grad_fn=<MseLossBackward>) Loss 398\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) forward_w\n",
            "tensor(5.9430e-06, grad_fn=<MseLossBackward>) Loss 399\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2926],\n",
            "        [ 0.2216]], requires_grad=True) forward_w\n",
            "tensor(5.7508e-06, grad_fn=<MseLossBackward>) Loss 400\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) forward_w\n",
            "tensor(5.5648e-06, grad_fn=<MseLossBackward>) Loss 401\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2217]], requires_grad=True) forward_w\n",
            "tensor(5.3847e-06, grad_fn=<MseLossBackward>) Loss 402\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) init_w\n",
            "tensor([[-0.7750],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) forward_w\n",
            "tensor(5.2105e-06, grad_fn=<MseLossBackward>) Loss 403\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) forward_w\n",
            "tensor(5.0421e-06, grad_fn=<MseLossBackward>) Loss 404\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2218]], requires_grad=True) forward_w\n",
            "tensor(4.8789e-06, grad_fn=<MseLossBackward>) Loss 405\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) forward_w\n",
            "tensor(4.7211e-06, grad_fn=<MseLossBackward>) Loss 406\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) forward_w\n",
            "tensor(4.5685e-06, grad_fn=<MseLossBackward>) Loss 407\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2219]], requires_grad=True) forward_w\n",
            "tensor(4.4207e-06, grad_fn=<MseLossBackward>) Loss 408\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2220]], requires_grad=True) init_w\n",
            "tensor([[-0.7751],\n",
            "        [ 1.2927],\n",
            "        [ 0.2220]], requires_grad=True) forward_w\n",
            "tensor(4.2779e-06, grad_fn=<MseLossBackward>) Loss 409\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) forward_w\n",
            "tensor(4.1394e-06, grad_fn=<MseLossBackward>) Loss 410\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2220]], requires_grad=True) forward_w\n",
            "tensor(4.0056e-06, grad_fn=<MseLossBackward>) Loss 411\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor(3.8762e-06, grad_fn=<MseLossBackward>) Loss 412\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor(3.7507e-06, grad_fn=<MseLossBackward>) Loss 413\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor(3.6294e-06, grad_fn=<MseLossBackward>) Loss 414\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2221]], requires_grad=True) forward_w\n",
            "tensor(3.5119e-06, grad_fn=<MseLossBackward>) Loss 415\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) init_w\n",
            "tensor([[-0.7752],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) forward_w\n",
            "tensor(3.3985e-06, grad_fn=<MseLossBackward>) Loss 416\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) forward_w\n",
            "tensor(3.2885e-06, grad_fn=<MseLossBackward>) Loss 417\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2222]], requires_grad=True) forward_w\n",
            "tensor(3.1822e-06, grad_fn=<MseLossBackward>) Loss 418\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2928],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor(3.0793e-06, grad_fn=<MseLossBackward>) Loss 419\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor(2.9797e-06, grad_fn=<MseLossBackward>) Loss 420\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor(2.8832e-06, grad_fn=<MseLossBackward>) Loss 421\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2223]], requires_grad=True) forward_w\n",
            "tensor(2.7899e-06, grad_fn=<MseLossBackward>) Loss 422\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor(2.6996e-06, grad_fn=<MseLossBackward>) Loss 423\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor(2.6124e-06, grad_fn=<MseLossBackward>) Loss 424\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7753],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor(2.5278e-06, grad_fn=<MseLossBackward>) Loss 425\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2224]], requires_grad=True) forward_w\n",
            "tensor(2.4462e-06, grad_fn=<MseLossBackward>) Loss 426\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor(2.3670e-06, grad_fn=<MseLossBackward>) Loss 427\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor(2.2904e-06, grad_fn=<MseLossBackward>) Loss 428\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor(2.2163e-06, grad_fn=<MseLossBackward>) Loss 429\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2225]], requires_grad=True) forward_w\n",
            "tensor(2.1445e-06, grad_fn=<MseLossBackward>) Loss 430\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2929],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor(2.0751e-06, grad_fn=<MseLossBackward>) Loss 431\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor(2.0080e-06, grad_fn=<MseLossBackward>) Loss 432\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor(1.9430e-06, grad_fn=<MseLossBackward>) Loss 433\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) init_w\n",
            "tensor([[-0.7754],\n",
            "        [ 1.2930],\n",
            "        [ 0.2226]], requires_grad=True) forward_w\n",
            "tensor(1.8802e-06, grad_fn=<MseLossBackward>) Loss 434\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor(1.8193e-06, grad_fn=<MseLossBackward>) Loss 435\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor(1.7604e-06, grad_fn=<MseLossBackward>) Loss 436\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor(1.7035e-06, grad_fn=<MseLossBackward>) Loss 437\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor(1.6484e-06, grad_fn=<MseLossBackward>) Loss 438\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2227]], requires_grad=True) forward_w\n",
            "tensor(1.5950e-06, grad_fn=<MseLossBackward>) Loss 439\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor(1.5434e-06, grad_fn=<MseLossBackward>) Loss 440\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor(1.4935e-06, grad_fn=<MseLossBackward>) Loss 441\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor(1.4452e-06, grad_fn=<MseLossBackward>) Loss 442\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor(1.3984e-06, grad_fn=<MseLossBackward>) Loss 443\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2228]], requires_grad=True) forward_w\n",
            "tensor(1.3532e-06, grad_fn=<MseLossBackward>) Loss 444\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7755],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor(1.3094e-06, grad_fn=<MseLossBackward>) Loss 445\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2930],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor(1.2671e-06, grad_fn=<MseLossBackward>) Loss 446\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor(1.2260e-06, grad_fn=<MseLossBackward>) Loss 447\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor(1.1864e-06, grad_fn=<MseLossBackward>) Loss 448\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor(1.1481e-06, grad_fn=<MseLossBackward>) Loss 449\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2229]], requires_grad=True) forward_w\n",
            "tensor(1.1108e-06, grad_fn=<MseLossBackward>) Loss 450\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor(1.0750e-06, grad_fn=<MseLossBackward>) Loss 451\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor(1.0402e-06, grad_fn=<MseLossBackward>) Loss 452\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor(1.0065e-06, grad_fn=<MseLossBackward>) Loss 453\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor(9.7390e-07, grad_fn=<MseLossBackward>) Loss 454\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor(9.4248e-07, grad_fn=<MseLossBackward>) Loss 455\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor(9.1199e-07, grad_fn=<MseLossBackward>) Loss 456\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2230]], requires_grad=True) forward_w\n",
            "tensor(8.8251e-07, grad_fn=<MseLossBackward>) Loss 457\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor(8.5399e-07, grad_fn=<MseLossBackward>) Loss 458\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7756],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor(8.2630e-07, grad_fn=<MseLossBackward>) Loss 459\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor(7.9957e-07, grad_fn=<MseLossBackward>) Loss 460\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor(7.7367e-07, grad_fn=<MseLossBackward>) Loss 461\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor(7.4867e-07, grad_fn=<MseLossBackward>) Loss 462\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor(7.2444e-07, grad_fn=<MseLossBackward>) Loss 463\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2231]], requires_grad=True) forward_w\n",
            "tensor(7.0098e-07, grad_fn=<MseLossBackward>) Loss 464\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(6.7829e-07, grad_fn=<MseLossBackward>) Loss 465\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2931],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(6.5634e-07, grad_fn=<MseLossBackward>) Loss 466\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(6.3515e-07, grad_fn=<MseLossBackward>) Loss 467\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(6.1457e-07, grad_fn=<MseLossBackward>) Loss 468\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(5.9471e-07, grad_fn=<MseLossBackward>) Loss 469\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(5.7545e-07, grad_fn=<MseLossBackward>) Loss 470\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(5.5681e-07, grad_fn=<MseLossBackward>) Loss 471\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2232]], requires_grad=True) forward_w\n",
            "tensor(5.3885e-07, grad_fn=<MseLossBackward>) Loss 472\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(5.2142e-07, grad_fn=<MseLossBackward>) Loss 473\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(5.0456e-07, grad_fn=<MseLossBackward>) Loss 474\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(4.8821e-07, grad_fn=<MseLossBackward>) Loss 475\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(4.7247e-07, grad_fn=<MseLossBackward>) Loss 476\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7757],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(4.5715e-07, grad_fn=<MseLossBackward>) Loss 477\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(4.4243e-07, grad_fn=<MseLossBackward>) Loss 478\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(4.2814e-07, grad_fn=<MseLossBackward>) Loss 479\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(4.1426e-07, grad_fn=<MseLossBackward>) Loss 480\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2233]], requires_grad=True) forward_w\n",
            "tensor(4.0087e-07, grad_fn=<MseLossBackward>) Loss 481\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.8785e-07, grad_fn=<MseLossBackward>) Loss 482\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.7529e-07, grad_fn=<MseLossBackward>) Loss 483\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.6313e-07, grad_fn=<MseLossBackward>) Loss 484\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.5141e-07, grad_fn=<MseLossBackward>) Loss 485\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.4002e-07, grad_fn=<MseLossBackward>) Loss 486\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.2905e-07, grad_fn=<MseLossBackward>) Loss 487\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.1839e-07, grad_fn=<MseLossBackward>) Loss 488\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(3.0811e-07, grad_fn=<MseLossBackward>) Loss 489\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(2.9814e-07, grad_fn=<MseLossBackward>) Loss 490\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(2.8852e-07, grad_fn=<MseLossBackward>) Loss 491\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2234]], requires_grad=True) forward_w\n",
            "tensor(2.7922e-07, grad_fn=<MseLossBackward>) Loss 492\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor(2.7016e-07, grad_fn=<MseLossBackward>) Loss 493\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor(2.6138e-07, grad_fn=<MseLossBackward>) Loss 494\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor(2.5296e-07, grad_fn=<MseLossBackward>) Loss 495\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor(2.4476e-07, grad_fn=<MseLossBackward>) Loss 496\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2932],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor(2.3690e-07, grad_fn=<MseLossBackward>) Loss 497\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor(2.2925e-07, grad_fn=<MseLossBackward>) Loss 498\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) init_w\n",
            "tensor([[-0.7758],\n",
            "        [ 1.2933],\n",
            "        [ 0.2235]], requires_grad=True) forward_w\n",
            "tensor(2.2181e-07, grad_fn=<MseLossBackward>) Loss 499\n",
            "tensor([[ 0.9999],\n",
            "        [-0.9993]], grad_fn=<MmBackward>) predictions2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeFE9Rt-k-9z",
        "outputId": "91c3676f-aebb-439e-dccf-333562c7b738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#1) Design model(input, output size, forward pass)\n",
        "#2) compute loss, optimizer\n",
        "#3)Training loop\n",
        "  # forward pass(compute prediction)\n",
        "  # backward pass(compute gradients and update weights)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "X = torch.tensor([[2, 3, 4], [1, 2, 4]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "lr = 0.01\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "MODEL = nn.Linear(input_size, 1)\n",
        "\n",
        "predictions = MODEL(X)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(MODEL.parameters(), lr = lr)\n",
        "[w, b] = MODEL.parameters()\n",
        "print(w, 'w1', b, 'b1')\n",
        "for i in range(4):\n",
        "  y_pred = MODEL(X)\n",
        "  Loss = loss(y_pred, y)\n",
        "\n",
        "  Loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  if i % 3 == 0:\n",
        "    [w, b] = MODEL.parameters()\n",
        "\n",
        "    print(w, 'w', b, 'b', i, 'i', Loss)\n",
        "\n",
        "X = torch.tensor([[4, 3, 1], [1, 0, -1]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "#w = np.array([0])\n",
        "predictions = MODEL(X)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "print('next')\n",
        "[w, b] = MODEL.parameters()\n",
        "print(w, 'w', b, 'MODEL.parameters()')\n",
        "for i in range(500):\n",
        "  y_pred = MODEL(X)\n",
        "  Loss = loss(y_pred, y)\n",
        "  #print(Loss, 'Loss', i)\n",
        "  Loss.backward()  \n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  if i % 10 == 0:\n",
        "    [w, b] = MODEL.parameters()\n",
        "\n",
        "    print(w, 'w', b, 'b', i, 'i', Loss)\n",
        "\n",
        "predictions = MODEL(X)\n",
        "print(predictions, 'predictions2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.4191],\n",
            "        [-1.2427]], grad_fn=<AddmmBackward>) predictions1\n",
            "Parameter containing:\n",
            "tensor([[-0.2342,  0.0577, -0.2614]], requires_grad=True) w1 Parameter containing:\n",
            "tensor([-0.0783], requires_grad=True) b1\n",
            "Parameter containing:\n",
            "tensor([[-0.1834,  0.1352, -0.1549]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.0517], requires_grad=True) b 0 i tensor(2.9555, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1250,  0.2126, -0.0788]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.0327], requires_grad=True) b 3 i tensor(0.9471, grad_fn=<MseLossBackward>)\n",
            "tensor([[ 0.0266],\n",
            "        [-0.0789]], grad_fn=<AddmmBackward>) predictions1\n",
            "next\n",
            "Parameter containing:\n",
            "tensor([[-0.1250,  0.2126, -0.0788]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.0327], requires_grad=True) MODEL.parameters()\n",
            "Parameter containing:\n",
            "tensor([[-0.0952,  0.2418, -0.0598]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.0321], requires_grad=True) b 0 i tensor(0.8980, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0437,  0.3474,  0.0646]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.0861], requires_grad=True) b 10 i tensor(0.3530, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0659,  0.3852,  0.1499]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.1463], requires_grad=True) b 20 i tensor(0.2190, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0860,  0.4133,  0.2168]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.1944], requires_grad=True) b 30 i tensor(0.1361, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1019,  0.4353,  0.2694]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2323], requires_grad=True) b 40 i tensor(0.0846, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1144,  0.4527,  0.3109]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2622], requires_grad=True) b 50 i tensor(0.0526, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1243,  0.4664,  0.3436]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2858], requires_grad=True) b 60 i tensor(0.0327, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1320,  0.4772,  0.3694]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3044], requires_grad=True) b 70 i tensor(0.0203, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1382,  0.4857,  0.3897]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3190], requires_grad=True) b 80 i tensor(0.0126, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1430,  0.4924,  0.4057]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3306], requires_grad=True) b 90 i tensor(0.0078, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1468,  0.4977,  0.4184]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3397], requires_grad=True) b 100 i tensor(0.0049, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1498,  0.5019,  0.4283]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3469], requires_grad=True) b 110 i tensor(0.0030, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1522,  0.5052,  0.4362]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3525], requires_grad=True) b 120 i tensor(0.0019, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1541,  0.5078,  0.4424]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3570], requires_grad=True) b 130 i tensor(0.0012, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1555,  0.5098,  0.4472]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3605], requires_grad=True) b 140 i tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1567,  0.5114,  0.4511]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3633], requires_grad=True) b 150 i tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1576,  0.5127,  0.4541]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3654], requires_grad=True) b 160 i tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1583,  0.5137,  0.4565]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3672], requires_grad=True) b 170 i tensor(0.0002, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1589,  0.5145,  0.4584]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3685], requires_grad=True) b 180 i tensor(0.0001, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1594,  0.5151,  0.4599]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3696], requires_grad=True) b 190 i tensor(6.7260e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1597,  0.5156,  0.4610]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3704], requires_grad=True) b 200 i tensor(4.1795e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1600,  0.5160,  0.4620]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3711], requires_grad=True) b 210 i tensor(2.5971e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1602,  0.5163,  0.4627]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3716], requires_grad=True) b 220 i tensor(1.6138e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1604,  0.5165,  0.4633]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3720], requires_grad=True) b 230 i tensor(1.0028e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1605,  0.5167,  0.4637]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3724], requires_grad=True) b 240 i tensor(6.2312e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1606,  0.5169,  0.4641]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3726], requires_grad=True) b 250 i tensor(3.8719e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1607,  0.5170,  0.4643]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3728], requires_grad=True) b 260 i tensor(2.4061e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1608,  0.5171,  0.4646]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3730], requires_grad=True) b 270 i tensor(1.4951e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1608,  0.5172,  0.4647]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3731], requires_grad=True) b 280 i tensor(9.2900e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1609,  0.5172,  0.4649]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3732], requires_grad=True) b 290 i tensor(5.7724e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1609,  0.5173,  0.4650]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3733], requires_grad=True) b 300 i tensor(3.5870e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1609,  0.5173,  0.4651]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3734], requires_grad=True) b 310 i tensor(2.2286e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5173,  0.4651]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3734], requires_grad=True) b 320 i tensor(1.3852e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4652]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3734], requires_grad=True) b 330 i tensor(8.6043e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4652]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3735], requires_grad=True) b 340 i tensor(5.3482e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4653]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3735], requires_grad=True) b 350 i tensor(3.3213e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4653]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3735], requires_grad=True) b 360 i tensor(2.0656e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4653]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3735], requires_grad=True) b 370 i tensor(1.2834e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4653]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3735], requires_grad=True) b 380 i tensor(7.9819e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4653]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3735], requires_grad=True) b 390 i tensor(4.9647e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 400 i tensor(3.0911e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 410 i tensor(1.9213e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 420 i tensor(1.1974e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 430 i tensor(7.4301e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 440 i tensor(4.5902e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 450 i tensor(2.8747e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 460 i tensor(1.7701e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 470 i tensor(1.1269e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 480 i tensor(6.9948e-11, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.1610,  0.5174,  0.4654]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.3736], requires_grad=True) b 490 i tensor(4.3151e-11, grad_fn=<MseLossBackward>)\n",
            "tensor([[ 1.0000],\n",
            "        [-1.0000]], grad_fn=<AddmmBackward>) predictions2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJBK6d1HXzu6"
      },
      "source": [
        "# Linear Regression\n",
        "# tutorial 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi4DsX23rgCb",
        "outputId": "569182d2-778f-45ae-d99d-77e776b9e387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#1) Design model(input, output size, forward pass)\n",
        "#2) compute loss, optimizer\n",
        "#3)Training loop\n",
        "  # forward pass(compute prediction)\n",
        "  # backward pass(compute gradients and update weights)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "X = torch.tensor([[2, 3, 4], [1, 2, 4]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "lr = 0.01\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#MODEL = nn.Linear(input_size, 1)\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, out_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "\n",
        "    self.lin = nn.Linear(input_dim, out_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.lin(X)\n",
        "\n",
        "\n",
        "MODEL = LinearRegression(input_size, 1)\n",
        "\n",
        "\n",
        "\n",
        "predictions = MODEL(X)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(MODEL.parameters(), lr = lr)\n",
        "[w, b] = MODEL.parameters()\n",
        "print(w, 'w1', b, 'b1')\n",
        "for i in range(4):\n",
        "  y_pred = MODEL(X)\n",
        "  Loss = loss(y_pred, y)\n",
        "\n",
        "  Loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  if i % 3 == 0:\n",
        "    [w, b] = MODEL.parameters()\n",
        "\n",
        "    print(w, 'w', b, 'b', i, 'i', Loss)\n",
        "\n",
        "X = torch.tensor([[4, 3, 1], [1, 0, -1]], dtype = torch.float32)\n",
        "y = torch.tensor([[1], [-1]], dtype = torch.float32)\n",
        "#w = np.array([0])\n",
        "predictions = MODEL.forward(X)\n",
        "print(predictions, 'predictions1')\n",
        "\n",
        "print('next')\n",
        "[w, b] = MODEL.parameters()\n",
        "print(w, 'w', b, 'MODEL.parameters()')\n",
        "for i in range(500):\n",
        "  y_pred = MODEL(X)\n",
        "  Loss = loss(y_pred, y)\n",
        "  #print(Loss, 'Loss', i)\n",
        "  Loss.backward()  \n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  if i % 10 == 0:\n",
        "    [w, b] = MODEL.parameters()\n",
        "\n",
        "    print(w, 'w', b, 'b', i, 'i', Loss)\n",
        "\n",
        "predictions = MODEL.forward(X)\n",
        "print(predictions, 'predictions2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2498],\n",
            "        [0.9619]], grad_fn=<AddmmBackward>) predictions1\n",
            "Parameter containing:\n",
            "tensor([[-0.4695, -0.2426,  0.5339]], requires_grad=True) w1 Parameter containing:\n",
            "tensor([-0.2191], requires_grad=True) b1\n",
            "Parameter containing:\n",
            "tensor([[-0.4741, -0.2594,  0.4855]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2312], requires_grad=True) b 0 i tensor(2.2060, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.4555, -0.2556,  0.4260]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2461], requires_grad=True) b 3 i tensor(1.8839, grad_fn=<MseLossBackward>)\n",
            "tensor([[-2.4087],\n",
            "        [-1.1275]], grad_fn=<AddmmBackward>) predictions1\n",
            "next\n",
            "Parameter containing:\n",
            "tensor([[-0.4555, -0.2556,  0.4260]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2461], requires_grad=True) MODEL.parameters()\n",
            "Parameter containing:\n",
            "tensor([[-0.3178, -0.1533,  0.4588]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2107], requires_grad=True) b 0 i tensor(5.8177, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[0.0190, 0.1164, 0.5716]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.1437], requires_grad=True) b 10 i tensor(0.0637, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[0.0226, 0.1403, 0.6077]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.1639], requires_grad=True) b 20 i tensor(0.0341, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[0.0152, 0.1518, 0.6342]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.1827], requires_grad=True) b 30 i tensor(0.0212, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[0.0090, 0.1605, 0.6549]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.1977], requires_grad=True) b 40 i tensor(0.0132, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[0.0040, 0.1673, 0.6713]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2095], requires_grad=True) b 50 i tensor(0.0082, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[1.5007e-04, 1.7274e-01, 6.8421e-01]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2188], requires_grad=True) b 60 i tensor(0.0051, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0029,  0.1770,  0.6944]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2261], requires_grad=True) b 70 i tensor(0.0032, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0053,  0.1804,  0.7024]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2319], requires_grad=True) b 80 i tensor(0.0020, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0072,  0.1830,  0.7087]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2364], requires_grad=True) b 90 i tensor(0.0012, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0088,  0.1851,  0.7137]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2400], requires_grad=True) b 100 i tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0099,  0.1867,  0.7176]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2429], requires_grad=True) b 110 i tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0109,  0.1880,  0.7207]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2451], requires_grad=True) b 120 i tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0116,  0.1890,  0.7232]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2469], requires_grad=True) b 130 i tensor(0.0002, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0122,  0.1899,  0.7251]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2482], requires_grad=True) b 140 i tensor(0.0001, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0126,  0.1905,  0.7266]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2493], requires_grad=True) b 150 i tensor(7.0175e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0130,  0.1910,  0.7278]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2502], requires_grad=True) b 160 i tensor(4.3607e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0133,  0.1914,  0.7287]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2509], requires_grad=True) b 170 i tensor(2.7097e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0135,  0.1917,  0.7295]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2514], requires_grad=True) b 180 i tensor(1.6838e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0137,  0.1919,  0.7301]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2518], requires_grad=True) b 190 i tensor(1.0463e-05, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0138,  0.1921,  0.7305]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2522], requires_grad=True) b 200 i tensor(6.5013e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0139,  0.1923,  0.7309]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2524], requires_grad=True) b 210 i tensor(4.0398e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0140,  0.1924,  0.7312]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2526], requires_grad=True) b 220 i tensor(2.5105e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0141,  0.1925,  0.7314]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2528], requires_grad=True) b 230 i tensor(1.5600e-06, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0142,  0.1926,  0.7316]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2529], requires_grad=True) b 240 i tensor(9.6930e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0142,  0.1926,  0.7317]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2530], requires_grad=True) b 250 i tensor(6.0222e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0142,  0.1927,  0.7318]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2531], requires_grad=True) b 260 i tensor(3.7416e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1927,  0.7319]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2532], requires_grad=True) b 270 i tensor(2.3254e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1927,  0.7320]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2532], requires_grad=True) b 280 i tensor(1.4454e-07, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7320]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2533], requires_grad=True) b 290 i tensor(8.9782e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7321]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2533], requires_grad=True) b 300 i tensor(5.5794e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7321]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2533], requires_grad=True) b 310 i tensor(3.4705e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7321]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2533], requires_grad=True) b 320 i tensor(2.1574e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2533], requires_grad=True) b 330 i tensor(1.3409e-08, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 340 i tensor(8.3349e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 350 i tensor(5.1709e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 360 i tensor(3.2032e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 370 i tensor(1.9937e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 380 i tensor(1.2390e-09, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 390 i tensor(7.7047e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1928,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 400 i tensor(4.8125e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 410 i tensor(2.9887e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 420 i tensor(1.8670e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0143,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 430 i tensor(1.1669e-10, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0144,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 440 i tensor(7.2399e-11, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0144,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 450 i tensor(4.5640e-11, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0144,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 460 i tensor(2.8024e-11, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0144,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 470 i tensor(1.8339e-11, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0144,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 480 i tensor(1.1717e-11, grad_fn=<MseLossBackward>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0144,  0.1929,  0.7322]], requires_grad=True) w Parameter containing:\n",
            "tensor([-0.2534], requires_grad=True) b 490 i tensor(7.0060e-12, grad_fn=<MseLossBackward>)\n",
            "tensor([[ 1.0000],\n",
            "        [-1.0000]], grad_fn=<AddmmBackward>) predictions2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPNYNNm89k0D",
        "outputId": "f2abeb97-774f-4aa8-ae10-0666b5ec4c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import  numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_np, y_np = datasets.make_regression(n_samples = 100, n_features = 1, noise = 20, random_state = 1)\n",
        "\n",
        "X = torch.from_numpy(x_np.astype(np.float32))\n",
        "y = torch.from_numpy(y_np.astype(np.float32))\n",
        "y = y.view(-1, 1)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "class LinearReg(nn.Module):\n",
        "  def __init__(self, input_dim, out_dim):\n",
        "    super(LinearReg, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.out_dim = out_dim\n",
        "    self.lin1 = nn.Linear(self.input_dim, self.out_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.lin1(X)\n",
        "\n",
        "model = LinearReg(input_size, output_size)\n",
        "lr = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "for i in range(500):\n",
        "  y_pred = model(X)\n",
        "  #print(y.shape)\n",
        "  loss = criterion(y, y_pred)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(f'epoch: {i + 1}, loss: {loss.item():.4f}')\n",
        "\n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(x_np, y_np, 'ro')\n",
        "plt.plot(x_np, predicted, 'b')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 5661.8892\n",
            "epoch: 11, loss: 4191.8594\n",
            "epoch: 21, loss: 3130.1287\n",
            "epoch: 31, loss: 2362.3770\n",
            "epoch: 41, loss: 1806.5892\n",
            "epoch: 51, loss: 1403.8298\n",
            "epoch: 61, loss: 1111.6871\n",
            "epoch: 71, loss: 899.5934\n",
            "epoch: 81, loss: 745.4899\n",
            "epoch: 91, loss: 633.4370\n",
            "epoch: 101, loss: 551.9041\n",
            "epoch: 111, loss: 492.5417\n",
            "epoch: 121, loss: 449.2953\n",
            "epoch: 131, loss: 417.7733\n",
            "epoch: 141, loss: 394.7861\n",
            "epoch: 151, loss: 378.0152\n",
            "epoch: 161, loss: 365.7748\n",
            "epoch: 171, loss: 356.8378\n",
            "epoch: 181, loss: 350.3103\n",
            "epoch: 191, loss: 345.5413\n",
            "epoch: 201, loss: 342.0561\n",
            "epoch: 211, loss: 339.5084\n",
            "epoch: 221, loss: 337.6456\n",
            "epoch: 231, loss: 336.2833\n",
            "epoch: 241, loss: 335.2870\n",
            "epoch: 251, loss: 334.5579\n",
            "epoch: 261, loss: 334.0246\n",
            "epoch: 271, loss: 333.6342\n",
            "epoch: 281, loss: 333.3485\n",
            "epoch: 291, loss: 333.1394\n",
            "epoch: 301, loss: 332.9863\n",
            "epoch: 311, loss: 332.8742\n",
            "epoch: 321, loss: 332.7921\n",
            "epoch: 331, loss: 332.7320\n",
            "epoch: 341, loss: 332.6880\n",
            "epoch: 351, loss: 332.6558\n",
            "epoch: 361, loss: 332.6322\n",
            "epoch: 371, loss: 332.6149\n",
            "epoch: 381, loss: 332.6023\n",
            "epoch: 391, loss: 332.5930\n",
            "epoch: 401, loss: 332.5861\n",
            "epoch: 411, loss: 332.5812\n",
            "epoch: 421, loss: 332.5775\n",
            "epoch: 431, loss: 332.5749\n",
            "epoch: 441, loss: 332.5728\n",
            "epoch: 451, loss: 332.5716\n",
            "epoch: 461, loss: 332.5704\n",
            "epoch: 471, loss: 332.5697\n",
            "epoch: 481, loss: 332.5691\n",
            "epoch: 491, loss: 332.5688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IctlQLXEYFCe"
      },
      "source": [
        "# Logistics Regression\n",
        "# Tutorial 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvUx5zdEInfE",
        "outputId": "f8733165-3f90-4eeb-d009-132e67730e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "bc = datasets.load_breast_cancer()\n",
        "\n",
        "X, y = bc.data, bc.target\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "\n",
        "y_train = y_train.view(-1, 1)\n",
        "y_test = y_test.view(-1, 1)\n",
        "lr = 0.01\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.lin1 = nn.Linear(self.input_size, 1)\n",
        "\n",
        "  def forward(self, X):\n",
        "    y_pred = torch.sigmoid(self.lin1(X))\n",
        "    return y_pred\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "for i in range(500):\n",
        "  y_pred = model(X_train)\n",
        "  loss = criterion(y_train, y_pred)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "with torch.no_grad():\n",
        "  pred = model(X_test)\n",
        "  pred_class = pred.round()\n",
        "  acc = (torch.sum(pred_class == y_test)/float(len(y_test))) * 100\n",
        "  print(f'Accuracy:{acc:.4f}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:95.6140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlqChomMYTYb"
      },
      "source": [
        "# Datasets and Dataloader\n",
        "# Tutorial 9\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6HL-8lDXPC1"
      },
      "source": [
        "1 epoch = one complete forward and backward pass for the whole training data\\\n",
        "batch size = no of training examples for one complete forward and backward pass\\\n",
        "number of iterations = number of passe per epoch\\\n",
        "e.g 100 training examples, 20 batch size, so no of iterations = 100/20 = 5 per each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3jGxM5oNHW2",
        "outputId": "f181ea8c-4e71-480e-bb12-7db8af859d30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/wine.csv'\n",
        "batch_size = 4\n",
        "\n",
        "class Winedata():\n",
        "  def __init__(self, data_path):\n",
        "    data = np.loadtxt(data_path, delimiter = ',', dtype = np.float32, skiprows = 1)\n",
        "    self.x = torch.from_numpy(data[:, 1:])\n",
        "    self.y = torch.from_numpy(data[:, 0])\n",
        "    self.n_samples = data.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "dataset = Winedata(data_path)\n",
        "features, labels = dataset[0]\n",
        "dataloader = DataLoader(dataset = dataset, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "dataiter = iter(dataloader)\n",
        "data = dataiter.next()\n",
        "features, labels = data\n",
        "n_samples = len(dataset)\n",
        "n_epochs = 8\n",
        "n_iter = math.ceil(n_samples/batch_size)\n",
        "for e in range(n_epochs):\n",
        "  for i, (inputs, labels) in enumerate(dataloader):\n",
        "    if (i + 1) % 5 == 0:\n",
        "      print(f'epoch {e + 1}/{n_epochs}, steps {i + 1}/{n_iter}, inputs {inputs.shape}') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 2/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 3/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 4/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 5/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 6/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 7/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 8/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 45/45, inputs torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOyK_lJdYV0S"
      },
      "source": [
        "# Dataset Transform\n",
        "# Tutorial 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu00NSnz3_rR",
        "outputId": "a2c27fc0-5400-4cc3-dfb3-d7177adfc200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/wine.csv'\n",
        "batch_size = 4\n",
        "\n",
        "class Winedata():\n",
        "  def __init__(self, data_path, transform = None):\n",
        "    data = np.loadtxt(data_path, delimiter = ',', dtype = np.float32, skiprows = 1)\n",
        "    self.x = np.array(data[:, 1:])\n",
        "    self.y = np.array(data[:, [0]])\n",
        "    self.n_samples = data.shape[0]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample =  self.x[index], self.y[index]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "class ToTensor():\n",
        "  def __call__(self, sample):\n",
        "    inputs, labels = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(labels)\n",
        "\n",
        "class MullTensor():\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    inputs, labels = sample\n",
        "    inputs *= self.factor\n",
        "    return inputs, labels\n",
        "\n",
        "dataset = Winedata(data_path, transform = ToTensor())\n",
        "firstdata = dataset[0]\n",
        "inputs, labels = firstdata\n",
        "print(inputs, labels)\n",
        "\n",
        "dataset = Winedata(data_path, transform = MullTensor(2))\n",
        "firstdata = dataset[0]\n",
        "inputs, labels = firstdata\n",
        "print(inputs, labels)\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MullTensor(3)])\n",
        "dataset = Winedata(data_path, transform = composed)\n",
        "firstdata = dataset[0]\n",
        "inputs, labels = firstdata\n",
        "print(type(inputs), type(labels))\n",
        "\n",
        "\n",
        "features, labels = dataset[0]\n",
        "dataloader = DataLoader(dataset = dataset, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "dataiter = iter(dataloader)\n",
        "data = dataiter.next()\n",
        "features, labels = data\n",
        "n_samples = len(dataset)\n",
        "n_epochs = 8\n",
        "n_iter = math.ceil(n_samples/batch_size)\n",
        "for e in range(n_epochs):\n",
        "  for i, (inputs, labels) in enumerate(dataloader):\n",
        "    if (i + 1) % 5 == 0:\n",
        "      print(f'epoch {e + 1}/{n_epochs}, steps {i + 1}/{n_iter}, inputs {inputs.shape}') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]) tensor([1.])\n",
            "[2.846e+01 3.420e+00 4.860e+00 3.120e+01 2.540e+02 5.600e+00 6.120e+00\n",
            " 5.600e-01 4.580e+00 1.128e+01 2.080e+00 7.840e+00 2.130e+03] [1.]\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "epoch 1/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 1/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 2/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 2/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 3/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 3/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 4/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 4/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 5/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 5/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 6/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 6/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 7/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 7/8, steps 45/45, inputs torch.Size([2, 13])\n",
            "epoch 8/8, steps 5/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 10/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 15/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 20/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 25/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 30/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 35/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 40/45, inputs torch.Size([4, 13])\n",
            "epoch 8/8, steps 45/45, inputs torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLKJzCkcgCVm"
      },
      "source": [
        "# Lecture 13\n",
        "# Feed Forward NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iONTDq1CY_9",
        "outputId": "86390540-e794-4ad0-cb7b-8492abfdeb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    super(NN, self).__init__()\n",
        "    self.lin1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.lin2 = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.lin2(out)\n",
        "\n",
        "    y_pred = torch.sigmoid(out)\n",
        "    return y_pred\n",
        "\n",
        "model = NN(28*28, 1, 5)\n",
        "criterion = nn.MSELoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "X = torch.randn((6, 28*28), dtype = torch.float32)\n",
        "y = torch.tensor([1, 0, 0, 1, 1, 0])\n",
        "y_pred = model.forward(X)\n",
        "print(y_pred, 'y_pred1')\n",
        "\n",
        "for i in range(501):\n",
        "  y_pred = model.forward(X)\n",
        "  loss = criterion(y, y_pred)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  #print(loss, 'loss', i)\n",
        "  if i % 100 == 0:\n",
        "    j = 0\n",
        "    for p in model.parameters():\n",
        "      print(p.name, 'a', p.data,'j', j, 'shape', p.data.shape)\n",
        "      j = j + 1\n",
        "y_pred = model.forward(X)\n",
        "print(y_pred, 'y_pred2')\n",
        "print('next')\n",
        "X = torch.randn((6, 28*28), dtype = torch.float32)\n",
        "\n",
        "## Parameters are reinitialized below\n",
        "j = 0\n",
        "for p in model.parameters():\n",
        "  if j == 0:\n",
        "    p.data = torch.randn(5, 784)\n",
        "  if j == 1:\n",
        "    p.data = torch.randn(5)\n",
        "  if j == 2:\n",
        "    p.data = torch.randn(1, 5)\n",
        "  if j == 3:\n",
        "    p.data = torch.randn(1)\n",
        "  j += 1\n",
        "\n",
        "## If weights are not reinitialized above, It will start updating weights from the point where it stopped \n",
        "## during last optimization.  \n",
        "X = torch.randn((6, 28*28), dtype = torch.float32)\n",
        "\n",
        "for i in range(101):\n",
        "  y_pred = model.forward(X)\n",
        "  loss = criterion(y, y_pred)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  #print(loss, 'loss', i)\n",
        "  if i % 10 == 0:\n",
        "    j = 0\n",
        "    for p in model.parameters():\n",
        "      print(p.name, 'a', p.data,'j', j, 'shape', p.data.shape)\n",
        "      j = j + 1\n",
        "y_pred = model.forward(X)\n",
        "print(y_pred, 'y_pred2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6167],\n",
            "        [0.5987],\n",
            "        [0.5593],\n",
            "        [0.5434],\n",
            "        [0.5986],\n",
            "        [0.6372]], grad_fn=<SigmoidBackward>) y_pred1\n",
            "None a tensor([[ 0.0109,  0.0270,  0.0238,  ..., -0.0089,  0.0096, -0.0269],\n",
            "        [-0.0330, -0.0210, -0.0148,  ...,  0.0310,  0.0184,  0.0304],\n",
            "        [ 0.0290,  0.0054, -0.0219,  ...,  0.0256,  0.0272, -0.0304],\n",
            "        [ 0.0288, -0.0300, -0.0070,  ..., -0.0252,  0.0039,  0.0105],\n",
            "        [-0.0345,  0.0197,  0.0087,  ..., -0.0041,  0.0148,  0.0090]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([-0.0149, -0.0329,  0.0054,  0.0267,  0.0272]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.1455,  0.2431, -0.1270,  0.1779,  0.2200]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([0.3807]) j 3 shape torch.Size([1])\n",
            "None a tensor([[ 0.0109,  0.0270,  0.0235,  ..., -0.0085,  0.0096, -0.0264],\n",
            "        [-0.0330, -0.0211, -0.0146,  ...,  0.0307,  0.0186,  0.0300],\n",
            "        [ 0.0290,  0.0055, -0.0221,  ...,  0.0260,  0.0272, -0.0300],\n",
            "        [ 0.0288, -0.0300, -0.0068,  ..., -0.0253,  0.0039,  0.0103],\n",
            "        [-0.0342,  0.0196,  0.0088,  ..., -0.0046,  0.0148,  0.0087]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([-0.0145, -0.0331,  0.0057,  0.0265,  0.0266]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.1470,  0.2429, -0.1283,  0.1769,  0.2191]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([0.3766]) j 3 shape torch.Size([1])\n",
            "None a tensor([[ 0.0109,  0.0271,  0.0233,  ..., -0.0081,  0.0096, -0.0260],\n",
            "        [-0.0330, -0.0211, -0.0144,  ...,  0.0304,  0.0187,  0.0297],\n",
            "        [ 0.0290,  0.0055, -0.0223,  ...,  0.0263,  0.0272, -0.0296],\n",
            "        [ 0.0288, -0.0300, -0.0067,  ..., -0.0255,  0.0039,  0.0102],\n",
            "        [-0.0339,  0.0197,  0.0089,  ..., -0.0049,  0.0146,  0.0085]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([-0.0142, -0.0332,  0.0060,  0.0263,  0.0261]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.1484,  0.2427, -0.1296,  0.1762,  0.2185]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([0.3728]) j 3 shape torch.Size([1])\n",
            "None a tensor([[ 0.0109,  0.0271,  0.0231,  ..., -0.0078,  0.0096, -0.0256],\n",
            "        [-0.0329, -0.0212, -0.0142,  ...,  0.0301,  0.0188,  0.0294],\n",
            "        [ 0.0290,  0.0056, -0.0225,  ...,  0.0266,  0.0272, -0.0293],\n",
            "        [ 0.0288, -0.0301, -0.0065,  ..., -0.0256,  0.0039,  0.0100],\n",
            "        [-0.0337,  0.0198,  0.0090,  ..., -0.0053,  0.0145,  0.0083]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([-0.0139, -0.0334,  0.0063,  0.0262,  0.0257]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.1498,  0.2427, -0.1308,  0.1756,  0.2182]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([0.3695]) j 3 shape torch.Size([1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([6, 1])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "None a tensor([[ 0.0109,  0.0272,  0.0230,  ..., -0.0076,  0.0096, -0.0253],\n",
            "        [-0.0329, -0.0212, -0.0141,  ...,  0.0300,  0.0189,  0.0293],\n",
            "        [ 0.0291,  0.0056, -0.0226,  ...,  0.0268,  0.0272, -0.0290],\n",
            "        [ 0.0288, -0.0301, -0.0064,  ..., -0.0257,  0.0039,  0.0099],\n",
            "        [-0.0337,  0.0198,  0.0092,  ..., -0.0054,  0.0145,  0.0081]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([-0.0137, -0.0334,  0.0065,  0.0261,  0.0255]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.1510,  0.2427, -0.1320,  0.1752,  0.2181]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([0.3664]) j 3 shape torch.Size([1])\n",
            "None a tensor([[ 0.0109,  0.0272,  0.0229,  ..., -0.0073,  0.0096, -0.0250],\n",
            "        [-0.0329, -0.0212, -0.0141,  ...,  0.0300,  0.0189,  0.0293],\n",
            "        [ 0.0291,  0.0056, -0.0227,  ...,  0.0271,  0.0272, -0.0287],\n",
            "        [ 0.0288, -0.0301, -0.0063,  ..., -0.0257,  0.0039,  0.0099],\n",
            "        [-0.0336,  0.0198,  0.0093,  ..., -0.0055,  0.0145,  0.0080]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([-0.0135, -0.0334,  0.0067,  0.0260,  0.0254]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.1522,  0.2427, -0.1330,  0.1748,  0.2180]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([0.3635]) j 3 shape torch.Size([1])\n",
            "tensor([[0.5899],\n",
            "        [0.5899],\n",
            "        [0.5303],\n",
            "        [0.5251],\n",
            "        [0.5459],\n",
            "        [0.5575]], grad_fn=<SigmoidBackward>) y_pred2\n",
            "next\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1380,  1.4910,  0.1914,  ..., -1.3844,  0.3247, -0.1930],\n",
            "        [ 0.4308, -1.2922, -0.6922,  ..., -0.1556,  1.5955, -0.0148],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9696, -0.5486, -2.1488,  ..., -0.1411,  0.7237, -0.2900]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6455,  0.3833,  0.6827, -0.6396]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1513, -0.9116,  0.0179,  2.5571]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9743]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1381,  1.4910,  0.1914,  ..., -1.3844,  0.3247, -0.1930],\n",
            "        [ 0.4308, -1.2922, -0.6922,  ..., -0.1556,  1.5955, -0.0148],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9697, -0.5485, -2.1488,  ..., -0.1410,  0.7237, -0.2901]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6455,  0.3833,  0.6827, -0.6396]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1510, -0.9125,  0.0176,  2.5569]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9743]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1381,  1.4910,  0.1914,  ..., -1.3843,  0.3247, -0.1931],\n",
            "        [ 0.4308, -1.2922, -0.6922,  ..., -0.1557,  1.5955, -0.0148],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9698, -0.5485, -2.1487,  ..., -0.1408,  0.7238, -0.2902]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6454,  0.3834,  0.6827, -0.6397]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1506, -0.9136,  0.0172,  2.5567]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9742]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1382,  1.4910,  0.1914,  ..., -1.3842,  0.3247, -0.1931],\n",
            "        [ 0.4307, -1.2922, -0.6923,  ..., -0.1558,  1.5955, -0.0147],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9700, -0.5485, -2.1487,  ..., -0.1406,  0.7238, -0.2903]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6454,  0.3834,  0.6827, -0.6399]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1500, -0.9151,  0.0166,  2.5564]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9742]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1383,  1.4911,  0.1915,  ..., -1.3840,  0.3248, -0.1932],\n",
            "        [ 0.4306, -1.2923, -0.6923,  ..., -0.1559,  1.5954, -0.0147],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9702, -0.5484, -2.1486,  ..., -0.1402,  0.7239, -0.2905]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6453,  0.3835,  0.6827, -0.6401]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1491, -0.9174,  0.0158,  2.5559]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9741]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1384,  1.4911,  0.1915,  ..., -1.3837,  0.3249, -0.1933],\n",
            "        [ 0.4305, -1.2923, -0.6923,  ..., -0.1561,  1.5954, -0.0145],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9705, -0.5483, -2.1485,  ..., -0.1396,  0.7241, -0.2908]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6452,  0.3836,  0.6827, -0.6404]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1477, -0.9213,  0.0144,  2.5552]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9740]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1386,  1.4912,  0.1916,  ..., -1.3835,  0.3249, -0.1935],\n",
            "        [ 0.4304, -1.2924, -0.6924,  ..., -0.1564,  1.5953, -0.0144],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9709, -0.5481, -2.1483,  ..., -0.1390,  0.7243, -0.2911]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6450,  0.3837,  0.6827, -0.6407]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1461, -0.9254,  0.0130,  2.5545]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9738]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1387,  1.4912,  0.1916,  ..., -1.3834,  0.3250, -0.1935],\n",
            "        [ 0.4303, -1.2924, -0.6924,  ..., -0.1564,  1.5953, -0.0144],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9710, -0.5481, -2.1483,  ..., -0.1388,  0.7243, -0.2912]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6450,  0.3837,  0.6827, -0.6408]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1457, -0.9266,  0.0125,  2.5542]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9738]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1387,  1.4912,  0.1916,  ..., -1.3834,  0.3250, -0.1935],\n",
            "        [ 0.4303, -1.2924, -0.6924,  ..., -0.1564,  1.5953, -0.0144],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9711, -0.5481, -2.1483,  ..., -0.1387,  0.7243, -0.2912]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6449,  0.3838,  0.6827, -0.6408]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1456, -0.9268,  0.0125,  2.5542]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9738]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1387,  1.4912,  0.1916,  ..., -1.3834,  0.3250, -0.1935],\n",
            "        [ 0.4303, -1.2924, -0.6924,  ..., -0.1564,  1.5953, -0.0144],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9711, -0.5481, -2.1483,  ..., -0.1387,  0.7244, -0.2912]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6449,  0.3838,  0.6827, -0.6408]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1456, -0.9268,  0.0125,  2.5542]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9738]) j 3 shape torch.Size([1])\n",
            "None a tensor([[-1.0861,  0.2187,  0.1215,  ...,  0.7137,  0.0089,  1.7940],\n",
            "        [ 0.1387,  1.4912,  0.1916,  ..., -1.3834,  0.3250, -0.1935],\n",
            "        [ 0.4303, -1.2924, -0.6924,  ..., -0.1564,  1.5953, -0.0144],\n",
            "        [ 0.3271, -0.1493, -1.8401,  ...,  0.1169, -0.4513,  1.0473],\n",
            "        [ 1.9711, -0.5481, -2.1483,  ..., -0.1387,  0.7244, -0.2912]]) j 0 shape torch.Size([5, 784])\n",
            "None a tensor([ 0.8334,  1.6449,  0.3838,  0.6827, -0.6408]) j 1 shape torch.Size([5])\n",
            "None a tensor([[-0.6253,  1.1456, -0.9268,  0.0125,  2.5542]]) j 2 shape torch.Size([1, 5])\n",
            "None a tensor([1.9738]) j 3 shape torch.Size([1])\n",
            "tensor([[1.0000e+00],\n",
            "        [5.0008e-01],\n",
            "        [1.0000e+00],\n",
            "        [1.0000e+00],\n",
            "        [1.1345e-05],\n",
            "        [1.0000e+00]], grad_fn=<SigmoidBackward>) y_pred2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvafGgorOLY3",
        "outputId": "853c802a-9e93-4bc9-a3ce-0c5f170c4612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    super(NN, self).__init__()\n",
        "    self.lin1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.lin2 = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.lin2(out)\n",
        "\n",
        "    y_pred = torch.sigmoid(out)\n",
        "    return y_pred\n",
        "\n",
        "model = NN(28*28, 1, 5)\n",
        "criterion = nn.MSELoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "X = torch.randn((6, 28*28), dtype = torch.float32)\n",
        "y = torch.tensor([1, 0, 0, 1, 1, 0])\n",
        "y_pred = model.forward(X)\n",
        "print(y_pred, 'y_pred1')\n",
        "\n",
        "for i in range(11):\n",
        "  y_pred = model.forward(X)\n",
        "  loss = criterion(y, y_pred)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  print(loss, 'loss', i)\n",
        "  '''\n",
        "  if i % 100 == 0:\n",
        "    j = 0\n",
        "    for p in model.parameters():\n",
        "      print(p.name, 'a', p.data,'j', j, 'shape', p.data.shape)\n",
        "      j = j + 1\n",
        "  '''\n",
        "y_pred = model.forward(X)\n",
        "print(y_pred, 'y_pred2')\n",
        "print('next')\n",
        "X = torch.randn((6, 28*28), dtype = torch.float32)\n",
        "\n",
        "## Parameters are reinitialized below\n",
        "j = 0\n",
        "for p in model.parameters():\n",
        "  if j == 0:\n",
        "    p.data = torch.randn(5, 784)\n",
        "  if j == 1:\n",
        "    p.data = torch.randn(5)\n",
        "  if j == 2:\n",
        "    p.data = torch.randn(1, 5)\n",
        "  if j == 3:\n",
        "    p.data = torch.randn(1)\n",
        "  j += 1\n",
        "\n",
        "## If weights are not reinitialized above, It will start updating weights from the point where it stopped \n",
        "## during last optimization.  \n",
        "X = torch.randn((6, 28*28), dtype = torch.float32)\n",
        "for i in range(11):\n",
        "  y_pred = model.forward(X)\n",
        "  loss = criterion(y, y_pred)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  print(loss, 'loss', i)\n",
        "  '''\n",
        "  if i % 10 == 0:\n",
        "    j = 0\n",
        "    for p in model.parameters():\n",
        "      print(p.name, 'a', p.data,'j', j, 'shape', p.data.shape)\n",
        "      j = j + 1\n",
        "  '''\n",
        "y_pred = model.forward(X)\n",
        "print(y_pred, 'y_pred2')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3865],\n",
            "        [0.4351],\n",
            "        [0.4517],\n",
            "        [0.4260],\n",
            "        [0.3941],\n",
            "        [0.3327]], grad_fn=<SigmoidBackward>) y_pred1\n",
            "tensor(0.2607, grad_fn=<MeanBackward0>) loss 0\n",
            "tensor(0.2606, grad_fn=<MeanBackward0>) loss 1\n",
            "tensor(0.2606, grad_fn=<MeanBackward0>) loss 2\n",
            "tensor(0.2605, grad_fn=<MeanBackward0>) loss 3\n",
            "tensor(0.2605, grad_fn=<MeanBackward0>) loss 4\n",
            "tensor(0.2605, grad_fn=<MeanBackward0>) loss 5\n",
            "tensor(0.2604, grad_fn=<MeanBackward0>) loss 6\n",
            "tensor(0.2604, grad_fn=<MeanBackward0>) loss 7\n",
            "tensor(0.2603, grad_fn=<MeanBackward0>) loss 8\n",
            "tensor(0.2603, grad_fn=<MeanBackward0>) loss 9\n",
            "tensor(0.2602, grad_fn=<MeanBackward0>) loss 10\n",
            "tensor([[0.3900],\n",
            "        [0.4365],\n",
            "        [0.4518],\n",
            "        [0.4272],\n",
            "        [0.3957],\n",
            "        [0.3375]], grad_fn=<SigmoidBackward>) y_pred2\n",
            "next\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 0\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 1\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 2\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 3\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 4\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 5\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 6\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 7\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 8\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 9\n",
            "tensor(0.5000, grad_fn=<MeanBackward0>) loss 10\n",
            "tensor([[1.0000],\n",
            "        [1.0000],\n",
            "        [1.0000],\n",
            "        [1.0000],\n",
            "        [0.9999],\n",
            "        [1.0000]], grad_fn=<SigmoidBackward>) y_pred2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([6, 1])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPomn3J6vAgK"
      },
      "source": [
        "# Convolutional NN\n",
        "# Lecture 14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Z-1gHIj4FV",
        "outputId": "f09052c1-7a57-4988-f08d-516a3a8c23d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "lr = 1e-3\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su5p_mdsTLaV",
        "outputId": "635b7421-71e5-490c-8a7d-dab97078693c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def image_show(img):\n",
        "  img = img / 2 + 0.5\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = data_iter.next()\n",
        "image_show(torchvision.utils.make_grid(images))\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.lin1 = nn.Linear(5 * 16 * 5, 120)\n",
        "    self.lin2 = nn.Linear(120, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16 * 5 * 5)\n",
        "    x = self.lin1(x)\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "Loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "total_steps = len(train_loader)\n",
        "\n",
        "model.train()\n",
        "for e in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    optimizer.zero_grad() \n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    out = model(images)\n",
        "    loss = Loss(out, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 2000 == 0:\n",
        "      print (f'Epoch [{e+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "#------------------- One way to save model --------------------------------------------#\n",
        "path = '/content/drive/My Drive/ColabNotebooks/results/state_dict_model.pt'\n",
        "#os.path.join(path, model.state_dict())\n",
        "torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_correct_class = [0 for i in range(10)]\n",
        "  n_samples_class = [0 for i in range(10)]\n",
        "\n",
        "  for j, (images, labels) in enumerate(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    out = model(images)\n",
        "    _,preds = torch.max(out, 1)\n",
        "    n_samples += 1\n",
        "    n_correct +=(preds == labels).sum().item()\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = preds[i]\n",
        "      if pred == label:\n",
        "        n_correct_class[labels[i]] += 1\n",
        "      n_samples_class[labels[i]] += 1\n",
        "\n",
        "  acc = n_correct * 100./n_samples\n",
        "  print('model accuracy:', acc)\n",
        "\n",
        "  for i in range(10):\n",
        "    acc = n_correct_class[i] * 100. / n_samples_class[i]\n",
        "    print('Acccuracy for label', i, 'is:', acc)\n",
        "\n",
        "#-------------------------------------- Load model ------------------------------------#\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [2000/3125], Loss: 2.2719\n",
            "Epoch [2/10], Step [2000/3125], Loss: 2.0216\n",
            "Epoch [3/10], Step [2000/3125], Loss: 1.8526\n",
            "Epoch [4/10], Step [2000/3125], Loss: 1.7307\n",
            "Epoch [5/10], Step [2000/3125], Loss: 1.6875\n",
            "Epoch [6/10], Step [2000/3125], Loss: 1.4968\n",
            "Epoch [7/10], Step [2000/3125], Loss: 1.5388\n",
            "Epoch [8/10], Step [2000/3125], Loss: 1.1842\n",
            "Epoch [9/10], Step [2000/3125], Loss: 1.6003\n",
            "Epoch [10/10], Step [2000/3125], Loss: 1.2293\n",
            "Finished Training\n",
            "model accuracy: 762.56\n",
            "Acccuracy for label 0 is: 55.2\n",
            "Acccuracy for label 1 is: 60.6\n",
            "Acccuracy for label 2 is: 31.7\n",
            "Acccuracy for label 3 is: 27.1\n",
            "Acccuracy for label 4 is: 34.5\n",
            "Acccuracy for label 5 is: 40.7\n",
            "Acccuracy for label 6 is: 62.3\n",
            "Acccuracy for label 7 is: 54.0\n",
            "Acccuracy for label 8 is: 52.2\n",
            "Acccuracy for label 9 is: 58.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNet(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (lin1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (lin2): Linear(in_features=120, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw98NEvRNKi3"
      },
      "source": [
        "I am not updating the below model with trained weights, so accuracy will be less. But at the end we will get accuracy like above model, means ConvNet() is not loosing its trained weight unless reinitialized the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlL4IYbOYAfb",
        "outputId": "5245467d-7489-4a54-d835-d91f7f1ab3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "Untrained_Model = ConvNet() \n",
        "Untrained_Model.eval()\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_correct_class = [0 for i in range(10)]\n",
        "  n_samples_class = [0 for i in range(10)]\n",
        "\n",
        "  for j, (images, labels) in enumerate(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    out = model(images)\n",
        "    _,preds = torch.max(out, 1)\n",
        "    n_samples += 1\n",
        "    n_correct +=(preds == labels).sum().item()\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = preds[i]\n",
        "      if pred == label:\n",
        "        n_correct_class[labels[i]] += 1\n",
        "      n_samples_class[labels[i]] += 1\n",
        "\n",
        "  acc = n_correct * 100./n_samples\n",
        "  print('model accuracy:', acc)\n",
        "\n",
        "  for i in range(10):\n",
        "    acc = n_correct_class[i] * 100. / n_samples_class[i]\n",
        "    print('Acccuracy for label', i, 'is:', acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy: 762.56\n",
            "Acccuracy for label 0 is: 55.2\n",
            "Acccuracy for label 1 is: 60.6\n",
            "Acccuracy for label 2 is: 31.7\n",
            "Acccuracy for label 3 is: 27.1\n",
            "Acccuracy for label 4 is: 34.5\n",
            "Acccuracy for label 5 is: 40.7\n",
            "Acccuracy for label 6 is: 62.3\n",
            "Acccuracy for label 7 is: 54.0\n",
            "Acccuracy for label 8 is: 52.2\n",
            "Acccuracy for label 9 is: 58.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEoYsSu8NaZL"
      },
      "source": [
        "I will update the below model with trained weight, so it should give accuracy like the first model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh7AQAeQNqqJ",
        "outputId": "74b4e8b9-ce01-4ba5-d56c-63a7c12bf0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "Model = ConvNet() # This is not trained model\n",
        "Model.load_state_dict(torch.load(path)) # The model is loaded with the trained weight\n",
        "Model.eval()\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_correct_class = [0 for i in range(10)]\n",
        "  n_samples_class = [0 for i in range(10)]\n",
        "\n",
        "  for j, (images, labels) in enumerate(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    out = model(images)\n",
        "    _,preds = torch.max(out, 1)\n",
        "    n_samples += 1\n",
        "    n_correct +=(preds == labels).sum().item()\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = preds[i]\n",
        "      if pred == label:\n",
        "        n_correct_class[labels[i]] += 1\n",
        "      n_samples_class[labels[i]] += 1\n",
        "\n",
        "  acc = n_correct * 100./n_samples\n",
        "  print('model accuracy:', acc)\n",
        "\n",
        "  for i in range(10):\n",
        "    acc = n_correct_class[i] * 100. / n_samples_class[i]\n",
        "    print('Acccuracy for label', i, 'is:', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy: 762.56\n",
            "Acccuracy for label 0 is: 55.2\n",
            "Acccuracy for label 1 is: 60.6\n",
            "Acccuracy for label 2 is: 31.7\n",
            "Acccuracy for label 3 is: 27.1\n",
            "Acccuracy for label 4 is: 34.5\n",
            "Acccuracy for label 5 is: 40.7\n",
            "Acccuracy for label 6 is: 62.3\n",
            "Acccuracy for label 7 is: 54.0\n",
            "Acccuracy for label 8 is: 52.2\n",
            "Acccuracy for label 9 is: 58.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLv9-wwvXkvN"
      },
      "source": [
        "# Transfer Learning\n",
        "# Lecture 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDPlVGxLXn-T",
        "outputId": "379abbde-34ed-4413-9d70-3a91e5fd529b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import copy\n",
        "import time\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25 ,0.25])\n",
        "\n",
        "transform = {\n",
        "    'train':transforms.Compose([\n",
        "                                transforms.RandomResizedCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)                         \n",
        "    ]),\n",
        "    'val':transforms.Compose([\n",
        "                              transforms.Resize(256),\n",
        "                              transforms.CenterCrop(224),\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize(mean, std)\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/My Drive/ColabNotebooks/results/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), \n",
        "                                          transform[x]) for x in ['train', 'val']}\n",
        "dataloaders = {x:torch.utils.data.DataLoader(image_datasets[x], batch_size = 16, shuffle = True)\n",
        "               for x in ['train', 'val']}\n",
        "data_size = {x:len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def img_show(img, title):\n",
        "  img = img.numpy().transpose((1, 2, 0))\n",
        "  inp = std * img + mean\n",
        "  inp = np.clip(inp, 0, 1)\n",
        "  plt.imshow(inp)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "images, classes = (iter(dataloaders['train'])).next()\n",
        "out = torchvision.utils.make_grid(images)\n",
        "img_show(out, title = [class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqIAAACDCAYAAABWbnlJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9ebQlyV3f+flFRC733rfUe7VXdbe6tSIkJIwASR7JErYwIGuMxzbbILBgbBDYB3OMht1YmNUry8EeBHjQsJvFYLYBDWgDgZCQtW8t9d6111vvlpmx/OaPyFf9uuilqqtb1e2+33PeeXlv5I38ZsQvI37xWyJFVVlggQUWWGCBBRZYYIFPNcz1JrDAAgsssMACCyywwJMTC0V0gQUWWGCBBRZYYIHrgoUiusACCyywwAILLLDAdcFCEV1ggQUWWGCBBRZY4LpgoYgusMACCyywwAILLHBdsFBEF1hggQUWWGCBBRa4LnhYRVREVESmIvIDnwpCCyywwKceIvJGEfn+683jSiAibxWRf3y9eVwJROROEXnF9eaxwAJPBIjI60XkF643j/8ZICIvF5F7r+P1nykiExGJDzdeX6lF9Pmq+l195TeLyJ3XSrKv64onlH6ifM1V1Pvya+HW1/MaEfnTKzz35SLy1quo943Xwm1fXSoiT7/Cc+8UkZuvtN5r4bWvnitWcPpB6PVXUe9rroVbX88VP6xXI/tXIw9XUNcVKzNXI/tXIw+PBa6HPD5SXOn1H83x8ZFgIaMPWs8VKzhXMz5fzZj1MPXc3I/l7grP/5TL4+N9vn6kWMjjo4f911fVW1V1CfiTh/vdwjW/wAILLLDAAgsssMB1wTUroiLy7SJym4iMReQjIvK/7St7jYj8qYj8exHZEpE7ROSL+rIfAF4K/ERvvv0JyfgRETkvIrsi8kERee418vtcEflzEdkWkTP9dcp95SoirxWRT/Tn/Keex7OBnwRe3PPb7s9/ZX+fYxE5JSKvuxZ+fZ2/JiJnRWRHRN4uIs/ZV/bGntPv9df8CxF5Wl/29v609/ccv0xEDonI7/b3sikifyIi19TPIvI1IvLR/vq3i8jX7yt7uYjcKyLf0vfbGRH5mr7s64CvBL615/c7/fff1rfdWEQ+LiJ/6xr5PU1E3iwiGyJyUUR+UUQO7Cu/U0ReJyIf6Nv4v4pILSIj4P8FTvT8JiJyopeZv+xl8JyI/Mdr4ddz+DERuaev8z0i8tJ9Za8XkV8VkZ/r2+TDIvLZfdnPAzcBv9Pz+9ae+y/097stIu8WkaPXyhE4JCL/X8/hbSLylH0cP60v2+z77Ev3lVWSn/G7+/b6SREZ9GWPujz2eJqIvKtvz/8uIuv7+LxIRP6sv+b7ZZ+1Q0RWReS/9HJ6SkS+X0RsX/b0/r53ejn6r48CT4DP6ceMLRH5WRGp9/F5lYi8r+f6ZyLyvH1lJ0TkN0TkguSx85v2lT3pZFRE/o6IvLfnd4/ss/zIfRbFf9TL4UUR2fPifSHwncCX9fze33//Gsnj2bhv36+8Rn5rvaxf6Pv6d0Xkhn3lbxWR7xORd/TXfJOIHOqL98by7Z7jix8LeZTH+Xzdo5Y8Ro9F5H+IyPP3cXyoZ8Lsu7+NXl7X+7InnTzu4/Kd/fXv3F+nPMS43Zc/1Nj0qM7hqOpD/gEKPP0hyr8EOEFWar8MmALH+7LXAB74J4AFvgE4DUhf/lbgH++r6wuA9wAHAAGevVfXI/0DXgC8CHDAzcBHgW++7P5+t7/mTcAF4Av38f/Ty+o7A7y0P14DPuta+PX1fC2wDFTAjwLv21f2RmAD+Nz+Hn4R+JUH6x/gh8gKdNH/vXSvva+B398Bntb3ycuA2d59Ay8HAvCv++u9si9f28f/+/fV9SzgHuBE//lm4GnXyO/pwOf37XeYPKj/6L7yO4F39XK63svAa/fxv/ey+v4c+Kr+eAl40aPQx68GDvZ9+C3AWaDuy14PNH3b2b4P33kZ/1fs+/z1wO8Aw/78FwAr18jvjcAY+Bt9O/7YnuwDo77Pvqbn/9eAi8Cn9+U/Avx237bLPbcfegzl8a3AKeC5PbffAH6hLztJfl5eSR6TPr//fLgv/03gDf3vjvRy8fV92S8D39X/rgZe8ij0+53Ah4Ab+/Z5x97z0LfjeeCFfT/+o/78qufwHuB7gBJ4KnA78AVPYhl9OfAZfds8DzgH/L2+7GbyWPjTwAB4PtACz97H/xf21TUCdoFn9Z+PA8+5Rn4HgX/Q3/My8GvAb10mt7cBz+w5vhX44cv4u33nPxby+Hifr1/fc/iH5PHidcAd/fHDPRP/HHgncAP5GXoD8MtPYnl8OXlu/o99e7ys7++9azzUuP1QY9NVzeGXy80DnnMFN/OQiugDnP8+4Iv3CfYn95UN+/qOPYhg/03gVrLiaK71oXsQft8M/OZl9/eSfZ9/Ffj2ffwvV0Tv7oX6moT4Ifgd6Dmt9p/fCPzMvvJXAh97sP4hK4T//Wr67BFw/C3gn+8T9jn3H0DP00+M/FVF9Ol9+SuA4jHi9/eA9+77fCfw6n2f/y3wk/v4X66Ivh34XuDQY9iGW+TYa8iD0h/tK/t0YH4Z//2T/NcCfwY871Hk80buv8BZAiJZgfoy4E8uO/8NwL8iT0BT9g1EwIuBOx4reWTfBL6vvTrygPltwM9fdv4fkgfSo+TJYLCv7CuAt/THPwf8FHDDo8j1TvpFT//5lcBt/fH/BXzfZed/nDxhvBC4+7Ky7wB+9skqow/A70eBH+mPbyaPhTfsK38X8OX7+F8+8W+TFcfBY8TvM4Gty+T2u/d9/kbgDy7jv38cfdTl8QE4Pq7m676f9i9wDL3x5wqeiY8Cf2tf2XGyUuuejPLIfYroaN93vwr8Sx5+3H6osemq5vDL5eaB/h4N1/xX7zPfbpOtFIf2nXJ270BVZ/3h0gPVpapvBn4C+E/AeRH5KRFZuUZ+z+xdJGdFZBf4wcv43Y8j2Zr3gPx6/APyZHJX7zZ58TXysyLyw707YZc8oMODtOEV8Pt3wCeBN/Vm/m+/Fn49xy8SkXdKdq1uk+9/P78NVQ1XwlFVP0leDLye3Me/IiInrpHf0b6eU30b/gLX1sf/B9lq8bHehfOqa+HXc3yd5PCGnb4NV3noPq7lwRMXfp6sXP2KiJwWkX8rIsW1ciSvcgFQ1QmwSbaePAV44d4z3vP/SuAY2QI9BN6zr+wP+u/hMZDHy7kCd5EtJod6rl9yGdeXkCelp/TnndlX9gayZRTgW8kD9Lsku56/9jHiuifvTwG+5TKuN3Jfm5+4rOw7yco0PAllVEReKCJv6d2yO8BreYTPuapOyQus15Ll4fdE5NOukd9QRN4gInf149DbgQPSh35cDb8ej7o8Pt7n6x77x6EE3MuVPRNPAX5zX9lHyYvpozwJ5bHHVl/3HvbGn4cbtx90bHos5vBrjR18Ctn0/M+Ag6p6gOyGkiusQv/KF6o/rqovIK+4nwn8n9fCkazZfwx4hqqukAX3Wvi9W1W/mDx5/RZ5hXEt+N+BLyavLlbJKymuguPl/Maq+i2q+lTg7wL/4lriN0SkIrs+/z1wtO/j378Kfg/Uhr+kqi8hC7sC/+aR8uvxg309n9H38auvkd8nVPUryH38b4BflxxP+oggOdbuW4EvJYcsHAB2HilHVfWq+r2q+unAXwdeBXz1I+W3Dzfu47xEdtmcJk8Mb1PVA/v+llT1G8gu+jnZjbRXtqo5W/JRl8cH4koOqfE9l3vIFtH9XEeq+sN9WUu2Iu6Vrajqc3quZ1X1n6jqCbLX4z/LFe5IcZVcT/fH9wA/cBnXoar+cl92x2Vly6r6yp7rk1FGf4nsSrxRVVfJIR/X8pz/oap+PnmR8jHyXHYt+Bay2/KF/Tj0N/rvr4TjA/F7VOXxCTJfw/3HIUN2te+NQw/6TPTlX3RZea2qp56k8giwdtm4sDf+POS4zUOPTY/6HH6tFtFRT+IC5KQW8grrSnGOHOdB//vP6VcZBdls3ADp8h/JfYHAN1/BNZbJsReTfoXxDVfJ7wbpk5tEpBSRrxSRVVX1fb1/hV9/7lvlyrZRWCZPjhvkFcoPXgW/PY772/BVkoPchTyRxAfiKDkw+s4rqL8kx4VcAILk4PW/fQ38niUif7NXcBvyw/BgbahyZdtqLAMTYEdETnJ1g+E54KCIrO677qtF5HC/Gt/uv36gNnyjXNm2GstkF8kFwInI9wBXYzm4vA0/T0Q+o7e07JKVsAfi93K5um2PXikiL+nl/fvILrJ7yDHUzxSRrxKRov/7HBF5dt9GPw38iIgc6a97UkS+oD9+tOVxD68WkU8XkSHZ/f/rqhrJ1vD/VUS+QLK3oe7b4QZVPQO8CfgPIrIiObnhaSLysp7Dl8h9ySVb5LHtgbi+Xq5u26N/KiI3SE6c+C5gL+nkp4HX9mOeiMhIcgLEMtmNN5acFDDo7+W5IvI5PYcno4wuA5uq2ojI55IX8VfD7+ZesdnzonxxP0m35PHjwcahO+XKtiJaJo9n231f/6ur4Hehv/7+Nny05fGJMF8DvEBE/r5ka/s3k/vnnTzMM0FWBH+gV7gRkcMi8sX98ZNRHvfwvZJ1l5eSFfBfe7hxm4cYm+Qq5vArxTUpoqr6EeA/kAPnz5EDd99xFVX8GPAPJWfo/Th54Ptp8kN3F1k5+3cP8Lsb+/JTV3CN15EFZNzXfTWZh28GPgycFZGL/XdfBdwp2fXyWrKL8oFwI1fWFj/HfffyEfIDdzV4PfD/SDaffynwDOCPyIL858B/VtW3PFJ+qjoGvols+d0it+VvXwW//wJ8es/vt8hK7Q+TV2RnyRad77j8RyJyI7nPPngF1/he4LPIis7vAf/tSsmp6sfISQG39xxPAF8IfFhEJmQZ/XJVnT/Az6+0j/+Q7Pa4ldzXDfd31z4cfgj47p7f68gu8V8nD6gfBd5Gdj09EL8/u4rr/BJ58twkB/O/Gi7JwN8Gvpy8mj5LXgFX/e++jex+f2f/XPwR2TIEj7I87sPPk+Naz5ITOb6p53oP2cPwneQJ9x7ywmRvrPtq8uLqI2R5/nWyBQLgc4C/6Pv9t8lx0Lc/Clx/iawA305OVvn+nutfkhNDfqLn8klynB69Uv0qcpzhHeTn5WfIXhN4csroNwL/WkTG5ISVq/FG/Vr/f0NE/gdZHv4FWZ43ybFvf8VIIXlRdpArG5d/lJyYcrE//w+ulFzvBv8B4B19G76IR1kenyDzNeSY8i/r6/0q4O/3Fs2HeyZ+jNxOb+pl5J3kuFJ4csoj5PFxq6/3F8nx6h/ryx503H6osYkrnMOvBnvZcA9+gkhD1tB/XFX/5bVc7NGCiHw3cEFV33C9uTwQ+lXsr6rqX7/eXB4MIvIm8sD20evN5YEgIq8muw2uScAfK/QDwvvJwe/+evN5IIjIz5BXv394vbk8HB7v8rgfIvI+clLExvXm8lBYyOi1Q0ReAvzTPgzicYnHuzw+3ufr/VjI46MHEXkG8G7ywv8bVfWND3ruwymiCyywwAILLLDAAgss8Fhg8WalBa4LROQLJW+E+0l59DKpF1hggQUWWGCBJxAWFtEFPuXoA8ZvJW82fi/ZfP8VfQzTAgsssMACCyzwJMHCIrrA9cDnkjdOvl1VO+BXyAkmCyywwAILLLDAkwgLRXSB64GT3D8j997+uwUWWGCBBRZY4EmEB3srxgILXHeIyNcBXwdQlsULjh85yKV9gEXQlBCRvXNRFE1gjeRthZX7thdWSH0YiggIAiKI5NfcpqTI/t/o/Xj0H/P5KaVL39PXqf2fKwcY05+vyn2V3sdD+88pxb0r9EWKppSPFFQT94XOKDFGNIH3AY2JvSsnuHTe/kibvVsXyVytszjnsNYhA3ep7fZOEiDvoJfbTxWMCEahm7d0TQdWqJcGeQmrl26JhCKAQUitp5nMsHWJHZSoKpLAIgQf6FpPVdRYY1FVQgx0bYNKvnRZl7jSQYLklbzV3l6fas9biCli9t/DpfbNB0Yks1K91Oa554Wkisi+cy71114/S+Yt+1uS+31njEFVSDQkM+lPkMxBBVTQ1H/XXzf3a+ZoRNi7tSx3ue/F5HsUIxgjiJj+nrWXjYRqRFEEg5HiUiVJA0kDmiAlQaMFNQjCYLBMUZT3NVTPSvdaQKR/Lu4vr3vPVhaufP29NsncTP8c7ft+T55VMWZPWGTfZ/rPkZQixjhEDKpK5yPW2PsL8SPBfV3+sOfcXz4uwwP8PsbIzngzt1XfqXtjgCpYY1haWsaavu16mcrjEIQYicHTNs19z+4lTnrf8yj7xrJe5mKMpJguPXP7eV5O1bmSzY3ti6p6mAUWeJxioYgucD1wivu/beYGHmCPOVX9KfK7lrnlpuP6I9/zlSQ1lNUAkzyd7wghYERwVnBFwbzx1JXDiGKto+s6rHV43xBjQlOirqs8yBvBNx1lWdGFgBqwAoUrUCK+9flduFiKskAFDA4fQlYrBEieZj7FWEOnA44+7/NYWRsSU8j1qDCd7eCcwdmSpmmJ0dOGBt/NEDGkrLXSdQ1NN0NTJITAvBnTNhOC98znYyY7M6abnjs+fprx9oRoAi2eeWiZ+47QRVLMWo5FKAScCOWwYP34OjffdDNHDx9leOAo9ctuxA1dVsaMwbkCIwVODeJBkmA6WJMBpz7wCd7+B29Gq5LnfsHnsvqsYyQHpRdMgBmRmfEUsWEYSjb++EN87C1/weCzn8aB/+XZ0HmOMuLArKbdCYx3PClkRXM6GTObbVPXltVhyWzoCbcs4dcFvQOm799hff1or7wJRVHijKHznpQS1tVobLEiJE0I0LRK0+6wsjICGTKbbyNJEQPD0SokQUmolGjqSCkgYjDGkDRSlRXODfDe41wi4dCUlTsfPEVZYoshdWVRKZmlv2R3+Bt0bcQ3ltQtoW2Jnzn8rABfUElNYRw+JHw0WFdQlwXWVRhjaZoZs3aHZMYUo4Z6VVk+ULO0UrO8ssRoNKQoBBVPiBNm7WlCnFKaQ6zWt1DamhAbtto72Z7cyWysbJ23NBvrMDnIwC3zile8huc/9/kkjcQQsMYgRohdQ1lXKAYxBo0d9aCiKEo0ZaW4bWcM6iHGBjR1tLMpvhlTj9aoB0tYlBg8amosHlcpsYtEqSkrh5VESoYUWwpXUlQDTLXMxumP43fOUg1XcfUyqVznwx8/y8H1Q/Sa2L7xID8nlx/LJQ1sT63ORdHPQQzG9cr33rqwr1M1a4V5IepBwVi3T3GmV9Ll0mJUVUGFCxvn+dGf/Q7UNMTQ4H0gpMjce0JQDlQ1r/rCV/EZn/WZlEXEBE8RHOoLzk2FrWbO7R/6IH/69t+li0LA0xJxpkCahIohJcWiSJHHoNI4rB2wvbXJdDbLiw2TiKpgwVqDNWBdwhmw1nHLyWfxe//tL+66tuF6gQUeWywU0QWuB94NPENEbiEroF/OFbyVorCCmgKRRIgBo4pzhqqoaJo5827CqB5CCiQFSQGS5lf5JKEqHM5CDBGMI7QBWxSElDAWKlvQhS5bm5KS1OB9oCwhhIgxhtbPKOuSFAPOFXStUlYl83lDEiWmQIrZYurbRNdFRBzWWObNjM43eaITxbkyW1c10XUeMYJzLk9qoYGkiFhimmdlNRlIhkTAlBCNQEqgEQ2JRLagGFFKgcIYqkHB2rGD3HDTSY4dPsLSaIA4Q+EKXGEvWWVFlcIIwWSL6yBYllrLne/7AO9+8zsYLI+4+e++iKUb1gg2YJKlUUcANCbqaGid0mpka3OH+aRjvR4hybDKEtPbL5KmQ246+VSe8WlHwBScPn0WjhxnZXnI7u4WZ+/4MElnHFu/hS0zI9pEsA2+C3jfMhwOcdbQNi0iSlXarEhJQejmiFFGw1XUKW3sLXIAyUEKxJSYTXewtqCqHIOqoPMl3ud2zoatjuFwRNAao3N88lgC1hRElNHSEHEWYwu8j4Q0A1PA9Ch+3DCfgPhlJA6JrYVgseIwpsBYR2WhEsEahzGWwg1IEQqxSOuZjRuiBKqVxHwtMFub0x4U0pphaammrmsgYahAZ6Sg+CagtkMlYMTgnGBtwlpwzhHFIuIw2QmAxoSzlrKsSBoQiqxuiWKI2KqkcI4YWkII+C7QtXMq53AkxrubaGoZDpZYWjmItUI1WGY+2WB3Z4x2U4gtxXCVogRnDiDaIUVJOx+zee4sk+1NQjNjMh1z5Mga1pb4ZkqqmvtMg/sVT/bUTED3juRSH9/nUwBNSgwdKURCt0s5XLtkYTTG3s9smDShsSWEDiMGIykr5OglK7T21kjVmC3pGDRFpABCxGjm5DGYqFSa8D7w53/2R1QVPPXpNzHQGX68STNTdrvDDA4/nXY2ywZVEhqUwhiIiWAUjRFQkkRiECwJbKSbNGzvzFCTcM7296UkVVJIiAGRhMHijGCuyaS8wAKfGiwU0QU+5VDVICL/jPw2Fwv836r64Yf5FTF0KL73VAkxKXVdAgkjwupoxHwypRqWhOBpmoD3kdHKMnVdkuYzOg/1sCbEiCktGhI+RKqyJESlqiu6zhO6Oc4WmLoEScQY0GgxFtpmRlkPaduOFPNkVlQVIVhUEyklYlS2ztfMtpeYtmc4cXNDOYyoKj54Uoz40JJ6JTSliDVCipHZfEaKAWMNEhRjBWcd1lq6OEdNQursnk0+EVFEBSsgojgMpU1UQ8Pa8TVuuOEGDh8+yGBUY6wjxAAYYhJSyr7hpNnLWKpB5pGVaNn5wMe59W3vYPnYCs945UuRE6vkeTzhm0DTzYnkQURItEGxc8POVsPUReyhAZVYuOjZvXfCjU+9mRNHjzMcLlPXS9xw7CQGQ1la7r7rTioNfOiOD8A9E2559jHOlDu0g0jTZdd9ioEQAkiBCHg/ZTAwCBZjS8rCoCSWhpZBfZj5bErodhgMCpAB81lD8nPKoibEkuksv5GwrAaIG5CCgWQpCkfbeIyJJHVAwlkQa/K9dw2j0QoUJaKWpq3ptlaY71Y0c1geHGR1dICZBHyXLVOFNVgjGAPWWkCIMVvXYlRSciglRga088hs0rC94Vlaj8x2E3Fq0MMWXQZTJURrYpBstZeWwghR5ngzxRjFFoIroDOJkBIxCqr3ucaHgwHOOnyQPuIhW0itMWgK+MkuKUY2LpzDiLC1ucFt79vEOctwNOCGm04yWD3CfPci0/EWTRvYOHuK8/fcSdtMKUQoypKirJl0kc531BYuXtxGfcBZQ12VrKyuYrxnvDlhOKqp1xLK2n3KZX9wKcykHwt0L0zjUuGejz2H7ATfEXxDM97Auoqgmj0U1iJic1gFSvQtzXSHZroLki3V0reD7fvMOou1RV7wAZgSUU8hJVPbEjUQU8T5SFSDqJCScu/mDn/8pj9ga/NpHL/hBKGZ044D0+g488Hf5RPvf2f2xDghmr17UYwmkhqignEGp5qt10mYz2aoJqxz2NKQUJxakk85TCRkjq0oMRn8Ig1kgScAForoAtcFqvr7wO9fxfl9HJpFhN4dr/i2wRpHWRakGKjqAlFFFMSAM0o3mzFYW6F1BmcdIUYKZ5k3LaoGY6GZzxgMhnS+ywqHddkyIoqT7LI1NvVhcoZ23qIpW2RTFwkqqCRijH18ZUU3OUD0NfPdNc7cdZpbnl1gbaRtWrRXQBXBGUcSIYSWFOOlayiJELK7vY/OQ4xAZXpLSSAQ+3jDbM0RC5WFwahm7dhBTp48zsGDBxgNhySUrckugY5BUPCJlPr4UTGElBXZpeRobzvFmU9+klte8CyWX/AMwtHVS7GxMSY6n61IIkIyhg5wOMwYup0GNyoZrA8YGsfWnWdZGRxgfW0d5yxt04AaVlcPYIxgbGJ1bch0dpgTuzezeecZbnjezWDHVIMRTZjTdBBiy2AwxJiC8WSbwnYMB0OatqEoDJ03hNAyWFojhjnz2Zj5bEJZOMQZjKtIUmdLkilom4RxiSSGpVKgrGibXuHtEjE2jOoapcKHFsUy71qc7YhlBTYxHI5opiWz7SGxqahdyU3Hb2R9eQkfDecuTmibHOJhBAprLsUZd75l1gViFELIFngxBSqWrlHiHNqpMtv2tDsTYmPwR0oGKxGphdDl8um8xaiBoiVVu9TLirXZRau0tF0HBJLuhWE46rrMClsX0NAimogpEjXSTreYXLyHohpy6t4zXNy4wPbFC5RGcIXFVhX33n0Xw+H7cQLnzpxie2uH0HUMyoIja8s0xjKfTUma8FHY3p4yqgqiwmg4ZGlphfWjBzly/AjFcAkxDmMN00lHimnP0/7A6GMmL1lI0774zBT6OGvomhnT7S1821GMVmAwpCgqxBgi0M3nbG6c59zZs2xsXGTuE7bIoRJFUTCoCgZ1wYHlIStLI8rSUtU1xpYk32CspYwQsv0UweA1INYCBpMMG7OO93zwdk5c3KYqB4g4drc2uOO2O5g1HpLBqKA2YoyAT1iV/qlOGE2oWIIRXFSSJmxlGdU14sjndYngDCEqqRO0McRCoLR0wV7lyLzAAp96LBTRBZ4QEJGsAHVzSlfSBc9wUGFdmS2JIVyyWGhSVCyu2EvEMHTzjqKqMNbi24YYI2Ig+g7UUNYlMbS4eoT3DSEknHNUhSN0HTF4UjKEAHVdEGJCCRhbEaLiHBgMvrfEbp87QDMd0PnEeNxSjqZ03uGKAmOEmKAoCoqipG1bUgiIyS5kZxwhaXYXGpsTVwRKV2GdR50SOsWnRIrZNShisA5Kaxit1hw8fpgTN5xk7cAyVVkRUmRnvMvZ8xepyhVOimCkj4VTpVShNnnCu+fOu/C338tTX/TXmBwd4JdLJAR8bEl9gpi1SkFOvgghx+rWbghNQ5hNWKoLhnVFbDztdM4Nx08ym82ZzWagju3tMWVZUVUlp06dYjiqWV8/zOrKKn/xvrfx8Y/cinMHsTbkmF45xHS6gw8eq12OccQynkz7mL5sl1WNzC6eI0UP6hgOhijK0qimHq4zmU2o65IYDbFMWGuoyyFNG4h+mxBaYoqk4CmcwxhFpGA2m2NswoeAE8V3nqKILNWrbIsjeUdV1AwGSxxeX+f4wTWMLRkNxmxsz9nZmaDJIyRiCPiuZeJfu9gAACAASURBVDadsztt8UGwxlCYrHzEGHNCSlJIwjRAO+vo2l1CGLCmSsmM6BO+cUw2IMwS1QqYZYs4cIVgUHwYszstkbZGk1KVZXbzho7YzWh2NtneusB8ssPu7g7dfE4722Vr4zy2KJiOJ7SzFuscZV2zO96lHg7QzlOEQBJYqWoO37SKsRZXOFIKIEJdD3BOKOoRqKGoS6xxLB1YJoZImxSvMN3eYbo9RkxiOu9YumktP/T7ko32HPGXLKSy57nPMZ7et2gKWR6jJ3QN0bfMJxMunrmX1WM3Yg4dhMEyMQTGO9vcc/o0H/vk7Zw5d4H5rKWJBfVwBWsd1mVl1FlYXak5tFRybH2JE8cOsjQa0Mw7rDWQ+hhNUYIGoii4iEbFYgjA2UnD9m1nGYrFYpjNPU2Yk8QiNi/+rFpMynHO2dMvqAWxFkmGSiABxgiDosSWOXygkIpYphyaYyKFg3Ye6bpI8jO6efspHacXWOCRYKGILvCEQc6eTjS+wxr6dHAlaaTrOqoiu9swELqILQqMKyitw6AQI4hSFhVN19J1CVeUlM4RfKTziikDKmWfla90846unSPGIgmqwmblFYNzJb6LDJfz5EYEP1tmtznOfHNECIadrQnNfIcDh8CHjqg+J9aQrVNGDE03p3JZScYoNvg+bvS+LH9jFVsqSTqa1NB6T+gCJMFqVmQGZcXy2hJHbzzJyZMnWF87QFUXzLuW7c0NTp09x7lTpzl25BZK6xAjWFU0KYV1rLiazbtO8fH3foAXvviFyInDYDzJQEiR6BXvQ5/JK4ja3poqJMAl8OMx0k5ZPlASuobzpzc4snSA4WCEMUJZVpw6dZbtrV2OHTuOauS222+naaY89ZZP4+L58zz32Z/Bb3/wTRw5UTOezhhUy3ivqAZa74jzHQaVYIslQjvJ7a9ZWQ+hZN5MqJxgizJbzZIHI8zmM2KY45PSNYmmm3FgfQXFEGLAGktV1aSYcrKXKxBxFGWJSk3nxxhJuCJbthKC7zyoMhquUJUVw8EAY8qs0IjlwFIELdEoTCYTuq6lmXum0xk7O2N2JnO8D6wsLWFKiyRPCi2aYg7/VYFkaLtI52fYIiK1smxnJMD7AvUDullBwlCwjHUz6oHmXQdSYDrbJYwrUkx08ymxHaO+ZXvjPBfP3sV48wKz2ZTZrCHFbFVMyVOYlqE1FIOSWQcTnzh09Cjro4ouCXVdo0A5sGAM9WDAoeMnaWdj6tKiKRJjS0oOUy7R+o5mexMZ1py983Zmu7tU1RDpE4TKqsIk4FIEaI/9m0/s/59yglQMHaGb00y28c0E4yoQw9bZu9m6cJpmPoeixJlIvRSY7O7w8U/ezoc+/kk2NjboUoErh6it6ZJF1GLVEFUR4xifbzh/botT5y6yvbPNLcfXCGRrZBIFzbHoRsAUCTUWTZqd4gq2cyQLuylioid5zZ4dlxCV/joKmkM2VCMqvTu+j/k2Tui6gKpgRIgErDhEHc5Eoo2XPBM2gnaQAuDTp2RsXmCBa8FCEV3gCQFVaNqOsqpRLRnVFt81WGdIISIoXRfAgqihrnLGvCkKxJrsbm1meWuVogDjqMqKedNgxBJiyJaI4EkpW7y8JArrELGkmDCi+JgTFowFH9ucCT9vqAdDJCjznaNc2BxSDyomk4btzTGDAzscPDogEXJcWugwYkkGOt9hbd5WKaSAppykkK0+ewk0Sgownc4InSf4QEg5MaXAYqxhNCxZP7TGkeNHOXnTUzi4vo6zhi55usmcjY1tzp09y2xnAmsRVDHkWDhnLbUtSLszPvKOd/GZz3s+1YlDbKQGlxSCEkQxqcRi8Z3vtx7ay14nW4sJdNMxVlsOHjoIkjh972lW9WZUDLYo2Nzc4Pbbbuemm25mPB4johw/foy//Mt3cfzYjPl8xvvf+wm2d7Yp621KqdEUEFOwduAgiOHC+bNURW6benQAlRLRBjEG4yxlOSSFCcMiYeyQEMDakqYxBJ9Ik47g81ZJKbRgPRo6koDEQOGUemkJYytsIdhCWF5eopkJMEeMyxn7SRhPpqgqS6MRo3pAPRhi7YjZJNLMt2laT+yEFBJt0zGdzpjNZuyOJ0znDV0XEFtQlRXg0d7Kjea4ZGMKFEtIidQYNrcSSzsRcQFjlbaJdN4QO0cISpABIha6SPKCRghdw+7OFpOdTe6+9X2cvfd2dne22bxwlqKbsr4ypB4usby6RIhK6z1VVVKVBd53dD5wfPkAS6sHwUSMRlaMUFYVIISYE/SMFXa3NpjNxxxcH1HXKxgZ0kynBD+jFEOxvIKfjDm4fojDR44jAkVZMBgtY4wQsFzo3CUrKNwXAqr7A0UVYvDsbpxm8/QdpOCJ3Yww22U22aZeWsY3c6YbF9jcnTMYDZmVlgsXNrnj7nt574du5fyFTQ6srXLo4Drnd+YIAYJHnAMVnCrGFbiUKJKSfGBjY5MlFyjrCqemdyhoXpQlpVBFkxAl70BggCABw31KZ5KcQJkTjQy+8ziXZSr1MQYGR1KDioDJCY9d7KAQKHOsMk6xCKjBmkgIkUIVUznaJPh5IGdSLbDA4xsLRXSBJwyscWjsEBLjnZATkFLIyQXOYYzQhYCxDiXHgaqApkjTtBSVJbSRoigpDHRtR11YRCO+bTC2IAaDLSxCR+gS5ahANeT9N52haZqcjBATxgllKYTkUI0IltnE0jbZXT2ZztgZn+I5L8oZ6sY4mmaONQVhL7M3JZZHy5RlzXQ2Jgabt4sxgnECPit53hs0FNRuCaTAuYrCGcoSKlewfvAgJ06e4Nix4xw8dIhhPaTxnvFWw8bFHS6evUC301Ik6ffz9BSVxSiUziFzzzve/FZWbjrO4Wc/jSkeoiGKIiFSBnJsnChEIfXxhcHnBKwYI9EJoWtQbRksFQRJVPWAnfNTxk3DwbU1xjtjnvvc57K6usZdd91F08x4xjOfygte8AJOnzrDyRNHuf2OD9A0M0pX5jhda6nEMJ/lzHdEKKsBaoWVlZrWO+bjKU4Sg3qAK2vapsanOYVEgvdokRjUy+zMFZGW5aVVZrOO3c1zHFgr6GZzBkPBUGLpGNUlaoYk3yBpzKHVFZrhQeazXULoaH0gxoiLeSuvqhqwtLxOYR3dPHDv7gW2tjf7RLgBOzszLp4/z/buLvO2o/MdMQZCVJZGKwQ1FOUAYxy28kiAFCIxBiBv52NdSWwc052IdS0qQkoDNJX4qPhZJAUhhJLQ5AVNaLJi27RzLpy9E9mNbJ07h5/u0s7neIGVumCwPqAeLaFiiSIMhyMUaFpPDSwtD6kHQww5u7CsaqrREiF2dPMZ8+k0JzINRxw5diS7tp2jGCzTNXN8O2O4vLovUSvgigIfYu9nFyY7m8zmc1J18H67ke4d7g8ZzVsuBfx8yvapuzl/+nba+RjViHUlbdPRzidUw4rbN2D58BhnhA/fdg/vfO8HOXtxB1VDEqgHA1I0FMZTSEFZClYiThJF2MbFrMgOS2G1hnkXiAhJEqnfezWRiBpIQCLLaBBPiaMQg0n5Hn0K/ZZwilWT9+rV7H2JKfbb8wpByJZQozhx2QDcBUrjqMTiSVhjSEZysl5IlJrwIRFMoqoMRou+vRdY4PGNhSK6wBMGpt/mxFaCGItqxLcpbyAfI2qU+XjGYHlE9J6qcETvmc4jIoZmFsEWNPMGP59hCos1FufyfonWRaKCS4mydpSVQ1PCFTl71oeIsxaxgveBushb8JTO5pjPqmbjjjF1NWQymXLu3L087Tkzjt14lBDnqOZEldl8inMFg3qQXXc2W1xTTHTB0wVPDNnymZW8QIqB1ZU1QjelrAeUhaUUhzOW1dVVTpw4wYmjxzi0tk5RVfgQ2Z3MOHfhPGfuvYfxxW20iZTW4IwQfUBDpKoGJO/56Ps+gBtUHH/+szhrWlKKaPRYoBBDXTgiOW41pRYQUmxpmznWGsqqpCwKprFjNKpYXV/mrHpGK8vY7YLltTVOnTnNiUNHWV1dpW1z7No999zDocPr3HzzjVRVweaFcxw+tMY0BVZXVtk8l93ZRbXKcLTM7nhCXVmSWuqyYDYNxDAlhZZWhajblK5kNBzQtRWGQFHWbI0b0PNEqXFigJwccn7bE/y9rA4HHDpwnK4paGYXaKbnqYuVnJRmE7YqGZYWExxTjRiBmCyRhHUFlV0hemU2nrK9s82FC+doQsPBtXVGdWC8s8vmxXNc2Nxg7hNmLyvblfi2xRcFpV1C8dQjxSchqCHGnJEt5EQ738LuhoAdUA2GOFnHaI2qZzyZ0+w0zOfKaKSIscymBcl7SErXTLADw5FDy5i1mq5rmE3nDFdXWTt0mGq4TLKOIJaiqrEYSt8CynBY4VyBlb2N5gVxjqqqGIyWWDnQIWKoR0NsURF9h5J3DIiYvMWWFKSY8H6e3cxpCinkvVW9BxGiuvuSkHrs98bvQTUv5EI7Zby7xSfuuJuzF7eYtYHl4QiTEq0PzMUxG9zA+KMXOTI4xXh3m62NbULrSWK5cO4CdVUxXDmIcUJdGqwTagmEZgevgjiD+DFrK0uMSqEsLCEpRgOiHckkhKz4R5N3J3AKEnMstSiXtjkLKZFU8ziGIqpgDEn6baFEwUhOuBQDmvs9Rs2L5MqSyJn8xuSwGIS8D3ACdYmYPLhEMSpwZfFYD8sLLHDNWCiiCzwhYEQoq5Ku8XRNy2hU0bbZYjTenSFAaYXgEzsbOzgjzAWcFUJSrDW4okBMJIWIq0t863PCknbEBPOpZ3mlwjlL03XMZzlT+sCBEV3boaI4AyEKw+URaERSwBUG+tgtVUuInt3xFqa8i+d+9o00zYSiKBkNltnabRGb9xHNm8nnR3DWzGi7vM9o6HxWQGJENZBIjEbLuDigLDyj0RKqgbosGY2WOHroCEcPHmJt7SBlPcRHz85kwulzp7n77ru4ePYszXhOCvmNNYIQXUEqKqJXNm4/xda5DT7zpS/BDIfMuild1xLmM0wMDAc1sXB0KeXY1JDyzgGtx5i8FQ6qdM2caTPHHT/MZH2J7VnLer3CyuERm+fP8M63vYPP/7xXMKiXuO22O/jgBz/AfJ73kizcyzh65Bhvff9buPGpxyjGO9k6pHlHAFILmnLGs0+0zZS6Kmm7RNKY4/J8IMSANxFnG4wrCREg4ExiOFwj2QpNynw2w083OOig29zk3Lldztz1AZqZ59Baweqh59C1m6wcHFAXxwnzlnYW0JBQ77E2UYrFaySKELucDHZhc4Ot7S1C7Dhy5CDLyyO6eUvTzvEx0nZzmmmDisWUjqpewg0S6grUVRhbYFPei1KcI4VA1+XkOmMMojDdiYjUDI8eIYYcsxx9ZLw7ZjyZMJ8J6egaqyuHqZyhLC/i3BZFUVJVJYWDQhLJt6ysrLC6fpTVwyewZUXslSbjBiAW7WY4CyuryySF6AN7Vkrrit49nzCDIYJiJbvpO59oZhN869mZjnP4ynCLpaUlrBVMWbG0eghrLd53NJMdYlJCErzIvrd+7XvrVY+ckBgJ8112L57ho5/4BG//8D1MuywfAzNntcwviqhX1hjUJdF7Ls5nHDt+Ay9ePcx7PvBRNre20Ri5eHGDE1XJoHCgkeA7VCeY1IIUFJIQP2P74ha+NNh6FTWGqAajFpWCoEqwBpdMv/+vyW8f69++lnf51Sw/Atb0b6xSQ9SAFZu9O0kgGpxNIAkVwRaQBGKZd+korJCcYCxYMYgZEEyFpUO0owieNsyIUSmLxfZNCzz+sVBEF3hCIKFETdjKIV3ep1N6y9Teax7nXaKuKlRANFG4gq5t8usjkzIcgUNR6yjEoniiyiXLTVkbfAykkF+rWNZDqsqi0VMUedsnYwWIaAzEELJltguA5M2/fcfO9iZNuIOXfdFhTKF0PmE1sjPeoO1ajFhijFhTkFJkOp8zb3cxZKsoqYQEVhJRPc4UjEZLtDsJnzxlPcQ4ZbBUs7qyytLKCvXSEFtY5qFlZ7zLufNnuPfee7hw5hTTnV3aLuDIk6cqOC0pk8XvTrj31tv4rM/+bOpDa0xnE4ouUhmDrwpUofNTolcoKrCWwdISoVWm0xZRMBZUE+PJLk2MlMeOcHeIOXZzB5pJxx+/5Y+Y7ky4+6n3MJ9lxeqWW27OWfTAuXMXOXL4JBc2x9y7cY5pPWO0dIKitBRlSdNFnHV0Xjl78SInD40IwwOE0FHXJW2bY+nUCNbavIuArWhDfjWpcwraolFovaLNhFFpUbtCM1HuvHubi+duZVQo6889SnESXHWQyhrm84Zm2lFXNcZV2QLfb+ZPUrrQcWHjPNPZnBiV0WiFg+tL3HLTEVKK3HXnOdqQKIYjVtcPYosxk2kDGGxRMRwtU9dDiqLIb3cSA8birENVcfMZwbdYozlpzTrwjtQMMZJ3apjNp0wmW8znc3wsOHz4GLZbQkNibcUw2ZlTD4aMlocQPYLHx47oHFoOiEWVk4YkZ2WLGxJS3q9TY0vTBFTjpTeZlVVFStlLEH2HKywkj5OOerBGcDZvsWaUA8tD/HBAPVrG9lbUEA1bm2epigEK7GxtEnyHrZaR5cOI6D7dc08xzVnzohD8nOnmKc7ddSvv/tgdnNmaZYutieCENkQqI6S24+jaAbZO38Vs3jI3Bzhw6CjHbjaUxa2EdorXiPo5jhHOKO18m3FqGbrEgaWCrplydmsDm1qmc09dlzzr056FBYwmXIpEDZRi+P/Ze48ey9L8zO/3mmOvC58ZaSrLZJmurmrHabLZNGIPORCGIy1mNRhAGwECtNNS0EfQN5iNAC0GA80IcpgFAYozBNnNnqJrtqsun1npM/x15x7zWi3eqKJZiRREdgHxAAnEJgMRce8953/e//P8Hi8kiHRdCjLgpUCGlIj/rKJTuAScJ6ZhWWsJwaFiJMR0rUMky40LFkeO88mvXmSCsioxcUAJRVFeh+ItymyCRaPNmtg+hvZjzNB8VulwpSv9XOtqEL3SF0ZKCJSSGJfhnE/JeR8pSo3rPuN/Csoyo2sauMSjfNZk1HeGIqaUq8tykBqdabyz5GWFvLz5eedRSpFrQfQBYx1SBUbVCBs80iWG52AsUkcKqYki4LynbVs2TcdXvq3YvV5i3ECeFViT1okSQdf36aQzgBLphl1XE4J31HaHHX2XVfuERf9jCAKtcoq8wqoWlWXkdU5eK8bjktmkoB4povYsTcO6aTk7OeXk2XOOj5/SNg0+XC42ZUBETwyBraAYtY4//9FPGB/uIncqBtMQSSB3ISNaCxCaYdgQQiQXNdYFTGjpB4u1LVF4CJreezZmYBMG/GAo4wRrSkQ/5s27X6Viwr/5N/8z3//+97lx4wbf+c530Fpz//59vvnNb3L//n3+1f/0r/jJT9/l61/7OlvlmEf3nrBTXUvQdxcYjyt2ZI3mFeoShIKMAucswUekJlkNQkRKR54pYqgIrk8VlEGQ5YrgAuazznPpuXY45ehsyslx5JXrFdtbu59bMHrjkTISlCRqRWcs0SW/48ZqoumxzhCFYmd7H2fTafbOtGZnMmK5XNF3HcYYJtMZh4c3WK7mPH3yjLY35HlJptPrK6XEOZvYtXmOzvTnp8B2yCgzyWScU4/G9G2HNxadaxSOZn2B96lxSgpNcIp2Y1MYqNAUdY5UCqU11g6YwdINPSFIqsFirMddpvRV0EiXTqA3zYr1eoEzQ6IPhIDUkhu37jCZaLxNWwlrTfq8KMWwOOP0YsF6uWZ7d5vbt17AheQLNf2C0/kZz58+x7RzdDGmW81Zz8/x1jA4ydd++7/5a0No0mWKR8hUGuEMq7NjfvDu+9x/vsD5QIgJj2WtoMhzyCTCOMz6nOfPn5LV24jlCmTGaLYPuqA/ehdvelzfYtsVpYrM8hzrE2Ktazc0TUPftqxbC6pgvP8Cs+0bqJOnqe89RhDhMmOYfKNBkJqswmX/kxA4kvUn9IFMKFwuiDoiFMQgLoNKaeCWIZEwpNSUQtK7Hq0EIlNQbjFVBTI6ysmrWLVFCAFd7qPULYh3yM9vcb76AVRXq/kr/fzrahC90hdDEdrOoFQKyigpcUGAh+g8QgGfpee7DqUUfdviQuqPjs6htMIHz9AbaqXI8yzdLFSGEDIl2FMJffoXA9Z7dJ5jjUlr6ctWIkhrROEFuQSCRwnohw356IT9Fyb0piPXBVrsc/pkhCyWqPGnhBgosjrd+KW8bDuymEFB+xq6PGQwnxLxICRlnieAvLV4ItNpTjXWjGtJWTosK4a2ZdMZLs7WnByfsji5oNs0yV8oI1qmlHyMMiGfesNqtaGejdl+6SZNcIzIk08tKqIPWAubzQZEqqnUISeGNMDE4JL/TUui9Kw3aywRPc25+PgRL4UDmouGyTDjw599gpSRTOfcf/CAruv4+te/TgiBPM9ZrVZ873vf4wc/+XNyVXJz9xbf+s1f4/s/fAdJmegHUqYhOkq0SmnyYGwqJ1D2EqkkCCJBv7UsGYaeGETC3FzWXWqlaNwAIuAcFMLjjAXX4KKiKgADpw8+opiNKMY3GXyOsxvGsxHj8QHeVpihR0VDlBl5lrMz2UIKRRM2tJsNbSc5PVvw/OiE45NzRFby6quvMR2NOT05pm8H/Nk5IXjM0NM2S1SWwnVCyNSqhaLv+uS9LCsKLanrEdeuHeKsYX70ANOc0Q09GT1lmd5LWQEis+SjxNId/EA+7WnaDY+fdWAGvBtS2lsC8xW9f0qMAucjQucIIt50mGGDMyZxZN2A1posL9FZiXUJ4+5sOuF2tuf89Bn94oJ2syEfzdjd/Ueslhc8e/yQp0+OsKbj+PgYrP+c/lBXJUqKywBSOrH/KxXzf2MojZ9/tWk2fPjknM4kL3Xwye6ipOJwu2ZUVxwcHrJcXDC4iBafd5xSaEW2c43SnzEVK9reEjQpeR8GNOCDROUVOg4onSV7T73NtYNbhKAJMl5eCyB6BUIjQ6rYTA1JaQh1wRODBBwySKKVICNSR4JKnlHvPQoFMaSNvk4Dd5QClEZEmXzhuaDIKopqi1L2jKcj1u2aQVqErHGxTn718T5b3GVSuf/fL81XutL/V10Nolf6wiiGVKeZ5wrr0vrL+YH1ckNdFQTnk5czeIRU6KxABIc3Hp1rjPEoIShGNUWVJ5+m1smAJcQlFD6dvmklcT4lemMUafBxkSwr6bu05lTGpppGJNZJhiFi3TFvfzsjCstgQYmc5fGIfrWNLFsOD3douwuindIcHRLEku2bxxhjWZ3PiE3Fif0YNXlIkWfkmUSpgDUpkasy2B7njOt0U/LR0nUtq8YyX3bMzxqWFyuabkO4/BspociURApSUCdEzp49J0wydu/chiL1rQ8xtfsQckLwOOORjC4rKjUyQvAWJSSDc+n7aY2xFtt7jLdUdc7Qd9j1mkf3TvhP77yDXwa0cgz9BiWSFeL999/n6OiIvu95+vQpz4+OUAGUd/z7/+N/5w/f+S6333qVF196A6k1oNj0qd61qMaIKPFhQ28sKhNEFRj8kHA2PlIUOcFFooTBOwol6HqLHQZEjGRlmaDs0mG8YquQ3L5+HaEcw7rh+KzH5h2vfw2y0V2CG2P6DjXrUOU+87lGmFOEkngcgzFY4zk7P2HTt+xdewEyTdMPrLqWvfEOo7KiKnKMMTjvCTHgrWE5P6NtVqhMkZUFOstROiPGyNB3qRdeKbx3bFqYbe0yHdWYzQLjLKN6CzWaoVYXtP0KWVhkfUq+NQbZE7s5O9cNzWZNd2oplUSrxLlUKtDZCy7mc3wE68BdnsYpkbYJAnDWsm6WqQlLKo4ulhRFwiZZ0yEJ4C3WJk5ujJGwGrj4g++iZGA9X9N1Hd6Hv/55JrLoGjIl6Y2lKGpi/OvRpLSm/6z3/ZId7Cyn8xXnTUKh+Zj4tpGID5GtSqKEp64KPvnkUaq1kJqyqrl+eEhdjzg6PWG5Gbh+bcRkAr1xbDqHGSy9jdRlSdu13NifcdEqBnPG0yef8sLOmL6+k1Lz0mNiIBBRMfk/U/e7J16S99PP5Ak+VfZmUuFl4qwKn4KUMUhC+s8IAkolfFaMkegEhICSCrzANk8R9gg13eb44mdIaZju11jv6duAtQrrBgp5RKm3/h6v0Fe60t9NV4Polb4QSpWIFqUEdpNaiKLtESEyGpUEFxmMI/qIVgopLms+o0jbLhGp6rSmkkojY8D6mLBMSmIGi840QkTKOq3+vYvoLEMpGIxhNXiqKqb1abQIqaiqCh8UQ2+QGq7d6dm5VqFkGvg2jWSzmNCbnun2Eud6IpKzZ9uIYZ+mX1FstwgmrM/2yBho/D32CodSJUJGrG3wzqGFRhOYjjWZDlgf6AfDctVzMR+Yz1vWzYa+S+xHXESG5GNTKLRKSCihItV0hNib0AlBFgvqTKdBxFicc/R9j3OpblT7iCwkm7ah79tLHmog0zlhiOAyajkj810atvJtNvMWRMDElvG4xvRpbby3t0dVllhrOT45oe866rpmNp0i/TXmJ2dYs+F8Fdg1N5jPL6hyxWxnBy8maBwCQ5RjFheBWmUgHKZPrTq51nTOM/Q9znSpDADFatGTFx3jUUIQ6aLA9IHBtUipOdjbppSwvHiKFytu7Ozg5JSRjtR7e5yHBmc868WC7esT8mrKpl8gbMMQe+49fMCmaWiaFXv727x4e48b126wXvU8fX5GkWtWyyXPnz/jk08/ZrlepAYg5wmDo78c+rKyIisKhJBY61BKUeYFqsgRIrBqWvq+5cbBHrvXX2R6cIvZdIb3gecnT3jy9ANsPCafbNCjFVJ36JEn2pxMQGjBOo8bPEIEyjqnlBqpUlZ9GAzWgpcKHwN9P2CtJXqPvfzah0Bdj6mqHNt3WO/IlGIwFmtMQjq5xNz1PqCVousMPoRUExtBZZJMpoCflOLzpPx49Jk39LNhVFwOpvEvwaIh0ncNR2dn+Ah5rjEWoEjO5QAAIABJREFUskzhrIcIi9ZDMDz96UesFitkPkIqwXgyYm9/l6HvWC9OMYPhfOVoNx25tLQDbHqLFTmv3q7Z2ZpRF4Hr+yN2C88sLmlO7vHTxQnFqwrpQLuE10ru8UDwIIVCy7RdwDuIgRA8IYISyccKXBZnBISLWG8RMaLIUEJfnuQHgjcIFYnRIiPocU6elyznFwxuw827BW9/6Rb9esW9x2uOnlmEcJRiSVnWf6/X6Std6e+iq0H0Sl8ICZFg0GVZcX6yZGtLkRcZfbO57HaHKCRaSkSMuH4gZqldJgVXJUWZ6jiFVAgtqXSGVBI3mHQ6qgTWB6RIOJWoBM6DMxahBHWd4Oo6yzA2ENHYweOjIcvA4nnpjTFSCsajLZyHk+cHbC4Eg3vCi9ccve3ZNHD+HGbTgPVLhiHw7F6B6ivm5gHj689AZmidEbxH6YKigDjES6SSxxjLemM4X/ScLwZWq4G+HegGi7EWaT1pshSpKlJGFAoVU/PUeiJRCmyfaiS98kjt8MGwblY4mwJYUipiFFhnMSEkK4FQaYXvQIaM2DlqOUPrLXzbszd7hSgNr71xg2xZcPzhEd4Khhgx1vIr3/gGdV3zn955B+c9VV1zeP06f/S9I2SRIzOo9jS717aock012sX0LahkoWtax9AfUeSSohzjrGJrp8LYnCIHKW16DZ1P/l4CVaVBaFaLBZnW7O0UNH6Gm8+ZTEt8PeL8vOXJxZpbtWM723Dj5t30uzdneBch01Sjmhg0I22wRU2/MUjpqIqCZp1qMuuRIIaBaB3BR8oirZ6fPXvMp48/Zb5cILPL9q4YcN7gfSI4GN8i2rSKjUCeFYiqoshmRKWwwXB68oy9rSmz2QwhJFVVI4Qg05qA4eSiQ+eeqgqUEwciYjcwUVNMdCwXK6wbKFUgVwW6yCmrInmJxUDuSQB1a/ksYCODx2rByVnLcrHhYm1QSmP6FuscIQSsdcjLRiAfLk/xUyMD/jJJLi7DhUWuiFqhM0mpdEKYhdSr/pf6G0EbkawAIQa6tqHtWsaFptGSGASjsmATB4z1fHqyRilJkdnEew2pWlVrjek7jp884OjJA6aTnHZQLDuHH7p0ko5gb5QzUYGRGnjhYAcP7Hz5Lm+8dpvODPzkwycsYktQEJ1AKIGPkiEGdEjXIx9sIgkgMdFeWm0EQab2JGRInlEXcS7gXVrp6wxkjFgxACGh1rxHZoq8KpiN9lk1hsFIZH2XLTWjEAUvffmAYD9ArjsWQ8N0Kji8cfD3dYm+0pX+zroaRK/0hdFkXNK1HWWh8UOPI5LnmuWqp8g1RZkxtD3GBupRSSahaVqEVkRyjPHIPEMqRUCR6QJjh8TtQxCcJYZIu+nwQZJl6VQzBKiqEcY6RARrBsrpiHnTsTuTRBTGDPgQqKqSoqgo8op+kTHM91A6cHjLkZWCZmG4OC5YLx3eztFbZ3SrCd18incrsp2HTGaKqqwQwhOipd+swWmG3oMIbLo1q8Zwtug5n1s2G4vpE+bHGocMGXuzl8ii5OTp+0RvSJjsdCMHyUCgjA4BKJWqTwkerQUuWGSmE2NVZYgIeEEpp/R6hWFDNIF8KBiFKW/deIlxMaFZD8zjGdde3mYIG4YomH3pBu+u/4KLs+e8/tIuL915kcVqzs/e/SFFqSiiYnl+ys54xK//yq9xcX7O8dkRr3zjdV59/S3WJwN5NebkpOfi4gk72xl5MSPGjMlk7zI0tqIsHVJrrDXpBM857LBBSIV1yfs72AjOEIJnsVhg+gJnWyaqwgyA1qisxosN64XByU9YdRn11n1GN14nssf84oxZ7CnGt8nyAjMUKBk52N3D2Z6To6fM53PuP3jIxUXDw8dP8M5h+g3r1YKzk2OcUIyqEUrIxJa0yfcMLg1tHoRXKJkjPhtYlUbo9L6fry44Pz1lMtumGk3J8gIpBGVRMhvtMNhD+miQRDIlUXmJMIHJdJ98NiE8fohpllTZJR+0rBFK4f2A8emhTVxW0Gqt6AdPiB4A5wLLZqDpN7jgwYfUsiVSt71WkOn0xOAuByuBuCRb/OXX3oMRaRBTIpAJzed+SwAEwTnadkOIKaWfaZ2s2zFgjKE3jnGukALKIidTAq0EISh8CETSKaIAZHD0/YZmdcFzYbj/8XvYfk2oJhzuXkMKyQf3ztFqoCwqmuaMkoFcOR48XHN44zrr3vLSy3c5yDXFaMr/ff/PMDgkAhUuq3KFApHW6FLq1EQWPCpENJIgAoNK9aBKC3wEfzmESi6RTLnHKXPJbAo4YxlsJNgAIieIGdeufZm3f/E3UNmUf/KPX+NgK6dfP8Ks/x3W/ABx0rF/fZQewK50pZ9zXb1Lr/QFUcR1fbrhZhFvHaYzUBRIDdYOCKcYj0c0m5bJ1hbeDAjREQKfr/6iS8No9IHlckFZlcTL9aGxlohEZiXRDBCgKHOaZmDoDFmeI5Uk4tn0hjwXZJnG2oCSmlyVgKAoaqpqh09+AkJUDO4h+WTByfkcO3gef5KzXU/ozDE7Rc/pwy2iE6yHR7x+GBiPR5R5QT+0OBeQIk91nz7QtC0XdsX5sudiaem7dBOzxmKNI1r46iv/lN/+1f+a04t7/C//9n+47LeXlydSESkEB8U2qtS0oSPTkSzPiDHHx0hebOiH9PsWeUm/6dBIdCygLJFiwFjD5mTD67e+RPPxgt/9/X/PwwfPWHcXGLshINmZHfLmq1/it3/rv2A2lizn5/zH3/uPvPfh++zfvMbW3jYZArC888ff4/ruDf7xt36NX/vK13j5G19mJSUfrY+YL84ocktRFOisZDTdwbgl3s4pii16pfng3gnj+jk7B6/io8fZSKZL+mHAOkeZFxSFpvWS1WpN163YqmZooXFDizE5q4tnzIpUevDB047dZYcvt/jxvTVvdj0Hr/wyIU5YbQRb6oJqvEWI28S4QGdbrNZLxuMJWSXRusIYy2AGuqHD2o7l8py+3SDLOpUA5DlCgPcW5wdEHilHkSgiphVoocnrktnOHgd71yB4+uMNq9WS5WqOsZay69naOSBTirZtCcZTiBlC30DYU3zbEY3Ddx3F/oj9ndsIKWkXpwSXrC0yK5GZJFqTfKshBX5SeCjgfWDTdixWDadna1atob302iqRKmKVlJdDYrx8zwoSa/3SJ3n5vWKMRKXAe4RTSBWwxtPljkldMBIKIgxtywfv/og//8Efc7zumMz2ePGFF3n97l0OdrZw1iFipK7G3Ngy5KMROjo+ejKkh6rLawbxsuXIGbpmydPH93j2yNOvV8g8w9q04Whaw2Acq7ZDqoa6yjlvK/ZChhgGgnecnzasVz/m8Pouy016uFM+ndIHEZHSU8ScuhxzY/8au9u3iVHw+OF9jo6eMhpPWVws8b4jqnRSb73HmgGFIp8IdAFKRoSyRK8JNrXaZ0pSVi+ze+u3cdmLXH/rbe68fpflk2foaptiUjGqtnk936X1/ydt9zuUypBl9h/qgn2lK/2/1tUgeqUviATD4FB5hkcgdM54q2RoOgSSrEjDpVSS6axOPfRKIzLJtK7QRQZRptNQnxK248kYY1xaaXqX1vhKEr1HKE1ZJli4EppMOvqmQ2mNyEY8fXTBdKIJ4xIpZRpymoFZUVIWFcvzHLMcgTCI4imOFZmSLFYZw3JMNinZyBOGJmdzfE7nzqmuH6OzjOBTaAQiSmq8TG0q1nqWy55Ts2bdOIz1xBATy9E4vAlslS/zrbf+Jat1yfvv/xRhHTmSTKgUPlHpdxxVuxSjEvwqJaEzSW83BCJlURBiWrEWeU6hMubnc+paMqk1XadwG8v69ILvP/wDfvSnP2TdLHDDBh0CDJ6y3uLuGze4fXeP8/UTPnz/AX/x8U9ZLVZs788gREqdU2pF7zqKwzEnwzl/+KM/4HB7m+//5I/YfeVLyPF1VvMF62bOdFIxm+6gpKbddEi3JC+2KcuMu3duglZkeQlFTp+VNMtTnO0oS8XewXWQY8JJqt1UIiJVjnABt+pYnp8wn5+RF7BeDUwKxenKsiMalAuY5QoVjqhGO/TW0ywXjKIAcpSK6axPCrKi5OWXb/LlL30JZwNHJyvOLpas2obFxTnD4CgyR/A+nSJqidIKnSnyMWxfg2ok2KxybDdha7TPjes32Z7OaJolZZEnn3F09H3Ds+dP6O17bG/vMKkrnB2QQTHSe8RYENoFxi8T8/VOngblGzdZ1TnNxQl91ySyl0zWlUly0CYmp5J47+n7nqPnRzw/X3O+HugHg/chPdoJIEaiioBCKpFsMpef2c+qOWOMqVEoRCAFdBAeHwU2RFwIbE1GFEXB4vyUex+/y2J+xpuvvc4rQfDuBx/w7/7tv0bqmjdef50bs5LVJpLrAilgf5oTgyLTinZIQTohBJkUuBgJ3tEPLWHu0CJ1tCslGVUZ01GBumwbK7IUUuy7gYfPz3g+3/BPf/EOs60pP/jZY6yP1IVCliNUkEzUHoaGsqjZ2zpAhoJpvcfq+AlPn36McQXnqyVRlIyL6zx4dMradkz2phSzAtMtkCFQTyR56dGZxsUIFlznEUCZzyjyO0yvf4di8hI+G3P8bMHNVyLleJeTC8PeQUVRK+z5AeP9f8Fb3/pVutWPycP9f5jL9ZWu9LfQ1SB6pS+EBImH2HWXrUrGEhDk4zG5EGn96iNZUTKYIQGwo2C2u5tuMt6n8EeR07eWclTSdyb10SOIaKRKDECiQyuFtQ5rPR98csxrL+9S5DnLNvL8/pI//qOf8uj4MV/9xhv859/5ClUtUCqdAFmjuP8ThTMFy82nvPaPBFU1xZrA8weBra0DOtNQTTyr0wOkLui7H7BXXhD8FkpOEUKD8ERSytgOBmMMjfOcXHTgE78weHDeEawn2oKvvfnPGdWHPDv6GR++/7uIYJGZRGiBygVZpSnqEmcjYvCIoJBBk0jaKeiVKYUSkuA9ZhiQHpQUqBy8Mfi1RTvJk2ePULLkrd96i2U3R2nLdb3FG/uvoFXJUbvi3J/S2XPEDcPX3/galRXc/95fIJcblqcXlJMxervi7d/6Vdw4w6wbRG8Y2gE5ychDyXQ2YW9/j35wzM+X5GWPVpG63mLoPev1OdPpDG8c3aZJHmGbXlWlMlSWY50nhCXRbzCmpR5LAoFM5jRPPmW9PGc0VmyPpzy3K2Zjz6NWcXGyZKQUtlXYiyMyuUOZjRCMWKx6vNtQbwm87ghyIDBwbW/Etb0dmrVjXI/QCtpmzaYZ6PuIriLBGZxVhOARwqPzwGQ7sn8QGY2hHQe6VcEkm7I1GjGpSqIfKDJN6zwhWDKlcP2Ks/Nzzs6ecnh4i+l4TK4zlBeIUGNxuH6gWxYIkTEajxmPKsbjMfM859mjj/FDR5A5eV5Qj7epygk6LynLmrKQWLNgZzdHZpFMa5rNwKbt6AdD1/QYa9Ex+UNR6hL4HyAmPydwydi8hGvC58n4GMOlPUSRlzlZkfNnf/pdDg6v853f/C+Zbe8gkHzr2/8Z/+Q79/njP32HT+7f495KYw3ksufx6ZIBTS4i3vlk0BSpgajOFJtL4gQRgrPpZ8wUQgheONyhKjKa9YoQPaWWEMHGSJkpbu4U3L2zz9nFmr3ZiI8enyULj9DksuDlF+9SV1O01ayXa549/YRnvmd+saQ9OedsvmDjLLdeus1se487d1/mvQ/vcf7kAm9m1JMd4rAiOoOM4bKtTWJ7kCGxbEu/y/7tr1NuXcPrmqBKegkf/uwxX337FZ4drbn76pQhQNMGXnj1GnDI+fLrPHv/HeB//Hu/Xl/pSn8bXQ2iV/rCyPSeqswSR1RokBJvHVmRU2Y52bhIzEmVwjQqT/7GEANKq+SzCpGyyj/L4+KdTRBvrUBCcAEhBEJGggtkCiotuPdwwWTrgD/83rtkQ4PWPd/8hbvs7BTEaIixTC1LwLNPJfPnCpmvGe8es7U3ojcbzs4iy5OcV16Z0GweMbQT+sUUGxumtzzbhzsUqkDawGp1gpCJQagpAEfvezprsDYgCMToiS7gbSBauLH1Nb708q8zmJafffAfaDdH6EyiR5pqlFGNM6pxyWgyYjAtXhrqqiLTGp1LnN+w3rQM/YBEoJUmOA8BxqMRUimGrkfEDC8sB68csn2whygiso+8MLrOm3u3ePTjD7lolzRTWE4MaidSxoqyyph0jpu3PFuVxReCfK/ik7Igv5khd2tySnKd+rWr5xPkU0nwEi7rC43xFHlgOqmQOrBYrVguLhjVJb2ROGPQEiCdTiITQaHve6TKkMWYnT2JjJbxRDNWM44XDVvjikmQTLHIKBB2wdao4F4TKSp4cm55+s49RrOn3Lyxzd23v0mVvYDNaoKY4/QJ+WRDzFf4aLC2Z7FasW7mONcx9C1d7zEuDUvODgjNZZragnSMp5HZVqQaQZFDKaEIGiUFWitynaX0uhkYbEehc8oyQ+eezi05X0ZQ19id7Sak1aWHM3hHDJZMCcqywDlHqTL2dIYXktMnn+CMQaqCTBVIoTAmIbdkAVJ0XNst2frqHbanM+aLnm4wNM2Gi+Was/mKYdORRZ9KBCI455L30afPXLiEgcYoPreICCRC/hWPqVToPOP2nZf5pW/9Gls7+5+v2Yu65tU33+LFu6/x0bs/5j/83u+zHta8fGeb5abh4wdnTOqSPM/QLoWClJSp3UlIPPHSKpA8pp6ECnvx5g7T6RgRA6VOYUWkYFbmXNsZcffWLsHDZt0wm9bkWnKytoxlx29865/x2gsvogPMn56x2m7g5ClPF0uyKDFCc+twn8ViwcmnD8APvPVL32bRthwdRZYXDYXKGJc7rE4v8EZQ7DiUTA8oSCipKEaH3Di4g65HqNE+y02kQNHMNxyfN0xHGYsLA3VGMdLsbkn6Ac6NIpTjv/8L9ZWu9LfU1SB6pS+EYoyUdYlxBoHGWYMWl0NmTAzDeBmYyMuC1bqjVIIQBCqTSCFRwqJVugliByTggkcIRQhpRZpn6Qzy/KIlUwJrLJ1z+FDx+9/9EYv1hoOp5OaLN7n74iG3DmryTGOsQ2U5zUpydG/MZHub8/n7vPX2BKUgesEn7xrG5W1OnrR0bkmudogC+vCQ27clWVkwms0QeLLYsTw9IfeKajpisF0K5gwpbBN9gOCJ1iG8YDu/zrff/K/Ynh3w4f13+PTR99A1jKYl02nFaFRRjQuqUUU9G2NdTy5qikwy9C1EnVbyQDf0mFSzg7epAEBnGcopYoh4IXFasH1zn7ou8UPP6y+/yd5mwv0f/JQidwx6yVqBrTSqyNgap9rL4fyUF3/5FfoffYjPFNlByWtv3uVilqGBQipscKgilQ+06zlESRQ6DdxYhKwIl8GaYA1FXqP0iDwmP50IGkKHVBm291SjMWU5IRIpcsH56QrvNphesilzTt1AtC3jesygSryo8P6MWwc7NJ2lpOeodQTX82oOubVkrkMUA6DwylNMOuresneYU41yrDOsmnOazYKuXRNiYDAkCkMI+OCR3iekj/dE7VE5FDVUY1IF57AhmhbnBlbrJav1ir5v6U3PZrNmXEwoq5KDGyVGeELYEPRzjHTkxQQlLHFoIKzJYkNRQJ6nIgcAX5VUdc1kXPPw3vv0fU9kydA2ye4xmuJ7gVabhFzSOTduHbKzL+i7nmazoWl7mralWcwZ2g1KprDVet3StgPtpmcYDNGnz5W4fAQUIp1Iys+wRyHhyKRUvPbGVy6H0L8Jsw806xXvvfsJN28ccqEa7j+5x82dmvtPBE4KSqWweY7x/jIgJdBKwiU2SstE33DWsrczZlaXXJzPKXPNpMjwwbPsPbkSeOd5fNowm63Zntaczltmk4pmOScvcnaG59gHT3G5YprBdFay+8tvcvb8iGeryIcPR8xPz1FZwWTqCM2G9370Q2KQjKsRzsFivuGFF7Yw5ZT1xRJySZU7XB/JCokYK5ys0NMpX3r9Fe6+/TX+5E8e8eRsyWx/yvGjY6o37/DsqKO8oykLRZaBCeBcqgy90pV+3nU1iF7pCyEh0s1P6RypFUJJylwTQkLFIDTe9FRFST/Yy25qiVSBvrcQJUWe3u6mG8jzjOBMGky1RkqFtYaI5Oh0w+FBxUefHPNnP/qEJ2cr7rz4OkJrvvzadepS8tKNPRbrgVNh2dsdg0h+urOnY7Tfpm3WTLYXZGXOYCyrecHyuWBvq+T05JgQFDduTZnbh0yuHaEvUVJd3xGCwQeotraIg+Px0yfMn11wcb5gO0qWATbWgQ8IH8lCyVdf/jbXZns8P3qPdz/8X5nOWiaTGePpiPG4pB5XVKMcXRagKoTIICqadUddlAx9h4kDg7MUeUEIA303UKicoh4RosNjiMIx+J7WdYggUF3k67feZv3sjPdWH9OIFZNC0wRFyCoUmjx6tusCHQQnAHtTtn7xa8w6sEXFMJsRdUZOwSgrCWFAtQasxEpJQKVmKF2grWHoHUJGvBcMpmd7qyYqgWk6pJYURYU3CuMcuRaMxzXdEOi6Fhk8MQqCjzRdxvHzRzTtGdt1QVGO6R1ctGsq3/PGdc0ku8n84phTs6Y3kf1ak/vA+eN7qOlAvnUdWQaCXiKKhp29mtu3r7Gze8Dz0xXOGaztUipephpSIQKSgAgeEUJKpMuIiwEXubRReGS2IZgF664gG2qGoSMSsM7QdRt60yGkYDxRUIGPA4KAziMUa0QmQZ4h5TFlEajqmrreBkBrQfAOZwtG9RuMxhMujh/hTY/tBpyPHFw7ILiBvu3oTSTImnoyohoJun5AjybMSFaWdr2kXa8uLSSW9bphtWpYLBpWiw1t24ILn59UKiUvV/ie4MF78DGdWuosu6xW+itTVEyn2h/+7D2EgK9/8+v87u885CefnjEuc3b3tnn7pQM+fLamOz7HeH95kqwTNgnSyWyI9M6S5Rm3r+8QQuD56ZyLZarCHVwC/Q/W03QDZ+uB2bhiZ2dGJND1PS/c2GM0qtlcnCL8R1S5xYqAru5QTV/j4PqIay8U3Lox48GzAz74+CGPHz3grZf2eXay5kGXkv1VPWazaZkv1xzeukF/zzCcGLIdgRCeKDxZNmG0fZe8vs4rb3+Ja9drXnv1Gqt2IDvYpm8Hzp+tKdUE7jXcvp4jo7psFBOEwJWu9HOvq0H0Sl8IRcAaRzdY8jynrkqE0kgM3nmElEQ0xgayPKfvN8QIZnAoJQnR40xMNz8hiUriXfJDWp/Wo0PneXy65MWbEwoteOXlPd59cMZoUIRhxesvXeOF6xPW65629+TS0ywarHHs7I1pNxv6ZsyoKDg9/phv//II61uM8bz3w47drRdQQmDMhsloi82mRRQP2bkWEELhjCW4NOwJKRBG8LMfPeSHP/4IAihjMKszdvbr9PfwIKm5e+2fszP9BY6O/oLHT3+P0eiU/Ws7TKYV48mYclSQFTkyF2lgDpfNPUKl08K8QDrFsBmw1uEDOBNQMiNEjxcWlKftlwzG0xpHP1huj3b58sEdHn94n7VtOZmsWbqWB02P1jW1GlEgGdcS8pyTecMQc0KuEbsaq2qkk+RBsC0FVlZU4xl1HJi4QDvJcdkSEzKa5hQla/og6DcL9nZ3CBREFAjFZrVkMJYyKgaaVOkqIEqPHTzteo0XEWEdQjqCUCzXa7p+w82bh8jOk9ExntWYgwOGo2d0jaG3jk3vKUTgIsCDi5ZDK6j7QE/HIN/npV9+ETk+SUGvasbe9j47WzfZ3+up6gqPx4tIVojUZpRDFJYYY1pdR4vOkiXEuYBxMeHc1QBqRQglPoAZ2rTGj55Nv6Y3HWVZkmUSVXdEZdAalGqRskPKQJUP+NYgjEJlkrqqGYxBCEGeS6RMXM/9g0Mmo5rz0+fc++hDmk0PRc1mdU67WVKMZhR1RiUyZKYoVEY10eRFhVIC07e0zZr1aslqtaKajKknG3S5QCpNiIK+HYh4dCao6+oSQO8YekMIgcVijdZnlygrLtuU0jmq857nT57w9PEzvvKNt9i/dkiR52RKcnSxZro1473Hc47nLS7EZK8REutTMr3ONb312JhOXkda8bU3XuDZ8xPWixVNN6CIbAZHFIKhM0gp2JtWBDcglWJ/d4sik1ysWw7v7PHe0ZpJuMbrtyvGxYayPIAA+9dfR+U5By+UHBwekevIZtnRdpKX7+yzee8Zfb3PvG3RKIbOIRVs7e5xfnJCu4FsnBEkdLGm3rvL3a99g639HTINL96ZcfR0xNwMvPLWDd5/95T79+a4awV3744IXiBjJFfiEoh1pSv9fOtqEL3SF0SRvNDUeYbW6UTFm54HD47ZO9hhOisIPtJu+hQ6KDRSRIpMMViP0proE1pGaYlp+8RItI4YBFFo2sHxwvWa6D3GCjKp+ZVv3OV3fvdPuDg655HoWC9njHLFdGeK7R0v3pyxXLc8fHhCXu0SyVnZCw5fatm/echiPeDWI86fLbm5N6Ftl8jL+sLz1cfceOMCZELbOOvIC4UUgjh43v/un3DvgycsTtb8t//df09zcspHf/EOTx99wps37/Dw9DHTva/yC6//MyZbO3z0wf8G+jEvvHiT8WREURdkuUZkCtRnGB2PN/GSAenI6wrrAr1zSKUYVROGAbzUgGewa9COwTU4YemdxRp4ae8Wd8tdHn3wARd6w3G5ISIZ7ewgxg4RBVkmqEtFNZlwZgxzG9kpp+yUFafrNV054uV8m7dne3xw+gknfct+MaNwiuv1jPc+XXB0Osc5Ab6hnmicC5TFiLYLFEVHXY8g9pTFDvg5PkAYPD4G5vMlgoG27RHRo/IpfduRKYeUiqoskGKG0pY25pyfnbFvW2b1hLi7jQoQTEdjLVFIcqX55MKxaBuu947n7ZpFt2H6xg5b11usybCDxNqItQNm6NJpnIrIPFJOk0kxzwEZiDGB4KNylKNInqcOgr5PjWBBGKKaI0IBQoEySOWJWNp+9vn4AAAgAElEQVShYbAtVVmjZY7OHKK0SBVRMhCDA+FRAnShcCESg0fiyDQpFa8lwiTbSgiOyfYO5WhKVm/x5z/4Mz745COapsV7KOtAWQ6U5Yq6KhmNa3a2d5hNxygtiZOSsD2j63ZoNg39ZkPfD5ydX/Bk9AShFBdnS4L15Llke2vKeFwxDAPNesMwpMG82fSfr+PFZ0UVMbJeLXjw8X1u3jrk1osvES/birSUWOcYTAoWrjcb8kyB4NLvmb7P1ihn3hi6LnFFv/TSNdq2Ydl0PD66YFZkrHqT1vbOo1VqRtrfqpjNxmx6SwyB6fYOy/WGi+WC1QpONjk+u8aXb2/x0ttvcvut30C4nOb8COXXZFKi7AWu83z3z37GEAMvX1f46Q7vPxPYLkAMmD5S1jX5dAvTL1DWECmY3XiLb/zSL3Dj5Rv0QVCQqArX9qc0T5bUU8WrXzngox98wGpZ024cdqoRCKoS6vIf6HJ9pSv9LXQ1iF7pCyOdZQTrGVwgy0GojDuv3MZbg7M9UqgU3siKhJgJHmcd9ahkMOmmL8Jlf3aWgxQMTYd3kbwMbM1yhMoherx3eNNzuFvzm7/+C5w8e8gHnxxzdLokRsWozqkyxWKxxfWDKVVZMN4ec3zusO4Zd79S07RLus5w/wNPra9BEGw2DePJlPVmTnX9MdNtlU4pnUMWGcYbshg5vXeP5vl9dgpHowx/8n/9axanTzBDj51VnHbPqOT/w96b/UiW5fd9n7PdNbZcq7L2rt6qu2d6lh7O9JCgOKZMyhYEyIAAQbBhmC8WbMAPAvRg/Ql6EqAnAQT9IAGGJMCWJcICQdGUbIrDWaTZZ7qn19orK9dY73o2P9zs4RAwBUOkaQ6QX6AyC5EZEYiME/d+7+98lzF6+Zyj42/y4GlNtfoDXnnrJUaTkqRIUIkEFYecwxhxzmF7h3WOSZbSe8d6vcZbB9FhVEZajFi7FjPKyYuEup5zcnZImo1xWlJs4NWdq2yLkh+/9w7VCKpMIp1BJ4aoNWawoZAYQ4xDduSmrqlsYF8rbu9c43z5AUUHN4oRtqpwwTKVK8p15IXde7TP5qi6ZVyWNBbqTQ/RkqWK8XhEXbckaUHvFH3nSKjpbU9e5vjeE51nMsrQZoa1HmMiUSiUUkgEbR9Zr+dMJwURiS4KYj9jvlowMafsXL1CtBrZrNC6xgjL/ihl2WpOe8fRcU1y0eIlYsR1nmZjEF2K0SmdXXE+f0ofNiRlIFcR40FK0BoIHm8j0QfSQjCZSbI8oKUgOHA+EvBEtcH7EyIelRl02g0NWDHS2wrbj4h1ICsCSttBFyn9T4LfY7zQSl5kKokIWZKTZPnQC688REMxTpGxgjJhNN7i6sEBR88f8+TRA46ePaNuLY31LM/WTG/MKGQkkeWF5lRjTI6MAs82MoRhKz4OW9mHB/t8uPsxR8fneA9SSGzfQIxINVyEaaMJ3mMSwyfVnp/UfLZtw5MHD4khcvfVl8mLkmazQknDprP0PpJnKb11bM+mQ01p24AIaG2Y5RnGKKQCoxRX9yZ88d41ThdL6k1F1fT0zuN8wF1MY40a/maPjlYcLRqiEGzv7OJaz43r11hu1rz54m3ayvLaL/4yn/nC27z7L/8xKb/D/mtvMr19E3tccf3GiNRP+fiHH+J84ONnljeu5VyfOFw/xnaOpo+ImCClIkvH2HqNCwmZPqCc3GO2v4/z0DlY9ZAqwd7+hIePz5DdnJ3pLrPdqxw+O+PdDxu29rNPrju5kANf4hJ/rnFJRC/xM4EYoe97ovP0DsZqyAy0bTM43sNQJRiDJxKRSqFEQAiD90NdpVJy0OWJiPcKpSTGGIwJRB/oOotzHQGNlp7UCILrCdHx6r0XeOnudU7OFjx8MudsUdN0gfcfnvHu/RN2pgUv3AWVbTDlU1qfYTca26Q8+9gzm85AOnyoEGqESx5w/c5wshBqmPBqbUiMAGEJ0mFSgcDz4q0U2zxnMsuwszFiq0S5HfzpPar5+3z993+D1199DR0UP/qDQ67dvk3nKmTueeHeDGEknbVY29K0DUIorihJog1VtUFJhe/7IddTGmTwKDk0wdjOkamCGCOiXvPG9k3STvDeh++xUD1LDZWtMV1P6jJqv8HGgFGK0LVkRqPV1qDR1Rmqi3SrBuMFt4ttXt+/xaY5R84dEzLylWV35EjHO5xujXjn4Rm2fUyih0OVSTVSJ/hYAxIZe4yWrFYtibp4D9sKZSTjMiMtZ/T9oAvt+5Y+ShrbYkxktr1DMZrQbRrq1TOMb+h6z6n1BFPhYsqz5RzrG65uZRRKs3Hw5GzDk3UF0bNlFMIH2o2jXikm2ZjxaJssn5GPRpSzhImNJD2EIBAStIx4F+laj+4jxQRGk0CWDjmv8cKlE4lE7RCsCM5hnUTkDdmsRwtDEGuarkRFhyglSRIRUl440Iepe4gOFxwxSpy3QED9VISSUgYpA53VRDEeYqV8ZDrZYXd3n5devMfZ8VMWZ0c0VcN3/933ePjxM+ytLR4/fsS9z/8SR89XlLlgPN3iylbO9u4WaZEhkFTrGts0aKV56YWhdGE+X1C3G9ZNy3K1oXOB6qLHHuX+yOfeOc/p8THnx2fcunuTrZ09pBz0rV3fYP2QZVo3DWVe0FmL837ILQV66+iNGDJMhWB7UvArX3gBoyV907FYbgYC6uIwTfeDfGfolgokOhmOMy6QJppHz454683XMFnP7RvbXL/5Itdef5OtgyuY/+xv8K1/8Q9YH36dO2//EonvMOERWxPDnRducf37h3xQVdSdZm9/RJZK5uuKDzeCKAbSLbwjNWGoQjXXWDaK9z8+ozy4jo6gIygJxW7OztaYs/mGJNni6gtbdMEhtmccLWB7fDFEvzzDX+JnAJfL9BI/M9BKDTov51B6yGKMUeC8QGk9iPNdRIiIEhDCsO0OGi0F0XlQegiJ95ZIggueLM9QUuJcj1FhIDquRyioGkuRJaw3gTzR7O/vsr+3Q9t2IA3zecXh8zOquuPweMWVa2e8+kZCaytyNebRxy3GX0FLzbpbYK3nZPEBL721RqfD9CXPSopsTJqlCOFxvuXaK69j8pyzo2e4tse5HC8Eq8Up9qQn2F3ydMK1V29RPD3h/PwZyhjqasNIJCQ64cHhQ84+LLEqodcGF3qS0nJwQxPiQNjbrmZcjvFElssV3mpG5RTnW5pmNQSeI+mrnhvpLldtxnvPHjEfC1ZS4LBD4Hvb4SU4GWlsSy8FQSm2pleIcdD3yjZidELsLLpxHJgJ7dmKk/MjkqXnzv4LzEY5MqSoYkY+rhHxGWmak2QZSmc42yAIbO3s0jWwXhySmKGNpusF6/Vzdnd3aDtJ21mq7hhjNFpnJKmhrQWzcoyzw/sc/QYPrOoe7WuQAeEkx2crVJLggCLR7I0URiiykHC8bMm1pLWeJjIYXKpIvYjsXDXkJicxU7a2rrC1O2ITJE0HPgwVmFJC8B5rBT4K0jxS5KDTCw0zESWHYPgQBUFZQlwTE4EZBUoT0cKiRE30Du8ErtOIThJFHHSfSgAG57sLo5SE6HGuRRBI8gIhh7grKQWpkUP8ps4RfYswBoQiK2dcuzUmz6f8m9/6TdbLJcvlkof/fgUi8uGz30OqgnJScvr093jj9Ze4fm2HNz/7OQSQj0ru3ruHElBtap4/+pjpJCVNb6K1YbFccDqvuf/oMe+9d5/ewaCQDYQQ2ayXHD1+yng65sqNGySJZnDdC5QUjPOM3rqhaU1b0jRjuVoDFy1P3rNsImli2B7l/NLnbrO/N+P+o+csNi2H8xayXdLMk7oN67a/KHOAUWrYnWaoJEUpgZSSRAwtZmkxYnb1DrMbL5FkEgRMr+9w/fNf5lv/y9/DVQ+599l76NiiipfZvb7DS3sJR2cN6WhMsXUVpeZsjxTTXiGItFWNoCXJJWQFMr3K4WHP08cbXu4iuQMbBHsSjBYUW1NWpwtC3VBujbn50jaIyPM5mAwyLeDSrHSJnwFcEtFL/ExAAL31pHlGVg7RKjFEkiRBKagbSzZKsa6/yC0ctuWU0EPvdIS6cxRjg7M9vQ0kMpDmOX3Xo/UQeq+URIhA8JE00STjBJNCvW4oS816VQ3dz22HlA3jzHDwqVs4b+niiHN7Tjmd4iLYTvPkg0BGwWKx4mT+hKgbRrtH7F41WOcwSUqRlz9xEQckUhnycsaNlz9FtrNDvdmwPrU0J4fsb+2x5haLQ8uyP0ZvLfnSr7zB+z/4gGuv3eD40Yrn375PU7eIRLI8jdx69TPIrOSH3/8eJ4tjXDfhjbbBFClJYvDegvAU44Sub0hCStMvCFSYIg6GL61QXRy2ozPDAkflW8ooECFifUAoRZoa+ujQKqKVJE0THj58TG17VGexE4Hqeq6ElD0zJtMFO/ku13f3yElRKiErckJn6dYrpISu60mSFBEtSiaDeUQabLdB4dE6R3jwomE6HmOMupBYgO9buqpDl9BZibUtWg01kut1jRKBposoJbHOEG2NEQGTFKhEcuuFbexiG18fkqWWzGhKk7Gb9pzEiJEQvKBbSzYLS9jvsHZDbzsCjrQIlKOASSM+CKSMw9QziMGoRMQkoE1EG3GRmymRApSIoCCKi0xQA1oIcjPoQE1sEI3FNRpvE5SHvq+Josd4eeHEdyBA6aGONgRP11QYs0CnKWgQ0Q3ZmjEQEUgZsG1NfxEMn2cFTet49TM/x/HJKXaxoq8rrl+d4ELL8dND9PUrlFs3eXZc8e4P38foks9+4c3Bua4ESZKQZQXr5TlaRcyFbKMornD9WsKrL97kc6+9wPmivZB0DDsgZ0fHuM5y+9UXKcvxRRrGIK8xRpFnmqQ1TIsUZQybpoEQh7CnEAlyuADYnxZ8+VM3mU0LluuG+fmgV7VkXL1xl9BtCPOPWCeaqh9qMSPgvEcGR1nkSCLbW2Ns3xKFQohIOU3RviWu36d5/E3i03dpY8nv/s4R6827XL+9hRf30fkut+/kPHp+jtOCJM+QdopWJxjfEToPwZIlcjDxZZp0vIVbS5arjk0biRtBmUCRRHIhUHmC0RKRCryGVFSszzdwZZfzJRxcBa3+/zpiX+IS/+9xSUQv8TOBCHjnyFKDcwGhNd57XPA479EKbNsggWADJlXY3uOiGyYsMZLnGVppqo3HZAW2H+JulFJD13o3dNF750i1BOeJQqPwiOjo6oGgFmVCXhik1D+pBE0zhXCSgxsKG1pG5ZSnDxRuVaJKzXpzgpAbvDjnxTcMPjpc8KjgAIl1zYVbHhI9BPL33bDFqKRGGI9OMmaTX6JbX2Oy1zJvv8vrb2lOH/6Yl3/uFmZc8uz9B8xeLBjrESpNOH204PjxA9ZdYH1+Rugsm1VPbztUUMQYsHaYwnrf4axh3SwJVJRjgZeWxnm81ax6yc69m+zmnuTxCQqFD5bz1QLjwJhhUpUmBqUCqdZ47+j7HucDVbukylMmCD57+1WMV+TFmN3r+/RuhSxzwmZFd/wRh+9/m5NnAmdHJGlOJFI3HaHvKEcpMvSkqcHboXFIyHSodI2Cru7ISk0IBhuH+VpnA303J0kUq42jWp4S6EmTHOssSoHKFCKbQezQKsW7Dc531NExKkuSIiOVgqv7hvI8MBpnnC4q8IJ6AW3bEURPwFLV56yqM1xoUBqMFOh4IcWQn4S7M9RrajFUr4qIuEja/OSLCAMhjQiijygZUYCUERUs0Vb0NkM1oCYSoQJ93xKiRGoJFxPSPB2jdYIUw23W9shgsX1LkhWkeUnXN/SdG+pKiTT1htY6bDboPUPv+Ou/9t/ywQfv8uzJE9Znx0iTcGt/j5//T/8C/+Z3f59itI9r1vzBv/09bt7eRwqYjCe4LMNozWR7i2RvD+86etdhhKRvWiBw8/aL7OxbtBp67qvNmuXZObO9bWY72+ifYlVSG/K8JEsN48yQGk3rHFXdDPWeDH/r1Gju3dzh7TfvMBkVnC9WHJ/MCd5xuugwowO29/aoni+5fmPKlVnO9x+cEolkieTq3pSyLGjaliRJyNMZRiuqtkGN9vFuA9UTwvkH+NU5I/WEOzclXz2a8gffWPPpTcPe1YZi/wp7125ycO2Y48rh+g6RGZyMJEkgSpBFxAlBYy0iCkYmMNk2aBOxm4p1HGEzEBKmKhLThN4qpnlC7WD+4TmVbyEp0Z0iSxOC/7M8Sl/iEv9xuCSil/iZgBCCvEgIxME1fxH9gneYLAXvcb1F6IQhmDDS9xYp1TBhAQQB27akCXjfDl3UJiPPzVD/Jz0iMYQgSIucrl5TNw3jyRabdYvOFONi6ILvuw4fBG3V4b0jzTQkKVs7CYLAZt3y4McdL7z4ItZKFnVLdD2TvRXjaYHzYSAFcZjcmCSj9y0EB8LQdS11U2FtgyClqzry5Cu0i8+T656V/wH3PlNTrU5x0dM4x/zwhHI7x5512CTjZL2gl57eJdResGmG1qGr29sYnWI7T5rkjPIxygRO50/RRrLZ1CACnQ1DBaMUBOmZx5aPF6fsT3eZPVD0ueRk04BRpEqRmwInwMUNbV0zGm8hekXsIut6Tu473KpCbgmyUcF0tov0Abc6ofcL6tMKVW/Q7TE39hWPwoz2SU0UDqXLQevpPbbrESrBOijHI5aLNcEuMakmBoV1jkIJXAgk+YiuWeMctO2G6bikLFKC38a7NcE5skxjXUbbrEm14Hzh6Js5+JYsTbDOUecprUnQfSRPHXGUYHpFk/Z4K2g3jiwVHFyZMJtsE2SB0hqpBGmWkwpPxBGCJcQw7JiKi7X9SV6mAIgX6yKQCEWMQ05uFEPTzk8+D4CMnmA2OA9t7Un7IUN0mLYGhBrImBSSUbaDRKKUHkxrXU8xGmHKBJ2W9H1NiAEpPMENObx5ZvDOsp7P8VFxcOs2WbnNpz79GV577VXW58c8+OAjZp+6ypuf/QzTUvCNr7+D8o5b13bZ3t5icfyE5fEamY/Ji5LxaApC0FSeUTEZWFXSoLIxtq1BdQQBwQfaqkIKwWx3mzTNfuqVg9YJo1FJ8I5ZkTJf1RwvNxRlQVEUnC03pEry5Tdu8plXrqPTjPPVhqeHZ1SrNWmacFz3zA5GJEayaFeU2yk3blwBAcfnK/YmGdNRxu6VfTbVhkRLsrTk+PSIVQPZ3i7YDtHOkeoUFWpSThglnmkemG8MJ8eeMn9OkA8w4zFbB7eoH6/o10vWixrftBRJwKaaLkAfPXXbg9ekLqFLJ5hRyvMf/IA7P/82wQs2laCJkZ2RwkrDk/tnFAf7BFLcYs7p997j2lslbnmFxWn3Z3F4vsQl/kS4JKKX+JlAjBBcBBlwPpDGSJYpfD8QUCklHohdhzIGZwNaSJwLoIZoohBAiogLEpOmOOswaY7zYWjqcR5xMUHoW4tAEXykqVaMZzld1yMRGAWmzDCJwY1SQhCD5tKMiAwVjtVSYcI2W3tT7n90xGpzRh0O+dSrI2K0F4apoS+8dzUBgVSCGKCpK4T0hNjivMdXnur5NVy4R6Il6/YJOzfewcdjjo/mpKOU54+PqeuKum4JjeN6dsBUJaw6uH37V6naHkJP6J9h+4bE5CiTsDXZpUjH1O0KGc9ABso8p+sF1aoiTVIInqo5o+5rnncHfOrKXW5uX+UHzROsD6Q6HbSK0WO9w7qaTDpuFDPuju9woy/5cLPixvg2N0bXmOl99vbv0q0X9PacR49/wHYRSbSnnE6R2/v4eAN/VtPZzRBnIzVNvaHIE5arit08x+iM5WJNYiJVb5DWI5QBKbG9x2hFiAopFM53SJOzqS15LhhPJ5yd9AR6Ml1itCLNpjS1Y9O3aARBlxxvPJMURuWYzhqibkn0PiO1Ii5b6n5JdIGRDkynMCs1UWrSZMLWeEaaGGxIicIjpCJE8NFinb8wsg/tX0MDUI7WI4wyyGjwbYuPNbJQVO0CH+NFnNGwRmUMiKwjqpSqdqSdJy8AxUW1bUTG4SJuVT+iszUxDJIV6y19W5EWY9pmjbUdCPCuR0lBbyNd26J8TWokNgqEkTi7QUuBMgX5aJ+7LxvWqzUfv/cjtDH8pb/yq/ziL3+FLBFIFSlGJTFIsvHO8NmLgugseTkCkdG2FTpNUCbBIYjtIGoMISBjYDKbUI7HAzn/hLkzvKYiNcxSQdX5QQILtF2PURItBL/65U/zhTdeoG4aThZr7j8+ZjnfsD3KOa8sQaUkClanh5jYIURCWSTcvblLSk+WSspCMxkVjMqCKARniwWnpwtWncI9/ib6mgMCwQqEfYa1zwgdXNu/gvOBzo1xcYvV2QavNUm6RaLmnC1PWFceJ0GoiJeWTdtTdR7rtpBbX2Zut9E68OCdp1j7nMneLcoXbtA1UOQCpyJZWfKj73zMrDGY3pGul7z82oQiWB5//zlnR8/+zI7Rl7jEfywuiegl/sQQQtwE/hFwheF88Osxxr8vhNgG/ilwB3gA/PUY41wMQq+/D/xloAZ+Lcb47f/wswyaQ+scUis2VYdSilFhsM7jhMd3jrTMAEl0AZNo2nWDGpc461EqIEIE64kBYvAE2xBRaJ0SfXMR8B3xfYV3gSIzFyf0QHB+0KG2PVLKIQYqStJUIYRm3Q8d184LPn6nZ2+yT9sE1oslLi45uA3FOMV5hngbwLqOphkme86GQUMoFSEGiBpBjq2u88LNr2D0mMeP72Pl17gyXnJ8dIqNHWfzc+qmput62s7hnGB++i5FvMFnP/Vfk5UzPn7wTQjnA1mMgapZMkm30CYORDh4tMrpbYcPFiEkWiukGjJ/ssyAGdH0HbXrmZVTwvwB2mi89RilkX1F4SP7oynXdrbRK0Xz5Iy7xRYv7H+O6y/cIyNhIhRqtWDz/CE2rCnyksn1PfTIsFquaGvLeTXn+bolzwvW6zlV0+CDwtmhIQgUTdviwxBAXkxGKIZgdRGhqTdkWUGSFCBTouhJdETIhN55oCJPBKPJDtaX1HVFIit2ru9CeoXF82/z1udf4fh8i/PD9wixoXeWiMTSMRmNqZqUxj8l05KtMUzHnkQ0bNqWaSIAi8IjpSZKidY5So6IImJ9xOgRk+I2UCBJ2C0PmJQ3sZ1nNT/k/gfvcXz+LnsvRWJR0ft+uNCJw9RTxkj0PTJpWc87sjqQzkAKiCJeENYhFD6KgHUOpB7SImSGEALvO2JwlFlB29eYNCVGSYiOmBZEKTAIynTEZjVHhB6TT0iSgn7TMtq5QZItybLA4uyQ+fETvCxYrQLr80MmkwnKaGSzxmTlIHtJUjrXE2PkydPnTMYTRpMSlU3JxobWDmH0xWRKNhqRpvlF09IfIoTAfL4ikRFTGNbrmpGUQx4ukb/ylS/yc1/8PMLVzGvHctXStg4fBTrLefxgiU6nGAXr0+fc3M7JUs2kTDByRrc8JxDAB7z3ICQnZ+fMl2s2m4rWp4y1Jxx9i0pUJImkq5/SORhv73JdL7HyCl4fsHERnSjWixXr5YIYPfNnc042ispHfCFpbWBdV9Rdgdz6EpiDIQvWtmBSTNty+o1vkB1cxeSaPBNYJzDjHJGMqA4XTDdzro8Fpltw+E7Fk6MUry735i/x5x+XRPQSfxpwwN+OMX5bCDEGviWE+B3g14DfjTH+XSHE3wH+DvA/Av858PLFvy8B/+Di+x8LIcQQQ+Mj1rqBHOUa6wMmMfTWYVKNs+6CJAr6LjIqc+qqJcs0kYh1g8PVOYfWajAs+JYegTYaArgYUFEizWCK8j4iItjeD40tWiNlxFqP9RbhwsV9BxLpugnrI8/+aMTJszmb5pzWH3PnpS0SndB1G2IYtr1dEOgYhumo0UMsjesuzFgC6rtcGf8n5PkuVX1O7b/GePKAk8OK0/UZdVNTdy22c1gbEV6jVUaa3WKs7yLZYj5/yOrsa2SqR0SNkpLWVkzUiN7XrBc1Qmi8F0hhSJNIVXdkWYJ1LW1X4XyPVCnrrmHVNVzZ2id5rGilQgnB7myHK63k/PQ50zRh1EiiM7z9pV9gJ5vQreZU6w3boxJlWx4+fpfRLENlI0azq3gB9x8fsm4aVl3H+88PeT5P0TpjunuHw8fv8NLdG3TtsA6886SpwVnLaJTQ94boG1oL0Tm0EhSlwQdNcBuMNlgLCIFWBh8D462MclJSVxLnPJvVht4tOX52QrQ1x6dLbF0xKRO00iA9iepQOqOzjp2p5nOvvEJe5BQFpLlAGIFUCQLQScLW1h3G/oAkyZmOrjLKd4hB4V1Ay8DW9A7WZtTVhm4zZ3UUOD475/r1F/mVv/g23/nGP8eMj3kWFoS+Y7ATQbxoDwKLyWv63lE3gamIqEFp8ocFmRckTl1oLDvrSYwiRocUXFzg9cjoUCIjSomJkiyb4X0JyGG7v7REb4eqUiVJ84wgAipLkYlmthOQIrJarbn/9CmHT89YVo8oi4Q37t1i/yBDKcnW9j7etdhqxcH+Hp2NRA/S9bi+IQqDUpKinAwJAnp4QZH4E7MSEU7ma5wquHP1Gj94/2uMAaEEv/DyLl/+3D280ESVkxUz9vYFAcPR0XPOljWrPjArLsxrsWVva4uySNjd3cNHxWp+RtO0SKXpektV9zw7OkNJxXRUsp+P6KMiBont55yfPMP6lHojEMoT5Zjx9i6LOqEODrFumS+WVJuOxvX0VtBlERs9882S801H23ry8RdBT4kMBQf5WFFKQdkp4vkp9vSI8tXrKB1ZLAS9FZQ7I9rH73GwF9hNMk4O5xydtCihIb0kopf4849LInqJPzFijIfA4cX/10KId4HrwF8FvnLxa/8Q+D8ZiOhfBf5RjDECXxdCzIQQBxeP88c9B70DLSTaiGErLgZ8AN9a+rYlzzPiMMQgCI/SCpkopAvE4PEXekylJBGPQGKtJ8Shtcn2PRFIswRCGGKeQgdE1usOkyR0VUeUApQkhGFC5gK0VU8fDTmS08PIbHaF4CRVU09DGx0AACAASURBVHO+OGR60DGapfjgcNHSeztMZAHIEWic8/hggSFayTV7jMRfIDMHPHt8yAePfptN/6+Q6zVNXVM1Q+6ld47gBSAxGKbli3z6lf+S9WJOUx9yfvLbaLEgyxPSuMX29j2arsL6jsXqDO/AewlR4ZzD+R6Avrdoo5BSkKQpIhnja8Ha99zZu8FeuU0Xz5EiUtUNz8MaV26IYUNZe2ZNRvfR9/jO42MWqyVlmbD11ud49wc/xEbPwfQurnZ8fPyU9XLFk/mSw6bm8WaOnI4Z62soIcGuubIz6ASrqmV/J0OqKb3zRLckBoVzAu8skY7JuGBTOU6Oz7DeUxYJiAKlJEoJ0jSj7T3j6ZjEKBZrQV2viULQdy2zScHuzusk2TbBLag3DtlbrLU0dYtza2L0xOBRCFQ2I9m6ytVrN7l9+y2mRU6w5+yNdxilv0LXtli7BhSpPqDteubr59SbJ1TLlnUL63oOnUeqfV6591lee/l1nj/4LjrpOaof0JmaqEBwUduohjzJiCctOxCRtoEYQA/Ld2gmkpEghgspiQDXU9U1vhgjGYio0BqTavqgqDcNxmiMMWjFEM7fVvgIRZ7jukgIDttvCBcxSs61uK6nKEcIIdnZm5GVU+69tObho2f0NjKZjjFGo0yG7RpsvaaZPyZPMrJEkRY5rU2IwHwzWLaU0RDFH+mdj3EgozEG1usNi+fnfOfREWUIjLXi5k7G3VHL4uQJ5fVPk2Q5++mE6XSPopziXcdXv/1jIsNFX7U855VrI3YnCVd2x4xnM3xUXL35Aqcnx6ASqk3Fw2enPD1ec+f6FZKs5MFJy9f+r+/w5ss3UdIj1RX61rNZfogpMshuk+mcxDtWp6f41nF8cs6ysjid0iSCNvR0zrLpOtZdhRS3wWxDjCQejFaoYMhzQ1KkSOXYfPxD5PY2YjunXkds77HNkq3tSJdFVkCd5djUESpLqi+J6CX+/OOSiF7iTxVCiDvA54BvAFd+ilw+Z9i6h4GkPv6puz25uO2PJaLDY3NRUxkRcmjJCd6hlCIZFYMLXUuCuyByURDbjs56pE7hIuRaaUlmNH3bIZRE+khwFq0kIUS89QiGrU1re0ySgYq03SAH0Ebj/KBZSxJF3/VkmcH2ghgM67PA/tUrVEtH1SxY1Ye8fCNBILCux3Y9MQhCBKUkeZojL0wkwQ/Z/L4bU/gvMdu+wdFhxccPvsPR/F/S+Sf4viVaCMETwif9MxIlMramn+FLb/137O3c5gff/d9pm6+RqSUqHUFxkxs3/hrbt3ax+Y8QUtD1PU1tGRdTbN/T9Q1dV5FmmizLCaEnSXJUhD5KgoF5W/GSV+yNtnnerZkVJZkusGHFuj6l2qyJUTPt4YP/41+T6IR3P3yIuV2wiKdcu3qTr37ze3z9f/sniDxhujXh9vVrvHzvDWZixrvvnjEymugEPliqyqJZ4twOgYhSGc5Hms2StMiQIiHPJV0/g6an6TrGswnejRjrSIga1zeEIPDB4e0aZRRClCwXntBU5IlEiBF5LgjCIGRkfnqOMQ1RJ9SbmlxFxvmEGOFsXtF3p+hJybWDW3zmc68xLrcYj6+xnB9zfHSfssjpQ8LZ8hQ4ZnuU8PD4PebVCYvmIUrWyM0Iayck8iav3P4FPn3vbZrVKV/9vX/G9975HcLkAWJ6NlSBXpiZEKCUGLSgRFThUImgraBvFWMMiRx65PtQXyyQocEohsBsPGbdWrI0RSUp3leIAAiDoKOtGrwW9Ao6J9BSAB6Tlpg0ZVNtwAXyYkxwnryY0dQr2rZDSo0xEqlzzCTl1dcmWOfR+YTYV/T1GX21pFsvMcojQ6Spl4OBqCgIbQdha6Cdn3yJ4sLJDxdZ/3gfWK/X9OsK7xxWKJLZmOvXt/jxw1P2/H1e3r1LUZTkowlToRAC3vluw7zq0YnEdRsmOuUz925zey9lsjVFmpzgIzvX7tAHTdfWtPM59XpD23YsNjXffVRx/dZLPH56ij35mCvbitm1V8hKTX5li+gLnEzY1DVnp3PW8zl907CpWzopaULPWjg2sQMVCKIllTmYa8QQkUZgUjCpJMsEuVGkZkSaSxLb0b7z7zlzGWG8S1cHSnvEeJLRC8Fx8HSFxG8r6m6BFJdBopf4849LInqJPzUIIUbA/wr8rRjjSvyUrivGGIUQ8Y+98//z4/1N4G8CbE3HECNKQwhD40/0ASnFYDYKn2Qgxp/UBGo9kL1cDsYQeXE6852nd4PpKcSAIuJCQCcJUUS0NsQQBqKoM6LQQz+3uSCqLqJTRYxDg40Ugr7riTJjeR7oNiVOaxYn5/R2w2hWcfP2bWIMdP0wbYwEfAgYY/AEQt8OJhAlwY2Q7RcozYsoJTk7f8Dh2W+y3HxI8D3SD7WdEoEUgFBIkbEzfYtfevtvc/3WXY7PPkab+wh/SlJskSRfZP/qX2Y8vkGnHrJqO6qmoUjGZNkQcO9DR1kmKO1xricGCSikSAdHtTL46Fm1NblKefnWizybb9B9oF455qeRjdd0FQhboX3Kay/fZVNtOPvwI06X5/z+7z3kl3/xK2R3bjF/7x1iqpgHy9n8lKfvfZ9JWXKwtcXVnas0JxnP+zWTaY5rJ5g0IUtaqqpFa8iyBG89xydz0tSze/UOm3aXw0ff48YNyNMR3itEbCiLDCdz5ienmDwwnYyYznbR2mOSEeuTDp0L0iSnaTrqTYOIHVjPbKzo0j3qxYJ1sxkuAHxL78Fv1sP7ECWL+QnzZU2SbDEdzThZfMhHJx9wXj1GJzXpQnFeV7jYEAlIEnR4iWs7P8dffPuvgbV8/au/zf37/44NH+J27kPWDPFjAbgYDg6O+WEVCS0w2SAL6KqMW9tvc3v3dT768XvIZIEfvYfTK7wQCKmIEZSQaAFV1TDSGXk+QgSHtY7F2SmJloz3D1hXm4s1mlFVlrA8YmdvjyJPqBuH956IBKkx6YSySOmqOYJAmhmEUOANiBqDxZoUFcasjp8jYiSiafqWtu6GCHtn6V0k6BnxwpUVhUAQEOIicB9ARISA7SKnSRWVgLmFz3/5i9y8ts/v/va/5skPn+Dz7/Cpz/0cRVkilKbbnDOfz4lCkiaGVEXefOU6V64eEJWnjwVu06MSg0wyyukOQkj6as12Ktlkhu8/WLK1d4P9/T3uTh6xP3LEMKYPgrzcx3VznEzZtJYHHz3i/PQ5MQaCTNg4xdG8Yt3XqP0EkQnGOzN27r7I4Qeaw5MJTlh2Z4pxoZmMBEWp2R1JspBhioxsVNA+f0Y4P8LPrrB9/Q57uwYvFW3QSO8hSHQa8aWg6do/4VH9Epf4/x6XRPQSfyoQQhgGEvo/xxj/2cXNR59suQshDoDji9ufAjd/6u43Lm77I4gx/jrw6wC3ru/H6D2d84NBxnkkgiA8vRvMNF1rh5NfAOvCEPOkFVqZIatRBEK0BBfxAjrrEEIiE42W0HUOoQTRXmxZSoXzkVRHBEMNKCi0EmglB43nxdZ+1zlUJrj//hITDhBCIpLA8vyEG3dztEnoXY/3Q9yTcxaBxAWHte0QYJ6mhCixy1swv4sbSw6Xzzla/D6r+od07UXuKUNRztAwA4aC3a0v8vYX/hY7uy9zev6Yxx/9c6R9SJpeo5j8KrPtX2Q0HlO1C47PHyOB5XqNTQN5UhKCQyWSECPaJEQiIToECm0SFBC8JWpB53rqtqZA055VNF2DrwJNH9h0HVXVsFqccNZaankAweGvJTw6nJPrjGs3XiYqzYsvvU4tPHW0bO3vYDJDWoxQnUMEz2hUkGQarQJLV+L7mjSb4LoVUgx96mlm6NyYJOmwXjFfbijzhNn2VRaLjuAcwbZAz3h3RN/t0NUn9F1LV1c8P14QgmU5f8bdnesoPaZreoyMeJ1SbzqCX5CMZlRWkIqAiBJtFKOtLRCWqjvj2+9/E6kTRuVNXr/zF8jKKWdH3+V59T1srBB2yLeNQAwS4bYZ5T/PL3zhv+Lazg2eP3qPr37ttzhrPiTfPUWVpwRarB/SyIb80YtA/J8ykEsFaQajMVzbep0vvvKXaGpBoRzv/vBb6O0Z+69FSNygS2bY7fbBYZ1nU9XUtSATLUpKtrf3qLuG1bKinJR0tqeq1gTfkxYJPlqcC+R5jnMebQzT2ZhqvQIiWZbTdR3RS9ACFyRSj0ELDB4BTLa3qTc13vW46MjGY2Lw6CTFVt0fJgPE4YLtk+55IcTFcFdgkowvfOnLqPPnPH36HInizS98kTzNeOvtim99/Ru88/0f09Y1B7fukBYjfLXA2cCd/TG72zNubo+4c+eAxaZj8kmjl87wTYdJFUkxJQRBXy3Ji5THDxqEzNHSsTp7xs6NliyxODlDFoqmc9SLiqOHX2fdJ6zWgd57bB84XS45W/QczdeoSWRkA5NyzK0bd8i2Zzz80TEutLgmUHdTtscZIy0otccQCCawc2uL2e42Hz89xOQZk/2S6cQgtSSgkCgcHqE8UkOUCnt5hr/EzwAul+kl/sS4cMH/T8C7Mca/91M/+k3gvwH+7sX3f/FTt/8PQoh/wmBSWv6H9KEXz4GWQz+8UIIQA847hByq/kQICOLQzx7cUJUY40XHdEBezKCkShASbN+ipEHKCy1dGDR31jO465XGxYhKFL0dfuidJXgPicbbiJQaZTRd7xmNSjZ9xLYZs/GMcmSwoabun3Lt1ohVNScER9vWtF2NFAppFGUxIjHJTzrnaWc0x69S6pKm7Xn49Nt4+R5tX32yK0uU4oJQCCQ5+9u/zFtv/vccXHmRxeaQj9//p7jNDynyl9na/S8YT18nBMHR2SFPT/4AtfUQXxnG6ZRCC+brM2aTGdZGYtBIaZBGkJiAc5a67hFRoIQjyTNm+YxeW3ZszlZfsrY9qQCjehLncBbaGPiwmaPcDBE895szwlixW27xwt0X+caP32Xr4AZZ32JCPcT8VJZlb9FFQt3OmTaavgssmxYRA10HaeZpXaRMerLsGnW9REuPtQG/OEJLx2hskOkEKY8YbY1YLz0n52usfYg0E0KUdNbx5Nkpy+UCFzqSLEWZlM56lEkYSTDGszMbsaxhfvYEJRrScsJq3pDKHiPBm5zz5pzO/AApFC8kivPz+5yepKzqBTKVJFJdrEWJ61N0eI037/0NvvyZX+bRRz/iX/3Wb7CyH2Enh4wPTkHWhOgJbqBhQ+v8oA0VgmEKf7FuZYxkiSDLxog+5x/+498gkTlnpw5IYZmTZIqDlwf2am2P1orMGGLw1PWa1sIkH2QmeEdddRT5/83em/1YlqXXfb+995nvGHNERkTOmTVlVtbUXV1dXcVusjhJpElRMAUZtAVDEPxgwC9+M+AH/wEG/OIXQbBgw7QFmQZpCmSLVHPs6u7qGrMqp8qqnDNjjrjzvWfagx/OjawSxW6R7KbYBcQCAjmcGzdu3HP2ud9e3/rWctSbTWrO0VeO8dCQF4YwrIrZorRo4xiNO9QTRbNepSGN+lmVXGYsLnOU2YS40UJaRegHBGEdL6zhxC5FNqk8XPMxQnkYqbBC4KyrGFE+s6uS0xAAgaiue8/j+dd/jtNPPsvbf/i71Bo1Fo4dp7+/zdzyAs+//BLdvS7aaLq9IbN+jUZ9jnMnj9EZ9zl/apnTp07w3tVPqSUpxi6iwjZJFJLrEjyJ8iKSusIZw9bHOzgxxHMF/Z0N5iIgXKawA8L2Ev7sPFu3HnGwd59eakjzgnFu2R/kdDo5u7sDRsMUPIVTkvHNHvlsRrvxkGYxIfIe4mEo9DqDYZMtF9JLFfONkDnbZyEuiZohtdgHLZFJTHNmliBJqgE8DEVucaWlLKohKIVF/gW3gSMc4ScRR4XoEX4ceBX4L4ErQojL0//7H6gK0H8thPinwH3g16fHfp/KuukWlX3Tf/0f+wHOOSwexlh8Vem9grDScFqgNA7Pr9J1jDMI4aaJSpZskhFGPkpBnpXV9LBw2BIMBquryY5SW4IowlpBWWqUlHiexMoSP1AoFZONcpQA5wyjUUkQBXi+wgmHNZJGPEerWUOFjmHapTEzJogihJBY+5m+1ThL5HnEUYKxJQ6HZ2P87BlmamsU2nBv4xNyc7VKZTIVSyuFqKx5ACFqrK3+Es+e/2c06gsMRnvcvfXbjHtXqddeYmHlHxAla4yzjM2dW/SKt2jObBMEDUoVgfEQhESBT5EaWq0Wk3GBEJJ6vU6cePT7XaRU4CR5npJnmrErGPkFS7Um68vrZCMfO5ngNWKMNOQ2x5OaxJ+j50qCQOEvzhJJQc1v8u719+nmJa2lOYJJSjteoj3bBlNSalBhhBUZ3m5A13VRyqFUA8/TeMojjEOU8NE6RQhJHAdYF1PqCcsLTcpcIExOrd4Gl+Epn9lWjaLUhL5DSUma5fgenFqfxcgaGA8pckJfIvFRQQMxGVc2WumYKKkz06xRGGjOCGzmk6ZjnEkraURwyMAbhv0dhsOSMhwSxhFIiSklNjvG6eWf48WnfoF8POTf/Jt/zlbvLVz9IbI1QXkWhMZN7ZcQ4jHzDVU0aJV5XnnbS1ddD+ORz2zteQaTAtd+QIEgyE6y++iAerPNo5uOJIpRp2L8MMI5Q+B7KL+JEynOFWhdVl695YhWq0lSb2CKnCCKmJ2ZIfQVg8Goei1iasLvSqLQB1tSTDI8LyCpVWENo3RCWVq01UwmI0olyIMYP3D4QUytNVcVfWUJnkeRpUx622SFgLBVMaGf80y11iKqNwMxHVgK44T5tVO88Y//KXk2YNjZYTLuk2UpSbPB0uopGq0ZonqbsN4gHfW4/MEH3Nvo8pVXvkTpzRGE24zGQ4zw8MMaUgUgAgoNQeDjxzEHW/vcuL+LznM8KRECjLXoYI4oWiJZPstokpFaGI3H5FrRz3IGRUk/NTzYHZKPU5qeqmQNmaA0gnSz4GrnJs3lOsSCmBKTTcCbI5NtpCgYexOKRx/BQsjKoE3Wm1B6Hl6rBX4NpwS2orhRrgRncBjySQEKpDnK+DzCTz6OCtEj/Mhwzr3J592m/338zF/yeAf8t3+9nyGYGIUX+GSFRUl/mnEu8PwAg0euDb7w8agKO20gUBKUwEhJrg0GH23AWgWqavkZKxGexEnHpDR4no+xBs8JSgDlURZU+dihpHACi8F6mlQ7fOHQ1uH8kCAoqc2MmWQ98HY4ud7ElhIpFKYo8EmqgrYYIqyPKQVaO0pdUuqYpojxWxM27z5iVL5Pcz6nMwqYnVtGYJHV7AYIn+W5l3nh0m8g8EknD9jf+x7D0YfU2l9mdv51au2ALN9gt/+AoXuf1nyfJFxAqRVE1sGWlvFoRD1uoMuM/d4WOMVkkpG3atRqIVmeYQ2MRxnalEjl06HHJ3sOt7SKzBThxCebVDZOdRKM10LHMTpo02jWUJ4i9NsU6YhIJTzY2MVEMUJaPBEQiToyDfD8EF8p0qHBD1vY3BCqEIqUZt2n1VrAaEXsh6R5AcWE2dk5Bv2SSXqA73kEnsdOV9IzO3gqROvKhgoXUkuajEcZ7URgXZ04dghZo9Q5yBLpIJ+UCJEigia+yBBC4SUJhe9hjKPod0giD8IGQgSEkSH0Q4JwtYpiZQHhLRIlmlKkRKLAWkk+WeK5c7/C+fVneOfdP+XOo7eh/gB/totQCYgawgqs1VXcvKi8FNzhwLis9MDSCYQRKFEVvhQCL1tGuAWsu8+xlVVmF3w+fSfGsw0cEVG0QHczZTwoGQwGFEWK74cYJ0izHCEUk8kYP4go8zH9Qcr8UpUR78k+nu/hnEYIn0mmybKUvCjBKeqNGgcHJflkUL1XQUKpDcNhj9JopBdgxv2q5SAUyAChAmpxiLAarECbEO0kWmriVp39fkG/1wME1k6ZUMG0EP380JLDGktZpuTjPsNul9E4I9cO5YXIKEbW6ogkoXCWrQd3mBQaEc/Qz0Li2QbzK6foHmxhTMlwNMKPG4RxDWwVoCHKkpt3H7KytMh4lCIQxPWYp595Ep0sUyQBB+OQg70BIw2utkCRGmpJxGytzuziAWGyS57C+kKbpBYz6nco05JOX9PJS6wR+IVgqSGYTSyZ6GLryzT8jDgd4vn7+M0nKG2AGHdRnkZFjqIcYIdDjDGoMsPorHovs5LQs0RJ5exwhCP8pEO4wy3nEY7wE4y5ubb7qZdOMH/KEJ2zKM/hK4kvAtbbL3J25Q10ofnjt/4fNnt3SLfqpBPN8y/HnDk3h6RF4p2kmZwCIdjr32Nj8DaaDGcd1lU3bCcqGkYIh3FmOsTjGI9z0iIligW+klgHZemmqUwOawWenuHU6FmWWw1mfEU5ydCi5MqNexxs7xCkgkROmG1qlmZjvCSkduIp/NocnZ09rl27iVp/Gr85y/7BHvV2ndZMla4yGytKDWWhGBvNwUGPhIznz65x+tJXuH7zU3YPesRJRG9csncwYE4OmEsEfX+Z/YMDkjimLEoyY/ntb36TdGqq/9ke4i/cC8R/+F+fP1QNkEyZ58Nn+MvuJ4+fx/GX7VfEv9c+VBUNJiw/+8Sr/PfZ30cai3MaW5Q4KTHCgK6mwK3VaGcq829PVvGZ1qFtiTZpdf6cwziNweJc1drVtkS7HE2lUywppqy7xYgS6wxNf4mvLP8soVdDqhjhBCUaTFkN11jNoG64vHabfDFCeR74Hn4QIIxFU7GMSnrgQPk+uc4qpq/UVY688lFKIX0PISy/+0e/Q293j9dfeIHzJ9Zor66w3y0oDCAEwkmMcNSSiAuvnCMII3CVLy7WkaYT7t7aIZ2U1JIaSgRIBHlZoPd/kxX1Xfb78N6nivfvReQyIUgSPD8h9j08wGgLApaOB5xqxxwPT1OMhwRyjrWZC9zOr9NIjnGi/SQ9/xGnzr/A7fd/F33wNkG8ztyZf0SvyPjTy/+WcT7EZT1cPsZqgxQ+VguEm2Fx+TRfuvRLLLQb+NbyySef0I+61I9paokiCBN0mZNlE6zVzM4fQ5c5nlJYJxj0O9y/c59xVjDudWjPtnjywiXScQ+Lh85SSuM4fvIExlRyHSEkfhhhdYmzmqIoCcIYazQIiTYapQJ8r+oGFGXJjesf8tH7V4ijhJmZNmWW4YTP2vICVSQrIBVCiukaACcUTggO58ysdRhXdUS0cRhjMNpgja7kQ27KMttqNVmm0iJdVjZxzkyH1qouTxVsoLFWV2vQ2krOYavrH2uxztGcWeJf/Mt/9Z5z7qW/fCUf4Qh/9zhiRI/whUCpS64+uMsTizXOxgLfE1htaNfPc+n8z+FKwdU7HzHKJvhJim0XUK5wevUNnj79JDgfZyVhEGCtJqlHGNVlY/Rm1Qa01QR95cxY6U0FVYFqjWFsxqR2UuWuK4nOJVo4hOcwwqBLiZAB2aPb3P/ex6i1WWxriROnz9P1ezSW6gwyw9LqRVy2R9m5ScOmtESKy/vkd69xfnWJop3gWgnt5gpFOsEN9hmP9ggbASquLJgCEWJ0ztx8zGrLZ2b4Fk9OtlkPYlQYc1Br8yiZI9Ax4+4evVEHJaCYjIl8n1GRsbu9w3A4/JHOyWEB+cM2s/+xx4ipHdfjx7rD5HVHujxhIUuQ1mFNgSmryFVroDQppc5wSiDxMCW4KlMA6xxGW+zUU7a0GiM0JZXvrHMSi6NwhtyVFC4FMcY6gUU/jnRfn13iRHECXzfAFui8SnKyRDhKrLL4uSaZi6BMkEohjIdvfQSuel3OEUgPqIIUDBo8hbQ+0gh84SOVh/QCynLAooSHt++z8MJLNJMZ6iJBthL2BwVWgERhcDRrLRbmVvACD0RViHgyINcFnb2CyNdEcYyy1W+jcsXYjPHMJokBLwM7UDzotfCSOcJ4jpm5BdrNGqWdkPg+N67dpttIyFdP8/SJFwhszjNPf40Tok5n0OX06RfR4iyNM3Oo8ctsvvc9ZlqKldPnCY/V+XT7Plc//hYq67Iy8wSrK8+ws/uQ3e1btL0Js3aPEEuttkSrBjvXNtDjHslJn3rNwwlHqz1HPglRUuIEBJ6P7weURYGOFO2ZhFvv3GLY79OfjDm2vsLc7By7u3scP7GOM5pavYbyJEoKwiimYlo1ZVkgkBhrGA/6REnCZDImikPCMKZWD9HaMLcww/bmIx7c2qRej+l39hHSwy20Ec7gpEAJRbWJml7vCCTVaxbWIVw15GaQn9u4fS6ydXrdVqLgqhSt7j0abUq0znHOVAaxuMq5QRdYpzHu0JpLY+x0qNJqNI4oqf2N1vYRjvCfEkeF6BG+EPAjwfxzmmA1I/QTpJMk/jwna68w3Dvg/uZ9wvoxXrzwCxxM3ubkq6dZan6JOG4SBCFGj+l1t3iweY1T669SS1ocm73A3uQjMts//PxAOFFN4+Om5ei0sLEWrEMphfKmDzZUiTO2aiHmec7GTo+gEKR7mtFkxKNPfofnn3sJb26Fzt4W608/xSBf4/btOt/89ndY7l9jcXWVrl/DbndoDgsW14+zUK9TigF7uw/wnMfmSNFa9nl490N645JLl57mzJkT3Ln1Pvbjj3j6a7+EPP4aw4dX8W58h610jrIzYDstebBTMBx0OH/2FKPOkJH+0QcYxF9xCOKv0nH57LkO2dXq38YaBsUA38ip5rfEotFugjEpSIGvmlgrENOkIIeozhMBhyWtlhZDgSPg0EHMOkNJhnH5VI8ocBhwEOARyAZn6k8SyKQKbvcCvLCBS4dYMwbr4823KTau4AcBfuBhlUAoMbUmqgZFpta3aFPgqwBpHUZYnASDxfckTjmsdOTFgKbLWfJyRnevUJ49RzbukMysoCYlpS5xU23GZJyTjgvqgSJQPkhRJekISdQMGPYmSCS+p7DSq5wdSkk/g6wAY6DQVaTs86+8ysHmLoP+Lq5xkmE/o74+y4L3NLfu3uSJ80tcevHnmOzdIG7X8YNlrNyhsCXtEwuIWFA/dorw9hLN2QDpHH4iOb22zqc3U4QUrmcl4AAAIABJREFUPPfkz/H1n3mDQhs6W30m3dtsXf+Q3ltX6M/kzJ8/TfPsReguUKrr+EFEHMekaYaQHp29DWYWjuH7lel9v3vAZDLEE5JhZ5+oXueFF55l5dg6zWYT6wxhXCcKfByCOKlVbKTWAIRRRJ6ljIc92nNLJA2wxhLFNaRUOKvRZcU2CiSvfeOn+Vb+B9y5fQdPSZJaA3AoAUJSdVGcqQjL6fVrkTgnpl6/1UZEOAvTGAspFXjV9WqtQzhZxYo6WzGkTlThGVZWbKv5LDHLOYvFTnXwDuemjgjTa++xpOMHKqaOcISfHBwVokf4QiAIBF9+tVkN6miI0yZeMcM7n77L3OweM4sniTyfqN7i5Oo/IKm3cRa0zhj0dxgOtumOr7KVXuHY8rPMJPNEXoIvEnJ61Y3eqWo+eTq1W7XMKq1pqQ2eAmsc0vfx/Ahrx+RlFa1ZaoeylnF7hZ5XYzTaROB44C3zVO99vEGDpHmG/rDHx+/8Af20pIwjPtmfcDfboyEtu59+ymzk4bodTp48y9L5F5l0LTevfJ+suURtyefJJ06yP4BmEvPpm39IuXEfnd7DL/4P5PxlVk+9iOkX6PKAR+++S2fpPErGnH3yGZJGi0xHSJn+yOfjhzGcP+z4D8fhp+e0WDQl/c4GqnQIDGL6vjvpEFGNQIRI64N1CFtikCjhMFJiXIkUEU6WYEG6ACG8amMhoHQZFUtZ2XApfKTzHg+C1f2E2WgRKUIcCkOKjCI8axBa4JRgmO8z7j/EmhmEFggPUA4pJcZZQOKkwdohk+1HxMuncXqEVAnSD7GZRhcFSnr4OKKowYnnvkp38y5ZqnFBDU2GJCNuhOQ9g/Q8irKEQCJ9he9VnrkOR+lKSpOTjjJ8z0P6U+mEMPi+IisdgxEUBgapRJcVS/5nv//bhEEEztHd3yZQIcPuAReevcTJ4+fY2L3Fg41NVsIQBPjRHFIoxp0ejZUaXgj15Tn82iLKXwIZITy4cPYY9z9ucOt+n53dT9Hu6yRLPvHcHDab4+SlF0m7GQc3u+zc3qRnIkw7wHOC/v4BPeGI63W0rXw483SIMSHSC+j3+2TpgLTTYbEu8GRGrCoN6f7uFisraxgHZVmivJB0PMZagxQWIQOMLhkNR2zfvsuNy9eI23VOnD5Lq9XGWINS4PICITysKZhpL/D6T/8Mv/WvfovXXv8a9UaD7sMHKFFpX6uuCtNrV06tsipbJR4PWVVu/OLwSheAFFXqlXDT1KhKDw+HoVKVV7F1lbexE1Xr3TpDJSRhunmritHp7vhxV+EHamuOcISfIBwVokf4wsBJR6CqovBBd4/+1h5if44tNeH5r8wQWcnsio8vYmyRIqUkz0t6/X2G4+tsZ+9X2c7Du9TjY+iyBCuRSj0ez9XOPP48EdNiQjiDMwbhVb6dUdBkNCkZjUqsdXiiYkRd6ah7JTLwSE49gSwsqphwZ6/DerhJOtZ0P73NhRee4/tvfZu0hEEZYocFPT0hCyJ6k5Ls3n2ax46xVvO58NU3KFDsdjdJggRpoemV3PrgLaK0x7HVE+y88ynJJ/t4D96iYJHrB4ZbV75HI2wSN+dIpEcUBHx67zbdToc0/dEL0R94jv6GmvO/7PsshiF7KKORziFLi8BDiIBAxRQSBD54Et+rg62iIRH+lCEqsK6oggdEUDkXSNC2MmIXqIpFJAHn0KKsNKOURF4NL2hCHII24Cc4o5HSQ9Tb5FHKnXe/SU0tI5zFKaoxdgEogSTAotnpbTHcug2FoJEOcRt3CE8+zcyZCwgfjHMIWdntjEcDbt25xbu9EptvcclkYHJ0OkLbGk4KijIHK6k3avhhJTGQCCQObUrSwYii18e6ACP8arq7LPGdz2giMROBkR4jLbFehAp9KBxpmlfyFG2QkaMYZ3z49nu8/upPkWU7vHf1u7x2+izmREm8vkLTe4MHH0nK9zRrRqFWAgbqLHawwNmgyoafmW/w0vklhqMuj/be5eb1r3Lh1UuoSCBjgegqktmE6JWE5S8dIx9pNu/vsysKOp1drl1+j4WFJeI4orm0hi5KimKfxWMrtOcX0EWDUeAjo4B+p4MXR+g8JQhjEJLtjQ1q9TpxIvCUwvM8pOdRFiXOGLr7W9x8589Ze/6rnD53jqIsSdMxQRCSTlKcg07ngDiO0GXBytIyly49RbPVZm5ujs6je1RRwVMW3x3+eajxBCck1VV56OJxaNTvPmMv5bSIFO6xpvTQno1pEIF0DiMrey1nLdZWmlHn7GOdtnO2ijhGVG18PmNnj3CEn2QcFaJH+EKgzKB4P0FcyJhMLKqb8JS/gJn1eO/jPa7+2R8w24j5AIVWEa++8bOsnjyLLgdM8iv0yg9xgJIRvXSbpTIlSZpEQYOJ3kUKixEOYSvj7OpLVbotB55XJTWBI00PyEvwfYUUUJYWqx1ox89/+SLt5TX+6M230aMUTMpOWWNp/RxLx04wv3Wbra3b7PYMfrJMZzxhNNFYq5CujfVLglrAwfYmv/e//S80ozatUxd4+Y1/jPMUd2++Q7H/KTPtGt7qGkNb4KJlHjQWGaOo9Qui/gHD/T43jKXbvczPf/2rnD69xlJT0ek3ORhr3vvo2t/aufoLiVo/9LE/7Lj2DYP5FH+g8QoQ2iFchudCCjdCaJBW4fIM6SKk9JB+hPJDSip2yUqQIsDZqv0uqc5Z5UBgsKL6wLbCoV1OSY4RmiCso5IZRNzG2Rwx3q8YrlbIIN/h1ge/w+7oBk/PnwHpUMrH+hYjLUYZMJK90S7vf/IBnc0NLhw7zvDa22jnMd7ZYWGccebUeQJVrzLbfUFQrzMQjqLWYNemjPcfIFtNChlilMAJD2mr6zLPczYf7HPi7ApQYpyjSFMePdwim+T4sYLCYJWgLE2lq7SWvIwZp4Is0/RGGaWLOXH2aTbufcwknWALQ+gbkAGjXo8b1z/ml3/1F4mLOq31C5RxQORLgvYCa+ccBxuW7WuG5VKBukR34iOVmJrRC5zRNJOIySTm+pUbDPuG9eNrrDw1j6cUWddhLNy7nVFrexgvZDAas7i6xuzKMXY2N8CWTNIRgY6QUlCrtQlCg9Y5Uhiac4s8cbGBkIqdzUfMLx5jPBzSnmmBsxhtKfMMKT2CMMDzPKyVjPoTli6+yPL6cYRU7G7cpcw08yvz1OsNijwny7PKJWA0pD2/wKtfex2lFGmmcc5inEUK99ha6rDQFNNWPFO/10PtcrVAeCwbOey6VDNI0y4MFcspKnuMaVtfIkSVFFcNLx224sFObercYYt++t6LQ/HpEY7wE46jQvQIXwgYLbl3L6DuCooDj9dPrxMHEe/d6tIMHatzAYHnuP9wmwcdTdyocf/uZeT8PpnYorSaYeYwVhGxxYvnZ0gnQ7JiNGWyBHLKRggHSqiqSLGWQpcYa4hk1YIrLfi+RElHWVh0CdYJAk8x6XY4XQs5s7bEx7c+JWw2mV2d5cRLT/D0i9/g0Yffp9Hpc73zZ7gi4LgQlC7iYDjBC0IKbcknPSabtxnXZtlxjkuxz/LiIkWpaZx7hujiSW483GKiJaP+AH3mKR7ubJIFs+xojyw8Rj/psp9LXjhzgvmkgd19RJLtMyoFg4n+Wz9fbur1+KNgNKN581cP8EcKfyKoD33a3YC5bkhrJ8H1R7gih9KBGSOcQ+YOaUK0pIrhFA6kh6fqeDLGGUfpCrRL0S5DY7DSUZCTuwxjS4yzWM9DBiFWj3DCIeYWcKFjf/sK965+k4PxJ/heghI+wjNYYVHSw4oSY6vBmBu3rnL/8g3+yT/5DZ579hy33lxif/MRj8Y5H374NtILObf+BBIFOKQf8fKXX6fT7ROODth8+IDnVl4iaCSELmDYLREOfE9gTMnm/S0CD5q1CCcsvYMBmw87WG3wpE8YKialwQdwms4Atu8aci354I5mZ+ThBTlFNmRhYYksndDpHJAWBXGkqDUTCCXxzCI1FxM3WqSTgM7bmjy1rJ7zWH1acf+yZnLPsZivUs57mMLhSsF4kHNzY5+Dccb62b/Hk8df4aOb3+X3//R/5afO/hqXLn2JzMX4DUUkCx5d65HqHHk8JKrVyPOShZU1ojAAFBZDqatibWd7mygKiOM6Dze2mSUgUIrF5RWUqiQ2vgoIo5C9vR1qtVqVaFZaikLw4ONPuH3lGi+98dPUGjW6+wds3/mE3d0Or6/+Z5XtVJoxMzOLUopJ4DPJcmq1BvJxtJXDOYNBIG11D2GqL3dTDvRQD1odOmQuD5vmVfForZtqPqdM6uOGe/W9nz1L5ZhgDlvvh5NOzj6evD9syQtZ6X+ld8SIHuEnH0eF6BG+EJBSMNaOzuWci6fXORgr0p5jY2fM+tIcW6ng5r0d0hImk5Kbv/dd5lqSb/zCPFl8QH9SsrWvGY7h/kzJs+vXpxGbBUJx6EeEQmKFwjpTDS4JQV6UlNqQlVULNPQdzjryokozKQuQSiKdIAk9xlv3Od+aITy5yoODEcY3HGyN6G9ukj38iKCc0AxKbt/do57UcSajfWyR82ePE4URtz75mPbuLrXJkOb5Nfp33+WDf/4Bavk8yxefI9vfZ3jnGjUUpjHL5sOHLK7NMbNwktW1pyibu7zUqDN7+Sr1vQcM15bZ8xrsHSjevXyNucXlv5Nz+NfVjwovRJw5h20JUl2Qa0HUW2T+3nnW761gd/oM9x8wHG4yLLtkxQEyy7FmiDBgVEkpLM5otFUg/Epfh6k+/KliTEtbojFoa7A4hAiZb5yrvCTJkfPzmMCxc/tP2Pn4LXTRqTYkrqgm5RVYVRnlSznN8FIeDeXx/LMXaTVmGG3ssXz6PM31k+y8+UdkD+9inrqEVVWbVRqDwRKqGr/29/9zJju3mEma1JYXydIRTisQAisrltc4RzoYs3V3h76SFB4MBgV5WhIhqQc+tcSjtz0iDiKchb2h4OaWICstOyOJtgKhNbtbD5ibneXY8hJx4LO3t4c1mlrDQ6ddfuf//k1WTx4n3075yjO/SFYoet2crAg5cTakPi8Za8fqyRb7W4bdW5YVX3Dvdo/rj0bIaInjx17AyYjN7i3qLkTtDLn87/4dHxXXaLbWODPzGuOeoJ8/YGYdQBJFMWEYEYchg0Gfeq2ONpayKNnfuE+cRLQXlplbWEThmEyGNNvrpJMRDoHvB4xGQ+r1RsWWuwKsY29ni8KVXHztVRaWlisdJoLheERYa6H8yj/XIYjjGCklUsxQc5Ves9/tIoRfdeHFtCR9PP5+eCsR2M9pno2Y+nA4h8PgXGWKf2i35JytWFH3mT1TVbROj03tmD7PmFbHzNSyzGKdxmGZmQ144owhrENqoh//Qj7CEX7MOCpEj/CFgDGW0GV0hpruuCSMDLVAcW45pJNbNvYHpDJE+BarQCPp9lOGu5pyTrC5k/FwQzMewrjZIUtHtJoNRnkPT1mUOrQR+ryqqrJFSbOysgIyFkPVvjfWoAuLKauiVLgqOen0U1+i5ft0Nm6yJCwLSyeYbycEpWF80Kd58qts3v6Q5148xUB9yrC/jx2lnJxXvPHEDIvHn+GjuZj+vRaDT66CTDCzHhsHO7SGPWbvvcfgYBPZXsZvrbA+02JmPGJvklOLIqJaE08Zwr1dRD4hn5vn7vaAiZuw8WgDJQVLM/W/1XP1eTb08/ZMf139qOd5tBt1pDWEE59Te+s8ef889a0Q3RkiUo3nYpJ4lcbMOTI9ZOfhh4wnDysv0abD5AU201UBKkyl5nu88wCLxgC4qv0phaMdrHCsfhpnxjjPA5XT/fj79O+8B2WOswZHiS8SPKEQ0jAdxkcKibUgPMXLX/4GpdU4HDkGkY8BzVy9ztdee43FUyewnsBgkdaipEB6CpRHvHSWRquOCEsir83ugy7Cq+FcNZlfUtn1UE4YDnIGwgMEpcmI85y4jBltH/D9b9/g5a99mcgL6U8kdw8kxlYT3QLwPR9bagb9Ho3Iqzw9RRUFGschtkzZ2fmUwo3obO3SbDZZOb7OVneXlTNf5mDfUq8J9BhoOMKJIE5AOcgHEb49y/ris6wvrrK7+wg16fFk80t4XsiH4+/y6fg7uNRwq/vHrKkzJPUapjyONU083yOKQoosJQwDnJ16BysPW07o7ewy7h/Qnl9hYXmZxuIiRpcgBL4fUpQlUlTr1uoMZTKsCvH8gLnlNWq1hDAMq9hTJQnqLT66cp8XC02jEWEthGGC5yk2NzdRQjG/ME9rdp5+f1TpN6deodMr/3Fb3DmHFdP7iayKVGunk+7OVZP0U62nOfT+nK6RQ93ntO9eHXe20n1OrZ2c1VhjsK6sWvrWVjpTKTi2BC8+49FcWGRrd+lvuIqPcIT/dDgqRI/whYDDMtcUPLhX8v0r96lF28y3a6ytzbGz1UXFLVya0mw0ybMRgUxZTBQXTq+xUXZ4aDMC3+Fq8OITTxN6Pnc2PqY76rAQNZGqSkvSthpkOPzA0KUhL0qErF6DPBw80I6ydHhS4CnJYKgRo5Kb3/w/ebItORhLwqVVZixQlLRPrhMvnKTfH1Dah4zub7BSGp5/+XV8RiwEOdHDD7j1vT8laT/NyTMXuZZpjn/5F6jVm2TdHnHoE5gubueAWjFhsnmXg8E8W7ceEvqGcPE0wtM8evfPWT7zJIv/xW9w5e5tPvizt/mv/pv/jg+//z0Gwy5R+IOX/V9H3/mDvv/HFZIh9ifM/cvbtGRMNFSErmDP7HLgfFQa4GUB+WSCEz6BFHQ79+l2b5PbCSZ2pIkm6KWVNk9UBagknEr3xHTQR1atceHhIXEozi+/yszsMUhTTDqge/UyvY3rCOOQQqGcwBMRsd+oEockeGGIcFOPWVeitKESo3pM8hSlQmZqLUg7rB0/z+LTs4yGRcVyTpuwnqg2OMKAFB5RHCGlRnkxtdmC/vYIL6xX+mVjiUIPPemSpxpq82ijEULhpEWPOxgkH370HrVWzCvPvYwnJZ70MbYqZpQCXwU0G01AMxwOeOXCOvsHIdfv7DC7epww8Llz5V3iosdgnPPNb/1fXHz6RV5+4RdZXg/YeeQoJ44oFgzSal0IDWXhaNUXWJ/7OsfXz7Byqk6UzHHx5ldoDhN2vSFJa5G1yTMcjG9j6JELy6nlXyaI76CLHIfAGoMxJXHS4GB/hySJ8fyI009d5OMbN9A6JU3HSM+n0WxR5CXDfpfeeIcPP7zK0rHjXLh0gVIXbG484uTZJ2nPzOMHPm46+CNwlGXO4tIKj/7td7h35z4vvXSRJIoxusBZRaOe4KY2bUEUgRsgpUQ4CXxu2n06OGSnX2CRzqGpNrFVaELFbBpnKkbTaoxxj+NM3VQ3KmHKglZspzYFpU6xpjLjd656/dVzVj/fGri9UeB/NOD4qQF179iPZS0e4Qh/mzgqRI/whYAxjofbGc1aQDpxdIclJ0/OsdBaJM0iyrDBSjHh+Qsv8Hvf/DdYUzLRjvsP+5w83qQ8dpyFuT5OKH755TdYmF3lnZvfprdbItKMxfUEoRzOms9psARFWTEPXghV+VJpurRxCAVhpDDakaUOOXGMxnvc7moGrTPkb72Dl6dkQZOF2OP8+RPUV05T9+tsyCZPvvISC6efIh/26G1/Qm/vFv7gEbXRfW5f99hnjUD8CXFrhlbos/781wjXXuaEsXS2NnjzrT8mGu3w7Dd+hW42QOcp+5ffwpQT1k+d4ntvvslgv8uJE+c4ub5O1rtHPmmz389/4Pv8+SLyb1JUHrKhP45iNCg8Vj5tEQURKEXgx8QyrlJrfAkBhPUEZxy2nGDFAOuPycs++ak6ZqeHtAaBwiNCOH8aECmrElQoHAohPMBHoPBVzIz0ySebhM15Brc/YXRwH5QEm+HjVXnkxqKch0zqKC9FRQqjDVL5WFuinUGJKvZUSUFWGIrAJ6nPEoURWiXsB4bhWE8ZWoObpkoZ5/A8nyCaDqJoQ22midjYwVkPCJjkQ4JBB2zGXmbJBhmNeoMw8MmFI89THh7scfvedc6cWsdefI5aErMyP8dOdwDC4iOphQnrSxdZnl/CFzuszhqOzR1n0CnZe3ifublF4qgGDmIx5v7mR2TlJq36IkmtTlKbY9QDLKSTag2sBBI/gtUzDYLaa5TWkY0sgauzEp3BJJJnnz3GG8da3L17n99+639mVO6QRl066l2inmZmfh3f8xgPuxjj2N/bp16PMRYmvX38IOTZ559HScVoPMIPQ8aTFKs1cwvLfPfGt9k9SHn/8rc498QTtFp19igYDTvo0uB7Ifg+2hQEQUxca7C8eoz1E4tc+eA9zp07Ti2JCYMApSTNZhtdVENT0lPTgSOmG5wp+4/Dycpb2B5GkbrDCffKJsxOp+XN1E/UorFOTxlPMTW3P2zhV4+zpkpX0mX+WcqSM9Mi9LPvm44vMRoILn9YY2MHfurZH3kZHuEIf+s4KkSP8MWAE+x1HF+5uM7kk22OLy/zzDPPQVGSDx6RDytPv8udR8R2zGxoORgW/H9/eJt/+PICs3MR4fwc7cUZZoOQ/u4GV9+7SdAQdIYptRlFWHdU4UqyMqnGURQa3xP4gUCqSiOGcwSBrLK+hWA8NEyGjiCD+ORLrCzPMr76IQe9IXFjDj02bDjL3p+/Szx7C714jvbZS5x95acod+4RtNoIdZFbH76H80/S9yIWfvpFTgqP7fsPuHfjY3pb27R///c43oqRzSZ5a5146Rxnv/orrJ59ht6wz53vfJP9/g26zVWKP/sj9vce0QsWiJozTCZdNAkHoyGltX/lt/3HyXD+deF5Aa3mMZT0kVLheVVb1SFRwpva1djHht5LjQu0llfZHzxgw79POeghCVFEeCQgQmBabAuFIJgGGSikCKdWOZJ00iXrbOOkj5pdg7QH6TbgI7wAr0jBeWgsnl9DCAOequydHASeAm3xhIdVoLyqSO3lAt9PqNV9Qufwa00KfUBZaJwWOCGByry8UUsASzaxSFei4ghVq+PKnDLt8ebb32V45QY/8/Uv89F2j720wJ8oXv35Vyjv3aG2GJDeu84vHY84Zvvk6Qjlcuq+ZBTXQZa0goS52gxzSY1vXPw17j+6wW//+f/OarvP08eepG9X2Rt2iaRi/2CLSdrHOcvO3l0+vf4/8TMvvcBTL/8jTp74EklzHiUEhYM8g5qC+nGFtbC/abnzkcZlE5S3yOqzCyxcCAFBeXs66OV5BKFlNH7E7t4AYYccP/M0UZyw/egBM4srzMzMkhcl2WSCUFXbPgwjmu1ZEJJBrwM4pBQc3LhKM5jBdQ64c/UqL3/9NVaOn0HJEBHDoN8Fa/GDJsr3Cf2AhaUVfv3X/yFvvfltsixlZmYGR9Xa95TCKkO9VmPnwW06eyOcYOoQMA0xmC4TKw4n5Kt1Zp2ZzhW5w4gMhDBTrei0pe6mg0lOVNf0Ia9qq3hbZwqszeFQE3rIvjqLc/IzNpXq77pU6L4P6V99rR/hCH9XOCpEj/CFgOcpfClRfkIj8Gi12kivxr3b1xl397m4GODXIgbDCV5sUbNNTodN8t4OMi/xOyVnGjOcVA0eXHubnm7yzvce8uQLMXNrIV5kq9z4igqt0k6cIy1KtHN4n1kAggMjwBlHOjEc7BvygUBKWD3RYnvnIfWZhPNPnaO5ukDz5BluvnedB+9s0663WDt9As/s8u6/+B9pjA+I1y+QmpgHD3rMPn2BKC7Z+fgjHuz2sAZm19ZoXLqIyx0bd+8yGAzoHNznqVqdMLjInW/9Fq1TZ3GTgodvfUCnvkAiUponT+Fmlxlllo+vXqPdaHPzxl2M+OEfTj+KKf2PtTWvFH69jkKC8rFS4aRXDZlNDRettghjEc4hPIufRCyq0zTzZQbLJ+ju3yfLi2pzQVgJK+QheySraWaqRCJPRXhS0su69IMdah1F1Fpm5uyX6N39ENffJtcFUnh4IsTolGLQQacZECKVqoadlKLUBYEC6yxCKUxZAtDLNPVmA6GHJL5kaaHOnfub+KqOExaHQHkKoRzaGoYjgxSChi/xo5jUGYywPNrapeYDKqZ1LOBbv/k72Kyg++BjzskOk/mE0uXUai3q7TqdRzep12LWFltsDvZYmFlkNVkjUpJR5yH377yDQrC2vAjDlEvHf5UgPM1uN6O/mNMf9ugNd9k9uMV+7y7DbJcrNzbA/r/8+e/9IV/7xj/j+RefJPAkO32L2nXMrAiSGUncdXR3IcsT/MTHr1UWT8PdnGv3v0Nhe9TiiNBTOCbUooB+f0i/3yEdDwjDGAEM+z0QonIxEIJskuJ7Hp7nM+z3CXyfNB2TTYbMDh+yv/ERx63EHGwxHvXBSnTaozCGSVqwuDJHktQIAp9Bv4cfhMwvLvPK668ThBFRUiMbDynyDKsLgriBpzxm5xfISxB702Qj8dl9AZjGA09zj5yHcSWGqmBkqh99rBmdttbh0JjegphaMVkzje60GF1ira7YVWceW0G5xxFKYqpTdghXrcOTsxHKHRWiR/jJx1EheoQvBJQwTCYj7tzeYiby6Pd2uX7lfboP7xDKkvWz6yQRXP3EUIwdflZwZiZlYWWeuZrA1yW9rYxPNq9z7jj86z8pyUyO9j3m1mscliafwWGNptQlCIvRgtIIPDU1TXeOPLf0DjSTriTPHEHk+OjmFufaCTU3IYszRnevMHl4D5lLlufqzM8nuMk+K/OzcPYURa9O9+5lHnUsprFA+/gqRecR2w+22B3lDL2E23fv0b3yKafXl1hbWKB2bJ7dBxt8/NafMvzeHzPXcOwFbUy8wMZuxjCx1F3J8NoNxiuOUa65f/sOX/nSizx1/jTXbt/63O/5H9q7/CiF5I+TPZVCUPcTrLOUvodRCu0ElKYykbe6ytc2BRiH0A5nSrQtQBiSdgs/Oke/02U8GFdUlXNTtZ5BCIegMrpXUuFJD4lHz8CBHKFsihgcEDYWaR2/RG85w8gdAAAgAElEQVTTp9i7iwWk8MnKHnl2gLMxlOB8cNNMceEc2pgqVUfK6tqyjrzUDMYFM80EU2bMNBJm23UOhinKSJQIkEYwGqcYo8iGBt+X1JuSuBYzyYc4UxLnQ6QD5WmeOPUkr7z2HPcuX2cmMTzamjDKNYUzzMw1WfcbNNbOYLbv8d6NDSapRjof2gGLy6/g8pQ7j66RjW5RNgVtb4WluVPU63ME9oDEwHrrBO3lCxz0Xqf7/7P3Xj+SpfmZ3vN937Hh0/vM8qa7q+1M95ieJYfkcnYpitJKhChCEoGFbgXpz9C9gBV0J3BXWHAhEUsBIlcaDrnDcT090766qsvb9JnhI479jC5OVveQSzccLjEN5AsEsiozIlB1Ik6eN37meY+mZMMR5WCfmjY06znHdxMeqinz8xHNluLJDUtvz4KUFNrh+4IkEbg4pHekEZ7lztM7PO29Txx61EOFryRCGjpYos4izkFUq8YNJsM+US3C86PqtfIUZVmQJglC+kx3H2G1ZenqC1z/+AYsneHg3U8ovZDm8jLD/W0efef/ZXxwyPlf/o3qg5tzZHlOWWqCMEYqhe8HxOtbHB7u0+se44ymVm8gVYDnBThrCBptZpd97t77AGHcZz16+YwXyqfb8NaVlE6d2NLKiFbfrxaQzMlXnm3B25/YgrdVprw5mZN11ny2XW/NyXv5s7lUTtihFvCUYy6WJyzSU53q51unRvRUnwt5nuKrzzd5tD0hVjGfbD9ldPshNU+wPNfgOHXE04RGLea8L6lHEotgNM2JtEFqzXBcMMk97ndDpiLi/LWQhbOSMKo4f1KcVCVO8CllWeIwBIE8uc6cGBnjKLVgPHJMx3wa8SmFY+/DT1BRQl0WzHoTdCrppntMG1u0z73Kwzsfk9tH3J1aVhuO1fVF1PIW9cUA1Z5j6+x55PoyRZby+M5DDvolxbQgLTTlwYTEa5LmPZKHOyxtXWTS26c17kPQZTgcUM4vc+ELXyW//w7J3j7GgQJwlsHxEZtr68Rh/Jcc4WfBgz8/FRQhJEgPBChR+cg8UBRk2OmYsHRIC7oskBaENRRFii1yjNFYo0FCrdNAKMV0OKmSg/AQwkM6gZQBVYCNXyUyiQBNwM40RckDmsWIJo64vcrCha8gowbF03dBQC4mjPM9nN7AFRrhKZwzKOGjpIdxBnliVJQQCFuFMnbHUxr1WSLfoYuEtcVFcrPPeJKByrHOkAxH9B72OVPfpAwUk/19EmPxUeh0ytmFNo8+vMOtH73Hy5tXef3ll3jz5RdoTscc7z3g5nd+yIMEGost2peeQ1vBNLPUgzazUYggpNt9wvLaJV7/8n+N0vDh2/8L/eI6m3MbzHU6DLOSaZoikcQBvPBCi8ZcQPew5M6tMU+ezNNsxjT6OYODIaOdMTIxjH3H6pkaSgfkRRWRa6k23nUBWSLYvV8y6Vk2/Ndw6hAj+iChsI4cRTjYgUaD1sIquMoMCqnI04R2q02WpoRBQKk1psjxfQ+djZh0jzl37hwrs22Gj/YofZ+Vi5dJ+kfs379PUeQcPbrP6nPXqqhez8NojR9ETCYToigmS6foUnOwfYOltbPENQijGCkEWZFQZBlFXmCcQbnPZnutqOZ7n2GWjLEU1lFaU7XcheBZfKe1jtLqzxBOJ+ltzuoK0WQ5MaQOY4pP0UzWgbE/gXCq0Pnw7O8nBVKtHe/tTHit+R+fGXyqU/2sOjWip/pcyCEJGk2+8lqT3ft9Xtpqs3OUYP2Yl+ZDanrKzGyN2ZZDBIscTUqOj4eYaY5oC5ZbNeZrEYcHHjfGGrU5JF4QxPXqYiRUVcV6FthnnSMrS6y1KA8QAmOrtEcMJFNDllRVCF1WFxbfl1x+aQt92CU/2meQN4jPXSZcOcv0xm2S0RG+KLh3dxebC45bbW7ev4Xv36Z95QrPvdQitod4s1dZf/U/ZbD7r9ChZW59mYVaTDKaUnoBQS3mTpKwvf2UAyXYCdvI/ohprc3INxQfvMt/9Tv/De++8zY3umOcSXHOZ9zrsn30lIX5rb/Ta/C3abv/rFv3f+65kCjhk+cj8tBHC0ffTkmjlPaVTmU07w0oj3LCwuCKkiwZUBZJdaEvJhTOVHOdYYSsG2wKfhmAUBWYXIrKlIoQKUOq4WBLUloe9h8xX2+y7AlEvyAsV2kvXiI3GZO9MaR7TPJDKFewmUHEHkrICgDmO0xZoFT07MBUbXq/SjXqjzPmW0HVahU5i7Mz5PqAMi/x1EllVWtkaSikIc1yHAFBLSKMQl77wquMnjxl795Dznxyk865M6wtziFtwcgW2Eiy5HmEjZDJaB8/mmVj8Qwbb57haPAQl/vsHe5z/9af0mx4fO31f87lM1/h4GiHC0sdAlHSDjzkvKI/mZLnEe9995DWbMjC2TaXXumwdK5BXoJOFDad43h3TPegRzJJOdjz2Lwww/nLHWZjn2QAe+OSAsHESep1j83Z57iwdLmKMS0mmDxhlB7zw93/HWeOGRtHa3EVT/lYB7WoatH7YUxca5ClCWXao3vvJuHMHNFSk9H9mzSvvIQXBDTrHqbWwDmLCmN8pegLhdeeZbK3S3vrLAiJsZY8Lwh9D5whSxIazTb7248pS43WJZ5WFGVJniWk6ZR+t4e1GucKcAonBMaAthZjXRWtaiylcRgrTsypQJy8P0pjsKXGWKpoVWcqDJy1WFsZd2cdxpRoU1TV0Ir59BN4p5PzDKoZ1ZOKaJW4JDgYGT7cHf1M5+CpTvUPoVMjeqrPhfLC0C0N6606MnJ0CFh5sc39oeIgSegUjlgrmnPLTF2dx7feo5xMWZgJmZ2rEUeSUBnW4zYLrXkel/dJ/S5h6CE9hVIViBwjME6AqTLtn2ECrQFjqq9Yh7ZgHJhckGUGISHwJbGBvaMB/vp5vLPPoeZmiEtH+7LDKzKKULIuQ0RtgUtvfo0//b/+T4Z7fWYfPCF/cIvevUtsfd1H2IikO2BlYYlaUfD05kdMC4mqV5WZZhQjWyHd7pBPUJQqIsIyGE4ZjSfsHh9Q4hGrgMKrYYVmb9AnDiIW/fAnjmxlpp+Bt/8q/WVc0L9JP8us6TPFYZObyYdsH+/hza2w9quvcvGVF5hZmkV6Aj3NyY7HZA8OGH7vOsN3q/vtvfM2R0WfTAhSJ/CdoW0VgWixImsoITAn+CaBB3hUlKcT8I6zZMWUnUmXbmvI2vx5FhJNLV2gFZ0jvLBCc3qP0eGdihdZgisNLnRYm6OkogSsEAjpqjjLIkMJibaa3mBKI/IIvZBslKKCGKlkVdF11fBAO24jCofxNDmGSBqsK3DSsXTuRV7/zYhv/svf5cGNG/zal17Bac3jwyN++LSHimvMeCHNs2cYTUoadsrG4vMsr5+n239MUQw52H3C7sETdm+/z512jXNb30A5QX98ix98/C9odDYpi4AiS4lqL+HXN/DO14jOeNjpkJmFEFcqBtsFSeE4s9FibbHO4c6U4eGA47tHlIclc+0YzzlG3ZS0EeLmImQnxO94WBTK1aj5NQIfIjmLLR1Ph5pLV1YJggBdZoRBRJGnhGGIMZpkPABrMDZHlylRGCKCiGI0xIxHNBYW2Xj9TVzgI6Rksv2Yp/2EuBNjgN7D27TWNnHKJwxj/DCkLDKsddSbbaSULK2ukeUJWRpijUF5PqPRgEG/S68/xTmDthpcxae1Fkrj0NahdUVOKDUYK09YtdVik0WgjanwUe7ZglLVieHTSmjFDbXm2Vyo+yyF6WQO9Fm6UhX3KaqRERzOCZRSKATCnV7iT/Xzr9N36ak+FxLCUogphpDF+RoPnkxprK1xpdPi6cNHTLSgtz/B7T8lKXJsOmVxNuDMSoOtzQ6+J9BZwobXYt9vckZfZv+ox971PRovQ23pxDQIBdaijaOw+mRD6WQzFoeSAnNy1bEFTEZVulJUU6hAcffuNu1Rl0bSYikZ0pmJSUzOYXrMLh1Y/QIr62OOemNuffweGsfs5hLjgz2aaUr3xm38lQ+I59eYFI61TpO653HzOKO0Ba1Oh96D2+TCoeKYxnyLelDnwXGPYVpSWom3uMSP3vohnXoT5ftcvLTB1bV59u4+QBwf4k8O/sLR/emM4l9nRn8S3/ST5vXZz34aOWMwmWH1K1/HO5OiarB48Tx+rCiLKUpLnNIECz7R3Bb11zbp7H+F5Dihey6is6eRZY/b3/sjRlmOjue4PG3ibIEWHp70kcJHEsDJtrF1Fi3KKlXLGXKXMphoDrLH1DzJZnSFRXWWujfLUrzFwuYZjuq7CGUxZQ5BWE0DSh+kwgkL1pDnQ8pSgAhwRjNOpkzrHrIRkaYCq6ck0ynaGIwssNbC2FK6BibSOFvieSHOGPywRlKW7G0/5mEhGHf7LH7vXV790st8uPuI7733IQvZhIaYsvPx+/jlx/zSN/4pzmp605I8VwRqiS+8dpWiGHPr9gcMDm4jVxZYu/B1Dh49T959QmPpAgubawy6PeZW6iyvznD49Cn7t32mu4eorIMfNHEotJHkhcMYR7zYgFbIcPBN+vICw8Esse+hQonSBf0HBYnvUa5E1F9ogNEwAUaa7YNHZF4bHRUIIZFSoVSI8vwKZ2QdSjjCMOD47gfgh8gwotZoURYFta3zDD/5iPr8r1QzoGnKtN9lev8mk/YSDT+n02mxduYCk6M9TFRnfmHxUxSSsCfvXxxRXMcPBVEQgFJMRkO0cQyGY6QKqfD0BuuquU9tOUG7aQqtKbTBuOrfrB3VspIVJ+31avNdOvFpSpKw7jMk0zOIva1SwHD209nSZ6GfFk6oxidQMlFxbmtRyFytgTECVQ3mnOpUP9c6NaKn+lwoUJKlsEbciJmLYsYskds56kGdWugT1UNIEybTMVlS0ogjjB/ydCTZudGn1Qj4witnacqYh3d36MqIo3sH7I8DbG7ZeEHTbEKhFWHsU9gco+3J/CBIBSDAQqktRQ7pBLLEoRDUayGz7Q5qtk1KwExrgVs/eofmwirXvvway1/8zwk/epvDYoCZqROEU8rcZ2brLMXRAetbV5malHfSHvEHN/j133mD8vw5DsdTXn7hMv/kF77Cezc+JlyaJ0p7tOoR2pS8fKVFZ2mTaepx/XDC7SdHNOdWuHblPHn3mNowIRlNuZEMUWEH7fVoTvs/9fH/SWP5tzWUP2trvrQlu/378HHA8qtfIDg3i/TB2JTCKGThwGq0zk8WPRw6zggWHFe/8TK7t+6QHhesDM9z/+33iYaGSMgKCC8coBBOVmaRqh1unMYK8HwflCKzgtQNoRzTLYc8yT7h5eVfZKM8RzxoEvg1RGkAhzOVeZBOYoyuYh7LEiEdSnoUeoAxIcKz9PbvcevOj/mNX/tnaGM5HPZIJsMqMtQqcBB4Hi43GG2Rjmrz2hh8QpLBPsuLa7TqNfYPDvm33/oOW1cvMT8zx1xTsDm3hSimeEFJ3GjirKG7d8z+jk+ppxTGsL+kOX9xhZWlCywBkU7Ix3/IxvnX2Q/Ok481RXxEPcjJjg9598Y7dO/vsXbldVqzSwSzTaJ6xfb0NSdRlVCW0BQe9aU3CRo+Qa1BOnLkI4fNIEhL3DgnPcpwNwusu0+vv0e3dBzrA/z1GkudFaJ6HV3khHGdKI5Ik2cmzVXBEp4iTSdk1HCHB8wvrtBaP8vjH/+A4uOP2D/sgxTUZ0f47Tn+6a+/yKB7zOrlFzG2pDjepd6a4ejJA1pzS8gwRAmBFBLhKeqNJsYYhKhez2arSb3ZoNFo0+0es3tQrSBZAaUzGFvdSmMpdEmhddWqN1BYi3DVoqOz8iSBSfEsk/5ZRfTZ+5gT0/ksaYlnxpRq417gTrKxqg/HSimaUUwj9Ok06swF0I5G5Gr6M52DpzrVP4ROjeipPhcyxvLk4RAPweJci7g5x/ziOdKipK8cqytzWBbojjS1wvLJ7W2mQ8l43MOUU5abit3Esb4xTz+F0WRA7kkun9tgfWOF9773Y7Z3ezgU5zfmuL8/ZPGcorEQUJu3iLAyK6W2ZBkUU8l0aChLaDdDzq6scX7lPDMqYubsBdrry0xKRff4kLe++cesbtzEmIi4HtOqexyVGaXTyNYc1Jps7x9STgvKUvH48Igrjx/x/MvP82//xf9KmZdcWJmnYQ3tvUfMBI7LZ5s0I8nz/+VvE85dRo4GvD4d8fBwxNHhkHfffY9bn1xHSoFaPs/8/ALC5EzrLXR9C/jwZ3o9/qaq6N+HrCdJnSG6sY/5w48Qv/UGdiHAKkfhSqzOodSYZILJp+giRZ+kzhTJFL+RkpeazvlZlh6vsbm7hnQSKQKUjJEiqKrfLsVKd0Lt0hhXYEyAEpaRHZC5lFBC01tgL7vPu/t/hp2DreAC0oiqjarial3aGPAVotQoJSmdQSKQXkyjYRkPu9hkRFmkvPXuuzz/0pssxh2E1pish2wtgNQILYmDEJMZjNPVkos1WOGYnZvnyj/6AibPyD3Bd/7kuzTjGoUxzM+u8utf/SqMDwg9H4NCxYt4nsToQwJmUMEuoQJrVrnz8Rg9KYn9BbpyjrizS+/wOs3WFGMa5EeOVm2TIIrxai3W32gxtxFw5pUZVFTj1ltHHB7koOtYKwliyfx6wOKmjx+sYHUVRlHm0N3XHG3n5MbgzQaECqKaQHCZ+7dLHmzfI5o1PPcrV9k6u0xRpAgpiaKIvMgpy4LA8+l2D4iiCNleIWo4yDLyLGc8GjEd9ugGLR7/+z/GH6cUec72vetsPPcC9STh7PMv4wc+vaM+tbkldm+9S7M1i7+6gQojsjRDSog8HykgT8f4fkRRaowuEFIhhOPpowdYxMkNnKi4n9oYSl1Q2rKqiBr3aYa8eWagncDhUaGW1LOTpmq3O6o2/knvvRoTqSgPnIyOVCh8QSAVYGnXO3hSoYwllD4kI2K7zVJ4TFZ7/u/lXDzVqf5j6tSInupzI1to3rs+oFObsHlxhucuL/Pt7/0xvYMeR8OCa9euce6Fy3xy/T2KfIzUmrWaY32xhiPg7XeecO/xmNf/0ZvI5AhZTLFJxsb8WVZ+dYu33/8e95/ssD0YUOSGybHH/GKM7jm8mQI/1GTakE5h0ncUmSCOA85tbnD17IuszJzlg3eu45UDXkjHZJ0Fxg/vUaZj7C99g+H2EZcuXiBeO0dx8ybi4XV2Dp+Seg2ElJS+Ry0znJmV7O0fcuX5F+gZy/sf30C0vsrya1/j5nf/HVPhc++e5mLwFP71/0y9s0m7USdbuEYr2eXhxx/RtxsU0ic/PKa9YNFlQU3CZstDe8Xf6fj/NLOhf+6+z0ZQf0rphs/7tSMu72d4f/hjhh8/JH99k6UvXcZuRRgxgfGAcjTCDgrK7gTtlZSxRtsJWZpS2gLRFDTXmoSHCmU8PBkinaS0U4zIKV0JFiQKTUbqxvi08YQid5OTeEbwrMRZwSAb8aPDb5F2Rlz0XsSKBgiBcj6m0FhP4AR4UoHIcU4hhCUOG6w/v8ze3hNMOubM4hw/+u6f8Y9f/1WODrfp9Y5ZjFoQ+BgjyBJFXIB2FpSqzI5zxFENmxck3REr8+v8zm/+c2ZWmmwfH2G1orN5DsUliskAVxYEYQehaminGCcegbdKPU5IJwN8b55IzuJlbUIkNj2Dd2mN5TN1MD4L8wEuk7Q7isWrNXRQUKQToljgzQsay9tkhzvMNJ5jfmmV1myAF4IuBEnfMOzn9A96jI52McMBk2GPaZYja2uo4AzpyOB7JXkak6YrMHEUxYgsT5DKp9Sa7tERzmmKUhMFAXGtTlSrk+cFZZmDlDQaTYQxJMeHRMIw8XzicoBvNUmjRegrVLOFCkKcNaxsngUBXtyg0eqgjaZ3uEcQ1fD8gN7xEUEUoVAEYUhhLNMko1ZvEEYxF69e4+idpxhxwgsVFiM0BoN2ZVVZf5Z15NynTApnHMpIRFniIamFDuV5KBxYSWEU47IkKzRCWHyl8Dy/okB41cnkrMETAlto8qnmoH+EM9XcaUjJam3CyvwA6Vucb376E+9Up/oH1qkRPdXnQlLCF6/N8/CgpDssscpje/sRx8Mxa89/lTe/+iqvv/ACj7qH/Om/+0Pm/YK5BZ/OQgiBx3u3hkynOcgJ2/d36Q6OaQYNysLw8pVXkGFIc75O+6NvsvO0C88JWmd7+IGhyCtjlWtLMoVkINCFR6cdc2ZjjWvnn+Pc2kViv43vPiDIp9RrBe886rNw6TlG45yZSZ+LZxt0736Iuf0OD773Fng+tdll5s5v0IhnKc0S29tHtFzK8mwHpwKYW0Iby3t37vFf/ObzbLz+Jh+8+zaxLhmvXub24VMaj24SS4dT76NqUIwiXD1hZn6G4WTEbFRdpCgtC+kN/DO/+Nce67/Ygv/LNuH/KlP6k/f99HmAz7Yr/vZKx0N8VTIRoKwlfthjtN3n8MfbtL64QP2NAKVyioMJupchypJ8f0KZp9itkCKYkI5z8mKEaReUNUeQOEo3wUooXYIhwwmFRJE7i7YZGSlCNbDCUsi8wuk4ME4CBiUg1VM+6L3FobdDYL5Ow5uvyllltcUmTjbflZKUpkAoxeaZVToLDR7sPsWfWeS555/DE030tI83PubHN27yxdoMS2sLCAFWa4xRSHycrEJAfeUz04wpxmMm44ThZMqs8znuTZimCQIfqToIBEEjxvOgLArywjCZJBztlXhhm3pjkTiu4XuOwvOodVqoMKJR9+k0awR+m7Dj0Vjy2L6X4uUCVwrCdky8GFdMsMRx7twrrC68zGQE46Eh6RbY3OEHkjAyfPyDP+B7P/5X1FSXX3v1l1h8/iUejGImuw2S4xSdFeRljsvHtDC4PMMZQ5IkJKMRKohIkwlSKUpt8KSiMzuDKjW97jG1OKLVaKEHfUa7j4nLhDkKRv0jksGIUhuizoS55XWM55FnGXkUIOQUz/eIak2yokSXKZ3ZeZTnkxcFSZKQZTkrKyskaYouS6SE/vE+9WYHrUuQ9iRR6SSqE41+hlkCnBQYW01yOu1waYEZlyjh4fkC6QmK3CIKwDniwKMTSzDgBx5RrY5EI6VEZyV4ESqskeeGwPM42Dti/2jCaJoglcIUOR1Ps76Y4886hAF36kNP9TnQqRE91edC2gnkzCoX2ikLR0N271/n6eA+/8mbX6E1u8mbZ17CL32Ot2+zFE2Ze2EB5YFXC0FZvv5qwMfxIeNpSX/vFiLqEDdn0NMJH935hC994Q2WF86wduYSmuscH1hG9w2dMwkiKsimhuHAMB0qbBkx22qwtrLK1bNX2Vo5w3x7Fl2CDmOWF69x//pt2qkkjKHTaHC4u8/mzBXMfIfu//0H2OmYR7ZNtKBYr1lCqTkeF1y8tIWvLbVGnXLY4+qLL6BkQO+4y52HD3njjTc5PNolHu0x2k1onrvIgbfEaHeH49SwubXCca3O44FBjwuCqM7uYEIpUxotyYzqMzz+5G883j87eukkbkY8S3wRJ9Dtv73q9Rrn33yB+LmA8TtP0IMJs7Mz5JdbHH28ze7dfTZ/bYnEHFHkOUJLxKzC9jLSuz10JyPzNGWaUyrNNJ6gSouyEueq5CJDiXMF1jlKW1YVLSyZneB7AVChdGouRJQZCk6WQzyctWwXj1mwI9pSoW2JPIl39KSHMwbheTiTI1yIyQ3e2NKJaty4/gGBDAk6AWnSJ26vcL5znTCQBLVZlAA1LhA4DBrnPKBkuT1Po+Ux2R/QHQ7I+yMyLyApC0zNIZ1B4qr/Fw4nPJznUUymSCnxfIfRQ9JEo+QswsUkox5p3MBb3cSUCj1wDD8esHmlhVCCUis+eL/P7t0pW+tNWgseepqh9Q7t12apb84TJZJoL+X+//cukamx+MIGzfMtfuO3v0KSv8XRfhu3/OsMkotEGdQXHVlUMNnr4/KEMi3QyQBnMoSIUMojiEP2Dw6pxTFFklBvNgnDCKNLsnTCzMwsndlZ+jtPSftH+DOzCDVPIfc48pp0ghxDwaQUpPtPqPkednjMpNxALK4ynUywTtDqdPC8iOl4iHUWpQJa7RmkgLLUSATOlHiej1KS/vEBQb39qdG0rqgqoO4kFUlYkBZnq5Y9psAOcry8IBQO6YWgFNpUy0UCXdXipxanQ7xag7pzYDOcEfiRD35IqTXTfpeydDw6GjPNMrrDMdOswDmQWBZbhk7oCAPQGk5OvlOd6udap0b0VJ8LaeP4zs2HeLYgmjhGo5wHhyM+vPcHLDci3pv9Jmphnffuv8fWDAQTwXhiiZuKleU5NmcV57/aZOfwmHs7GbrZQQhBZ+UsH3z0HudfvkY5F6MOHXPrEVr1uPt9S++2j1yBcCWlTAMCW6PRbLIws8i51S1WZxepnwDiy7KkoUrGH3ybzY2rfPk3/jOkgnR4wHH3iCcHhzz68D3Emcts7+xj4yZ5ELObGMJanajVxhYltU4HoojR4TZvvnSeQRFgL23hrGNhcZXLv/ANxsd79Lef8LBv2Dsu+eWr10j2urx9YFhcaROoKZ6fMbt2hvr8Et3uENsJ2c3XSfoJ8OcrluLP/eEzfVrhdM9++FmfXfDs8e7Th336FALqtRiHI0nzP8c9/Eufn//Q/FoBPZkS1hOir3QIvWVELUT4U4LFDvrDCfnNAfkFTdaY4tISMzGoWUfBmGI3J/cNuSgpiowiiCjjAJsLhD5BhjqFQVc3VWKcwTmPIh+Tlw4nBHUbERmNJkOg8F21ZGKEq6Je3clGmydPtqc1ygocJwlGosLz7O8fExaOlh+g9w6YPvyY8PI1kumIo/oqjY0XWJpbQFiD8wJcIBGewNgCU6QIaZhtnCdPE4rpmNsP7+AdW8p2i8LLK/6kLgg9rzJASmCso3SawmasX7zK1twZDh7vY51hbnEOXMjx7pBxb8petk0YNlAqxJYZ248DLr20SbMVEzRrbA8SJnrIfC+g7E2YHu4z+s4P2Hr9HMubazSjGN/r8ypctRgAACAASURBVODOdQ7H11kvVlh98TW+/LX/iSd3fbAtVtZCopokn2Qc3spRvsQuzjBttyiLVXRYYOxdbJkhpWR9YxNfKcoiR3oetVod5xyT8QiHIE1SbF4QLa+DUhhdsFhrsbG9z/50wFQA0xG21qJYWCZwlnIwJG8vUBYpSnkk03G1zLX7mIW1MwhPMhoNiaII5QXEcYT0FEaXeEpR5LtIqU7mPjXGGvTJzeJAVkELQoBnNG6SQl7iBT5+4IFUSOcQgURhKLMc4wROKTIkFAVShTgnkRhsUlVjjYZeL+GwP2EwThklOaU1lGW1We9JwbSUFNaiT3Bz+pRnf6rPgU6N6Kk+J3IkuseGqnAxExHghyUvXZhhrl3j9k6fKAWdaT56XLIw02Z59RzjJGXnkzG1QBJ4hiwzWBky6O7y6hsvsDx/nm//+9/nW9/8Xbaeu0gx6nF8eIg1AUmZcGH9DEZ7DO4dEc4WtBozdOqzLM0tszAzR+SHWF0ymRRMphmvv/5V9PIsyfYR+9/5PTrzs5jGArKc0L1zh5p0lGFE+/LzmFzTmekwzFLcZMj5rS3C2RbNVo3pwQBpNUprVleX0cqrWrVlgrSao/4ELWOG+oin44Rxo81Xr2peFjPc7RoW5lqMDh0y8FhsxpxbaDOapux7FzmY9H7isFZVS6DKzOYkK1tUplNQ0at+UsLx2WPcCUJGVMsWuMq8SeDSmS2Wlxb51vd/SFGU/E29+b84EuAcFHlKkZYUqkmIoRYIouYKszKkE+xi+ppBkmGUQccFhc5xRUlhJxg/o+gajDDoQjN5JSZsz+L1StTQQN9hhjkucYgCpBFgLIrKOBo0dWr4ZopxCQaNdApPKEqqKEZ14s2FEkih8DyBdQVWgJQBDot3YpBGScpeWbKyssBv/Y//Ax/80b/h5ltvMzQpPS/g6EnC8tZ5WjqtgLVCYYXF8wVCeXTieWZmZimmYwZ5yo/f+j5fO/8LlEWBpgQDEo/SaoQKcKLiUCbTCUJKttYWOf/SSzx5eIbuQcK0nzEZZdRrMwgbUiQZZZlVbF1j0cd9HrxT0tqYpzHTICsMRU0xSiz1dpNGfBW7v8jjbx5zL72H1hPGo22y4hD/Scb+eMjSzjXS6QJbF2Pi0LF//RFJFnAwfMTEPSA+/4sU0xgzKalhmDkL7ZljHAVRHBLHdaSUTJMppsjBFkRRDWfqjEYjhsMBskjInaU1O0OZW4KoztnFiKaeZzTN2bj2ImI6otO+gvB9VDLClBmtZhtjS4zRKM+jtbiOCmL8MGAuXgBrSaYjhGxhygJjLDhBe26JwXB8kvleYF1ZMT4xJ4tFDoTDlw5nS7S2+KGPF3ggquUkUxZYLfE8hYxaxIFgOknRNsBmGmPGCGuI6y1saZhmOWliOOyN6U5SesNplbDkHL4fUBQFCMGwFOwP4dwcFCHkyd/9N+6pTvUPpVMjeqrPhcJQ8bXzm/z43V3GRcDGyipRYFhdrhM6x4UzZ1laP8OV5QWmwx2G/Ql2eI/xMAXnGElJEHhEUcytwzHzyyFPnnzI9jvv4MWC452HLK62OdrbJc9B6SZL5+Y5Ppjy+vO/iDGGx6OPkElCrRXjOUmRpgzNMVZAVmTkuebS/FXWfvkbZFPH7tPH3PvetxDtGH9hns5Cn2n3iETC0SSl1DCzBGeW1lhd3eLO7fvs39um3/Fotjp44SyuHDOHIwxryMjy4Y3rlPi899EnKODs+iqvXmljgzZjAuqRz6vPdej3DulOc1584zUy52F2Pma96NHLCsL2HHCyRCFBeRUn1Rlb5a8LWUGxPQ8pJb6n0GWJLkuM/QtlUykIPI9C689A2676/v7+U4b9fYzWf+W+0l82h/pMAoPXBNcMcUUF+U7lCCEc6viQcXIfOy3gOKach1KUlMGENJ9g/RKrDCiLmQpMbtCXZ8hePodEoEpHOLXIQY48TnGDBK9roVvgjhLsYQrjCbZM0C5Fi7zCFOEjkEiqYwemMqJCIFU1fqC0QlR0cSwGD5/EJox6u5igQbNRw/NmeO5XfoO5567xR//mX7Nz9z4bzTrZ7gOC5XVU4PBzgRU+JZrAD9hc3wBrKIqcqdaszzaI/JBcT8mnfYSpEwQ+Qnl4fkRpDViNSVOa9RkOD3bJtz+izGosL6/TuhTg+RLhgzaW0TCje5zR2xkx2tsDV6DHxzy92cdrN2m02kjXxNZDdg7u0UufsDF7mdUrl7DjgqLIKItLKHlE3OhShCmel7Jybp5WW+EHgs0vXWQ0dMjBAseHbSbTCe3FRRbWDRdejam1Cm7uZ8zOtsjLgjydUqvXqddqjMscz49JplOc1TRqNYrAY3i0gwpjytKBH1EPPKaEzF5+hbh/QLR8gcHuHupgj87GFtPJGBULiihiNOzjeQFxvcloOEIqH9/3KiSSH+BLB2kKUmCswRpLXuQEfvDp/noVvWlPGKAO4Wy1CCWpyvpKoaSsKuPOobVFOKpqNxZjcoT2QEmCwOFkBFLhCYikYlLmJFNNdzBhMEkYjjNKfcIXBazNT746hHN4CqwGXZ5s4J/qVD/nOjWip/pcKPR8btwe0B0WXL26QZo6sqzgo+uHnJ/p8E/+2/+exc0NvvP7v0uzVePaag2V9DkewpW5gkJr/p+bBW/tGNYWA2p1w97uEJMaIp3TXprjaO8phBoxVWxuvMTy/BWum+8zomRxZYm51S8zuv8jhkc9UBItLQ1RQzvLYNJjPMn5wY/vM3Nws8rsaS5gl6/x4pu/RL3uo/Ur7HzwXQZScXB7ly9+6TVE7wHZ7n10LUDrIS9cXKHIEu7vPGWaPObcubNMJjmZmNI7OmA8HjEcJawtL9CoRWysrWCTHmHsMSklw9GQyUfvI7tdWhcu0+936fg+VkXMbL7AteUN7g5G8PvfRnmSej1grhWQa8000czNLDA/vwzAKy+/QbM+S15OufHx22w/fcLD7b3qQiqqGdDNlXm+9tobfPP7P+Cw16tMGYCzTPKM3JRVcsxfUQ3969ikRmqmwQFKeqAEOprFmIzpdJsy3mVwLae3u0Nj1yeWbcZ6n5ISm0E+ysBaglDhgNJaZhoN/CjEqarl7WYVchWc1hUonZCGCVD3Bgz/t29THo9Otp8NOIEvPAIRYlzVlhfPZmChQvJIAVKiAONKnPMRVf8fk4/xhMQEsDvYR+iSWqdBpzXPi699gR19g1v3d5i58zF1r8AubKFEjFA+VqfEXp1WVCObJuR5wTDLWF27iisUu4NtMAVNP0Q4n0BIrKtwP4UpkZ7CjyKUv43xnzDp5Ux7W7Sn12g1FzBlQac+R9krKfZy9HEKicVoQaE1eTkhLzImxz1GOzFzrTa1libL9jnwSqTXIAoa1Os1ZhZb1DoNbGMTPB9yn8n+kAcfG/xODdGOqZk92nKEnr9Cc77ATA/Z+uISq2sxw17B9fc/RD+3iStzFtY2UH7I8PiQstDsP31EORkQRDF+cxZtDF5cpzG3xMFxl2aryeRwH3/1HPl0iuts4UURR/0etUadMIwoPR+TpyjPp97q4HsBpbbMzc0RxnVAApI8TZmdW+Lo4/cI1s6BtRhdkk5TonodIU44w1S7W/ZkVKPCMgkUAiscyq/4oVU6m8ECXuBjdYm21WZ9qasPf36sEJ5CKR+bjMkJKPOS4WhCf5oxSguyosQYc9LMqM4fKQW+JzEC8lJgdAXMF/I//IB3qlP9vOnUiJ7qc6E0K7j7OGd9JkT4AZNuH1cUhMbgLTaZaS0Qzy3RCuf54x+/jfYlOhQM+gWPdjU3hoIHY0mRTnjlWodeYZgmBQutJn7D4LUUuZmS5RlSetx//F1Wl15g6/w5Bt37ONdB+Yq1tbNgJmitWZqdoVmrI4SjFjqe6D47YcS9pMHm2gpP93s0dr+PevoBTHuoYJ7dyYTOl36Zf/YLb/D06S3uvvU9BodH1O88ZebKJTIVgq/YWPLRVhM5TUNp9g+2qYUBZRBwkB5Tb4VcWl1lbraDipcoJ1PGO9vEQQ25tIzDp3xyl+j8WbL3v83cTIPQd+Rig8Lv4AUeKzPrzLdqJNN9rBVc3Fyk1qhz9epzaK0Z9R8gdB8/aHHu/DUuXf0Cf/Zn3+L2nVtANWMpbY4oM85tnqU7HFRMTah4iSfzkwJ7Uib96eSsI0smFKKgtA6yA2yhafs1RD0hU1OMlOwOh3h7h9T8Otpa0rKAwqAyQXpoUWG1XhSHEbHwEdaDEsAinanQS36E59fxCo/Jn3yf7Ok+WPHpqILnqhQmUDgMz0ZenauKXlY4pAMrFK5yJSBMlUFeZngK5FwHJwSZkuwWPdpPDmkuLBIsbHLvyXcZjkrujEPWZjrYtEtNrCC8Kkc8jurkec50kpBkJf3xkMDFWFfyowc3qddjvriwiu+FoHyUEAgvQJuCWrOJ9ASibBAHCm8xRw3h1od/grEBM3IT32xhZYBVPmo+prO8gdbV6xaEkkhp8mGKyQtmmgEm9bhY+zrtTkRjbYaDvZyjnqblAsLjNp4p8BqC2Y0WXlMRrTuOjzXlNGFG9mhMf4/65m/z5Mkc2XCHjSsxvac1uns5M50OtVaLfDLm8GCPyXRKq9VibnGG+3f6mMKRJV02lrdQnsc4GeD7AbOdGRrtNjtPHtDZOkumPKKwRlEULK6tkHYPgecJmy3KvCRNJrQ6M5iyIGrWcQ7q9Xq1EY/ADyPSvKR27jLT8QhjLGFUQ3keo/FPgOJPxjOkkAhh0IBxFmtF1Xr3HMZUiUrWOaSUIAXWVS5WSInAIJyhSAwqqkE5IhIeunSkqaF0CmOrWXlrTjKVBEgpUUoRhSFKOhqxj/MTNNUCE/Z0WelUP/86NaKn+nuTEEIB7wA7zrlfF0KcBX4PmAPeBf4751whhAiBfwm8BnSB33LOPfrrntshObdaQ9ZmCAS04hSdp2w0FMuzdVTYoszqSMaMDAyMQmmYGp/33TJ9OSXPj3n16hIpKaM8Y66zStTyaC2u4oRBlw1sXiBVQBw2ONp+zNrWeUaDPY7vfZ+LF55nZfUcZVZxDWVmWVrpEPoBtUCRpoJsyac3XWNnmnJQKlwQM+w+Yd6DwXDEgW4zvP4Ba1GDo8dd0u4Q3yo2zp7jaHcXsb7O0JshiOqoMmUy6XJwf49We4WWOWbeL1h/YRmTp1xemWG21aLMCkZhjafqEtI6ZlbPsN3cx3v3gMc7B3Taa8S1iEAGzMyuEduc9dUVFucWmQz6PDhKsdYS1yGohdx78ABdFuR5SlwfsDCzwnQyIjGGS5eucvf+bYyuWtP9ZMJb7/+AF1/5VRb2dtnf36sCqE568eLZftNf9br+dQbVWbRNSUVezcemGlcI4vmSQj9lbEakKqMMNUkOk8mEOFeVuZSWsidJD8Gb0QRxg/bMHJELcCLA4WGFQUqLpwSKAFn4TP+P7zL+9g2wtqqLOYdDIUWAFH4Fl8dVSCdRbUXLXKBKiQ3Acw5zgm5yWIzRTCY9mu0OhSjAOazQjGxGrkvEQcp81OKVq2f47rffJRn0OfrhD6E9S7Q1SxhFSFcyU5sjSUsOukN2jm+RlyWzYpaJSzkYbOONA14uX6ahFNKT4AkCGZCkooqHdAY9XKc2eZ5W2yOYm2Xhy0f8/+y9Waxl+Xnd9/sPezr7zHceauyqrmLPLQ6iWqJESZYsy3FsODYixEgMJICBPAUIggR+S4DkIUCADAgSw4YDBAEcSXYc07EYiJJIiZS6SXVT7IHs7uqqrvHeW3c889njf8jDvuzESawoNmWJwF0vdfapXRuoew/2WXt931rrvXff4vlPv0LaXsV6QVFDkTmiSLI6DAi0pJtKOrFg/KhgdLqkt5Ly5P4ph/tjRpnkwdcP8E4SJCE5CjlM6W/32b0V4azA1p4w9hSlYzpaMsvbROFLmP0v4+vXaPcSzu6OWOQWP5+y/ZnLBGGITyWT/X3uf3iHq7dusb6+yeraGuGly+AdcZJS5QvMOcGL4pBiOcdUObUxtLpdpuMZSguG69tMipwgigjUkPHBHu2kGfNLPGnawQvIlgv6wyGL+QwhFVmeE8cJSgcI5Wh3OuBqsrz85GP9iVHP+6Zw3jXtT5UFrzUykPi6xnmLkBIklEWNVAIpImQQYKuCQIUszkZEUc1qt81aO+JsWlEVJYiGuH5/xC+FaBrCAClAySZf1NU5IQYhwct/rue/C1zgXzouiOgFfpD494APgO758X8O/Jfe+18WQvwt4N8B/vvzP8fe+xtCiF86P+9f/8MurCXcevYW15/9DK+/9b9z+9IQ1c+Y5YK9J/f48q/+HcKoxzQ/hP4KdllzfHLSNJ8EOflywbPXtrl5VfOt+yOWlWcwyCFQ1DaGrEOnv0Y/ajNbHnN57VWq0BPWMcO0orveJgoV3rexUlDmB+T5jPHkiF7aocoXVEVGsQwIgxan5Yxg0OPQwpPTkvj0jDRKeOXnfo57b/w+3/7Kr6DaPbae/yyje9/Dr+3SLnL88T4rtzZpt9ssxqcs84R2O+H45JS4G/DstU1WzSHr159l5ZVfQOgEs7fPYnZIZ/51ZukWBx99l8vDK/hPf5GnTz7muS/+OdoqxJw9Ivvem8wPztjYTinyMw7Ge+hQ0u30SdOUyXwKUjIazZE4wrjN2WTCwf4jTsen7GxsIc7VmFCBFo7C1ozHB3zq5k2OTw4/2V0TCMIwIIogz/8ZIfrfd+N/EjMjPnnb4iiqGuEEri4xlSPWbRbZAaU8bQwc8xpbNSPxrM4pPCSlJKhBVgpvPc5BtLpOdzhEqhaBSlEqxgmHkBaFQDrN4msfsPy19xDWgZB8367VeLMiLAaDwQqHEQ7rLXiHKjzBvsDsCuq2af4PEiw1+XxMO01BgkI3AftCIIRiFuSY2VNaixY/MpD0XrnKo7v3uX8H7CBn0LtG1DXU8Zw7H7/Jdvo8ZlGRL54ifYJSG4io5Cd+5CZFbQi0OI/eV0ilcLbZR9TKYz2Uy4LTuiSWIbJYgg25vv4i+VHJ8vAUGaV012JWL0e0O82eqygMQV4QW0M8uk9wNGJpXuThx4cc7Y/orWxw5bltVndSWl2NCAQWKHPL4V5OqxXirUCHguFGSNjqMT50zOyPcvszv8CPv9Yj+zDn5A+mjI5OORw/oXxuSeXm9Dop27ubKAlJK6U/XKXM58StFCUFpq45vfs9TNTm4+9+yMlozIu3nkF6z3Q2byo6TU3S6pw73B3js1NMWfD4/ffYBeJWh9XNHaqqwtoKU1uyeXju0o+o6pogUEStmHyxwFmDqQ1a60/qN51vFk+s97jzbnjjPN4JSi9RWqDCpqo1CDTOOITyCClxCLCWepkTtVr01teJpOf6eotimjOb5SxKy2KZAw3hlFKgdYAxBkTTW5/lGYFWhNIhpKPy33fMX4zmL/CnHxdE9AI/EAghdoE/D/xnwL8vmuWlnwH+jfNT/kfgP6Yhon/x/DXAPwD+WyGE8H+IPOac5fTpE97ZuwOFg9MUUda0U8mgBcXjb/DuLIKNHcKki5gfMeykPHdlnY9PZkSDNpc6NXf3JyyLkFUhmY3GjLzjctRha/Mma8MN6voQLwzd7jqH8z02V68zXNvBlXOyasx0OacsC5ZFTSfRfOfO+4TSk9U1RzPLZzY+zVmoGGjNwem0UbgWnlkZ8toLL9HbGDCdTzmdG1bnJ/zi8zc57b3Aw/EewbDPvY+fYOaG6bKkM1zDecft3RWetd/jav95WnaOfPw6Ni4Zv+2oJydkT+7gyiXFaI7JI3xZUT++T98bxt875J2HH1DnhqODGTNh4Po1OjcsalFwRQZUkwjpIyanY6KVVZSOMWbK2soGUvfw3jOfzRDS8/jgAK1Det0W3ajEVQUGyd7jD9m+8im01lS1QQhPEAS0W32MW5Ln/7Rr/v9pUhKfHEspsc7hBSzdKc56nJWEQUqUGHI7Ic9rqpnBlZK6rlG6icMp8hInISgipDGEbUUgNVsvPk+YDolUi1bQJdIxQoB1Fd5YyocnlN+6D6UDJxBC4pxDIlGiGcuXrtnvs96f78I26pQQCp1rOLTYq2ADh1TgbY1wBuIEZ8/76MW5qxqHEBWLxONmM3auf5ru8ASqE16/P2UjDFF2SZBu4/0Zwi/JZzNsYZgePGVjcBPRVxiZM1zvIcMeKtEgLFIpvHdUpqSVpEjtsMazmI+pjx7gZkMCX+G8wUWK6599lpXNIUGkqCrL9Czn/vcmZNMnbKUjrq21EVGLvQevI+eQ2x3SMGWrf0DYeoSgy9ETS2U8XkpUEjGdVsSBoF5ktDoxaS/BGk9ZOfJlzWh/j/nhIXe/qVmORhzdOWA6OaWKcq795AbdYZe03SKKAlbXN9FBs2/rtMbUJaWpEEKzdv02Imrxzodf4fd+7/eRdcnLGy3iwRApJVu7u0wnY6azKcY6luMR/fVNOv0BSZKStFqcnRwRBBoQJGkbayvipIWpa7ppCykltalJ2l0ePXrInQ8fcv3WM41r3QKuqXl1zpwblhwCcEI00V5SYM7H8UIqPK75HSEQDmxVknQ6iLSDsxV9XRKZkswYoigmjirCokYqSXaeGWrOc5mc92ilkOcqqRKC8HwaUVvOH6MucIE/3bggohf4QeG/Av5DoHN+vAJMvPffT7LbA3bOX+8ATwC890YIMT0///SfdXHnBFVeE4bQ67aI0hVO9mdsDHusrIc8e6XD9M093n70iL1xQVaWXNreJO2ssjY6YV4XPDm0hCs9YlcRDteYjfY4OjjGGk0UpZT+jHH+iHYypDQF/f6QRT5mONikpkPAksXslMIYvFSsbd3iq7//MQejBygtSYI+hXnC/OARsQgJVIwhJpc1QRgRViXvf+ubnOqYldWQqTD82lsfsDVI+Mt/7sc4W2acXbrJZFzyO++f8J3v7NFOY4as86IbY48/xpous0nO4mtfQct/glIGHYAxkrv5Dh+7NaLOgLPHD8BFFIHkYD9DFDVqdY3o+m3G5Yx+Z8qQIaW0PC4OKJaesshZT/r02202X71Jp7vKe999nWFvDeUsbakwScwz17ZJWjH7D+9glKI2zReiNSVpmlBP5oAgDAJ6vSHWC07PpnwSDuU9//cGpiDQDPoDrl++wtWNPqOzU/r9NjbLMVLi8hrrcqCgkgvKLKdYWqqiidTxWiC9J/AaSoV1Eh96lHYQDlh54Tl00CUJOrSDLmEY4L2jPhlz9Kvf5OzLb5IdHWNtAYhzEiqagHphsL45VkLjPGjOMx4RjYveg14KzFxgVgChUFIT9QYggybe51z9db4GDJKcWiwpuzFl3kdWAWF7jZtXFbd+/BqdwSZaRygjafVXqY4KXA7TwwmXVxI8NdNigo1KYmVQioaQEGBMjbOGOOieNwAVVIuc2cPHlMdHtLQj1DWqHTHeC8kmhyyyUxAL8rMzvvPBe4T+hOVglfnKDr1On7jVIZSCXiekric495jOZp8ymrEMIga9Pk4I4oHkmaTNva895cE3v0NhSmyng9MBWnhsNkWaCfPYIKlZjMaoEETqoA3buzvsXFljOT7FOkiiBK0VtjbE7S6mrqjKirTTQrdT5vMZn/2Rl3j11ZdpJwHz/fskSUJRFtSmQirFysoKYrhCWRToMKa/dYlqNiJtt1ECwjDCOQjjFsbUTEYjPIJ2O0aoGOcFUoZU1vGbv/EVnnl0i3TtvEce2XjnfdM7DyC9oImYp3lfhgjtmzpQBb5xNaG9R6QpBCHWWVquZDdxRFVGrCL6vQ4zm7MoKqaLAq0aRdRah5QSKZrXXjb6/bSESQlZCS4FV//z39AvcIF/Wbggohf4F4YQ4l8Bjr333xZCfPEHeN2/AfwNgFYSc2hC1lYk2gd4KxkMh7x594AHJ20mlWZzGMO9YwwSL+Dw9IQH9wqOJyWT0nDr0iVeeekG33zrDkNq6l7I/onj6eiAaXFIe6CIE3C14r3vvE9frdBqJwxX17l56TatOADOMDhUqHlydgcfFKxvDpHKk4Z9doaf4+jhKfXiAc8ONuispkgluH+U8eTJHjs3+ohul7mxdP2Cpc2Z+5iv//pv8Pmf//N89os/TTHN2Q7/IV/50HJ/Jvno6RQXdbm02GfzdA9fOvIyIpdtxtOCWZZjF5YzZrz2Z3eYPr6LCDSFD9DdFiMfMIoV/e4qYuEIgxhd9Mk5xvYnZI8zirqkDuCjh98lDHr81BdvcvPmNR4+fp/t7Svce+8NPrXS4f2TKY8f3CFuxcynU6QXVB4CJTk7PiEINN43ik+oJJqaRbYE/CeRpd8noUII4jhmc3ODdtJifWWTRCt6cYv74wmt9Q6zfIEzIHJPGLYIY0dNjbEOj0F4ifUeazzaC7xXWOHRsUc5iSkEW5//LMnaOkqGSBXiZUMI7MGYR//Fl5i9cYeqzLA0YTweixCNYlVjz5uUHB6J8JKQBIvFiqoZhEtFEEY4Z3AzcH2JCUu0D3GiaTgS8pyAew+2RgiD0OC9xNglhT8B12aw9gzxNUtvI0YHIGpHEvdoxQMWdokvPTs7WyS9Drkreff+h9y4ttKM0pVHaImVFlfTNEOJZlUgDmM6PUG60aalQqKgpDZnFOURH7/7lO5mwmAn5tmdy5xWIR8sZmAMj6tjKul56Uc/R7e/wr3332GYnrHenqHdOif1Lr6zzeX1HnFbUs9qxg9mPPhozkd3HrGIWsysw4xGxLKmG0q6nQQvV6jMDKsPEc8+Ra/DqvoUa1uvocPmoSXpdKhry+j0mG5vyHw2I4pC0k6H/krC6PSYMFDUVcXlG89SVzmjgz3iwSpR0gKh2Xv0gMFwAAKiKEIHIVGgWdu9jJaKYnZGOwpRkWJ8dtqsTSjLZHzG2dmUl155ibqqWSzmKF2QRBrpaj6+9zEvrq4ihELI5veqpMZJh/LnOqRtzEjee5wAH0aUziOcRylBCBAHWAS+rtBCcrkjWQstBCmZCKlO59RlhZCSfxqakwAAIABJREFUtN1imueEWjUKK+BcYyhrBHfPrIRZJSisZ56BKH5Qd+MLXOCPDxdE9AI/CPw48K8KIX4RiGl2RP9roC+E0Oeq6C6wf37+PnAJ2BNCaKBHY1r6p+C9/9vA3wYYDnpeyIq0HePzkKKSbKxt8GTvKWcnY94ylt1BxHoakYcRo5nn0jDi2bUYiIhqy9pwSJbVbK4O2D9+yKP8CA3oJuoPrR1SKJyR2LqisAvsImdRHHF69hG76zucmTmFzQlbEUmasLMzQAaAdGgX0408O1uXwG7z0udewyjJ7/7GP6Abb1CfTmjLio31de7unSFkyG7LsBI5tl75Sab73+TB3/9d9Man6Izv8oqfcSy2mJuE3zmyWJOQuCUdqUh6Q/amMxZLTSIS4mVGaOfkv/5lUmEwRtEbbjDY3Obe03vcOx1xzcEwzYnX+jzNHpOHR9iZZHZW0Q7a5GWBqQWDnZucncz4vcNfR2QZo5NDIukZ5xWzqiAIY5KgQ2c1wZqaypaUJqOqS6QUSCWRAtqBQtmKLGv228S5wQI8SgdsrK3R6bSZLcZk+ZzTySlVUSC8ZjKboDcGtNZqnHW40iGdwsQVVtW4wlAvLHiJDsBZgbMObzTSN3E61kH3yhUu/cRnz6N3KpyuEUTYScbe3/oqy28+wNU1lhrjC4QXOPT5QLNGIpvMR6EB1RAPRBPXQ4QVIL1CWokQmmDuMac11YYEBFKB9bbJPj2vPTWixlPSUImSWizJ1VMidY2V7iVm6wegy8ZNrT2xSlFKN2UBvmZlawfVSqiXp6SdEXHYJYgVUjiEUI0KKAOkEIBD6xiloN1P0Jsamz/BuRxTlSQCLodDru/eYqEdb3/jHUand5Ey42G+T6JjIhJ+63u/jQgcka/53EoCZU1pIReXubw9II7g4esfUM9jkktrVFsrhCW0pmNEYOkNB1z91GXa3QGz8YI77/5j9p7+L/hkgVYDWie32L35PJUtKYqCs5McY2qUUqSdLnmRkecF7W4H52E2nhBEMdl8SpYtKfKC/tomKog4PNyjkCGBUvQHzYAmy3KkkggJQkmSOMVWOd5B3FthmRdEaR+kQIcpSbvi+Z2r5FmG1AHdThdX5CyKnJXVDa7fuE0pH+O8RuBR4vxDR5MNCg58kxrhhQUEQkmMANCNjq4lzlq8dagwIZWwnniqrECUI1QpCEWCloJOO26alQINiWBZ1o0hTjVVs9b/nyW6k0Ji7HmC6MWK6AV+CHBBRC/wLwzv/d8E/ibAuSL6H3jv/5oQ4u8Df4XGOf/XgS+d/5N/fH78xvnff/UP2w+FZvSrWoJeP8TpkOOTmtliye3LPV7civnGnQlZWVN4waXUspW0SGLF3XnEYNhmY3nC06MD+qvPIKKcJ/kxtajRGtA0AeKhQIUebzyltBS1wTsPQYXXJffPCmSgEVpDCUkc0mp1iBONVOBKzZf/57+HKQU/+dM/R9pN0XGPmxuXGT/5GoetlGD3OXanU+LdNY5mGdPBLu9WFSd39vmxSyvsffAHDO89IHAV8ShgoDrMvabT7mGEpq6m9DeGFEVFL4yJzB6xCwmHmmpRMS5qpnHC9VeeZzswfPudR3ycW0wrZVpOYXFCL7nMSf+EPF9SLgXeSaxZ0k5iJkXJydkTxqN9QmXZXNng+OwRNio4zB0ewfbWdS5tXCJfjsAUdDodvvPe79NOY1qhQIcVrSBgK02Z+pqyrpo8Q9+oonEUcuXKDsLDbHpGUeVUtaUoaqxx58qpBwcBCluDdwpvbcMurcRmIF2EsZay9EipUEISBA35k6WkqA1q2AdTInyNdyU4wEjG/+gdsq8/QLhG7ZQ0u5tOfD+g3CORIBXKKwRNNSP48+MIJxzGe6RXDe9wDuEd+kSgWwLT8SilqW0TESWVwkuQtcPZGkGBosLYDKMlcQeC0zYtcQkjDwkcKG0IgwiJAG8QzpEkQyw1OnJc312l1VJoDIGWKBTCg9QATTmBEOeKWXVANZngs5xeJ2B7QyFEwWj5mPe+/jX2J1NO5odE0mJtzKwoULUn80tOTg7o7XQITMbsZMpma8ArN17i3t3/jS+9XdCKI7xRrG2/BiPHYpRhFyOUmPPyT97m8rPbVEvLx++/zXtv/h1OTl+n1e5waetVrtz4NEErpsgfcPDoEL+9weSsIE5Tev0hZZFj6pokCfDOMhmdMZ+MEErRbncYrrQ5OTnGWcOyqqmJGI/HaO/odLuk/R7WWcYnJ+gwojNYRwUBRbYk7K+QFxlCBASRahYuTN0QSyxVvkBHKWm73TjVJ0f81R99BhNo3hwLpAxwzqGFR6omUVR4j3S2abYSHi0E7jyuyYpmZ9Sfp0kkaYfaOhSOnqqQ1QxVnFIVCVWyjY7bDBJNpzYYKzgbT/Eua+LZjCOvqnPjoCRQEiEgd4bCO3TgUdEP9FZ/gQv8seCCiF7gjxP/EfDLQoj/FPgO8HfP3/+7wP8khLgHjIBf+v+6kMdR1jmDZLMxqlQ5hRGEThJtXObH2z0Oi4A7372LNQWTWnPnaEYUVWzvxBgXktUjbJXRbkUESaNYCA8qEAQJ6Ai09pjAEiWeUAkcgqouMFlNS8ekaYd2p0XcatFut4gCTaA1OpA4p5ibmpOq4s0v/T3mv5ERrQ2QLoR5xu3XXiPd3uUof8Sy2GPYTpCuZung7tEJz23v0l69wfGDDxCV4nAB7WsJn91eo6gsSxujxSotXbMczVgUlmB3k2Vl8d4z1ob5ZEw+ruHOHd7JJbPS0os0UZqSW8fHi5KtoENlPYvcUxuPWgcnHMnCUpWOykwwKHbWr7CytoaKUt6+u4+WIUGgKKoMa3NMMUdrxS/8a/8WpZfUyxn3Hn1IgCWNEkpjMFFJfyegmDts2RCjSxuXcFXB8eiU2hjKyuFckxva7I82hNV7T1WWCCdQKkIp0VQoGotA4QqPF4366p1onO6yqcUshed0YXjy1rc4Oj3isz/6Ba7eeA4dbVK9e8jsS3cI60b5tOdrA+fJoOcVp43CKYmQIkTgm3ByH6JR53WoGiloDCgapJU4K9BzR3joMYmHGAIZYOoMR6OsBlJT2QInKoQogSVGl9hwiaKDmvUx/Ye4oEL4FB0mjbHIObwzBDqmpmBh76F1BkEXtMILBaJGKt0Q7/OA/coVJEGLs9EDRk/eQYgCZpb6oSEvchZVybKuyKzBYFFK4Z1AeIn2gvJsyWR6jDqSeGvAv0836fDGu29wNB5TqpBO9xrP3f6zyGAFu1yyvRNz7dZN0oEmDBXHjw558OE3OHj4Zcz0XS5HLf7iX/p3uf6pL5JVhpI9Nl54hlzH/OYbX6Lb69EdrlJVlqoyCFeiwoCnBweUZYnWAb1uB2MtjoLh6hp5XlBWFTIIqa1DSk3tRGNWOjsjTVuMJxO8eMRwuEqr3SZtJZydPCWMWigVscxyvC2xxpLlOTLQIMDUNVrA5qdeZXrwgCdPT/FIvJDnKRIeJSSopl3MOINGgrcgPQiJ9BJhG4Mb3iEDhQxTWM4wzrDerVmtZ1TCsgwjcmvJXYVSMVErxFhY6bexzlEu8qbL/jzOKYkCAiUxzpG2JCI0iBh0eCGJXuBPPy6I6AV+oPDe/zbw2+ev7wOf+385pwD+6v+f6wrtsaKgyGv2D8ZMxw5PiZCaL33jLpd3BrRT6PS6UGgiVyCloJ2m3NrosTeasV/MeDjbI44CRAjh+XBUBY0aKqU/D422KCpkUOCtRAmLcDWmEtgqxNsa6f3/xZHadK174bBCY53lZHWLzewQff8IHyW4pIWMNK3uFuP8I0SrSxy3sFWGmB8iK8PbT+b8hRe7PF7c4Ox4jFlPcZ11jp4uqMuc1d0rJEmMy8ZECubOkhUlp6LDMjNMAYQnVnAwl0xFhFKeAo2Z52Rlxkroka6mMp7KNgTMRR7nK+KBYi1VVDkY42kNLTIdsaxzUCATweW1HR7v73P65BGdSLEx6PM//Df/Caubt7h09QUEgnxxynK+wEcpn3nhs+zmU+azEfPlMb1kk5vbr/DgwT2kkJxNpxib0wQPWc7ZGg0hBGtrtAiwxqBkCAa8VSjrEcbjpEdHEu81QgmMCultXGKyyGD6gDCwnGZn/NYbv03nnW/zzO4ttr4r6GcLWkqhXAtBjhSNScnT7JlKJMoHNNG4TUe4FgqNRnmJRWCFPQ/uV1gh0IFCSYWratSZIOgryi2JkA6pNdYVICUohbOGyk6wIgOZ4xCodIFMushFG1WmOL9EBkNkqKhNgZcGFzTu+9ovKMw+SoGQzROVwOKp8ech/RqNMQVRkKJ1xOpaH7894KMn95iN5wQyBBST3FC5sqkslc2erAya4CpHE0XkrAXT9LcGUjEvZmRlhpYhMt4h7Fzn0u5PMOzFlDrD2YooEqjpnNA7rg40l3/qZzi9/RwffOuXWTf7PHftWfbf/xa/885X2V+8y4985kf59J/5a5xOJly58TKL+YL7Dx5x/fI67bCgrjRhnFCVBc4UnBwfkqQdlIRSViwWc6RUTGYzfuVX/xHP377Jn/mZn2J6ekZZ5KSdHmmrw3w6a1R3CXhLFHcIAk1Rlti6YrGYUZeGDk0xQ9pROFthdUTQXWGl3aNoH6LeeoCzqhn3n/+0cDTEUwJegPJ4BNKHIEICr/FO4JzBOkORzajzOV5IwqhZ85iUkuPMUUQGV8/pb/eJWy3qfMlyJcHWNZV1zLMC5wRRGNLptLHG4sqCtN3i8bhiY2jpXnR8XuCHABdE9AI/FBBAZxAw6ITMkpDhMz3CKGXjyovcf/SQ+w/vsbkxYFEa9vZPaceNK3rv8BDpZ4z8DBt43j96QBRLlG6+LKSAIBKooCkhscbjjMSbisJM0V41eZmhRCsQzmLqgqoMUBqQlgqFMDWyCgl6q5ize9wdS/L+Npo2URTh04CNQLAcnfKFn/wCb775bXxlsCohWl3DLStmos07BzXdS7f49uwRQirMuKQdxKytdZnNFoStlLTTBueRJmZWKaanc/IypxVA2EqQSYoIW1AaumkLYy152SIoW0jhKb3EeYFGNuNoJQiVx3oHSqJih8Izl4+ZTwPKrEbHUNRzwvYWWnpKX1GUnsNRTpKkSHGX7OwRmztXmBcjUCVlEfD07sdUyYBqZqAKWMwnFO0jNgZdqnqDbitikU0pSkPpK7x2GO9wFqJWs69bmWZUH7cA4TClIC/BhgIpNEpohAoIky5bt19h47kXqKXnwdtv8vS77zT7v50WVni+8+Adft86WjcEl2YR1086dCYOUzaqckPomgB7RYSkIRBaRmgfoGlUVy8UmpJQSLSXSGcbs5OTONEE3McnAfVqjQlqkP6Tuk+BQkqNMAWOAkeGqKCOTglbW/iRRGe7iKjGqxIvDabOUUFIHRp8aMnqY0o3JQ5bKKVQUuAweFvjhAErESTNz0ZIvPe8+rM/y8v/5r/N67/+ZX75V36Z2XxGbWuUVmivUbrZY/y+Gxvv8c4TJjGBVrjcoloBvdUeYRgSSkVcSiLdIzNTyvkd9g4+oKscVVszO/N0gjZZ5Vn4HJVq2mmXwa2fhnu/hqhKNtox/V6XPzgq+ejjh2xv30F58NZiy5xuGuO8Zl6EhIls9nytoa4KBmvbSCVxpqTIC6IoYjGfEyrN5159le2tAcvljJW1NeraUFc1g+EA5zz7+0+Jo5h2u4fQgqIqKfIcqQK6vSFRGJEtxiAk0+OnyCSl0+1T1hWmXJJly0+ixkA2RiXrcEKiJChZI00FdQhSI0SE8CEeBR5sXWAXJ+cNTB6Tzfjou0v2TMA6IS3tEUFAreDk4ClxFDDshKz1YjA1ti7RTuKiABlEWGeoqoo4VIwXloUP+fjQsJv+Sd61L3CBPxouiOgFfjggoJOGRLHgZFZzMp/yi7/wGj/zhZ/n0t0P+eC9tynaAVESsKgNpa3ZXWkxWhpOshlhHNKJNTM/RgiP1E04tFAQRE0ttsBjDPjan6s/lkB5kqi5rtYKBfi8pLIeZ0rqWhPUEhHVhL7D5z73IglL9qdLZnmG8Zrd7oDBapck1sxmTwl7LV5++WUefHSPg5NjjNMkKymJ1hyUmlDAFz7zKg8PzwBPS4cgBWFYkmA4PZkRtgd4NSIMIlqtkpVhyOxsQthboddtM88WdLzl/qOHpJ0ew14f6RRJt0/UCxnKJ1jlKcuy+TIUgtKBtb4xVAionSFQkqAVYCuLDS1n9glhT5BEfepySuYsaa9FPOyw9+AhB0/vsduNKLKSrF4wm8+o5B5ZtqR2nn63x0P1hFac8ujpPldWVzGmImklTKbHbF7qMrjsCCLPIGkxPxo3weoixNWeZeEozXl2ZxSA1yiT0L92nfUfeYW0v4qMO3STLs99cZN0dYeHb38dU86IdcxqmrBAstQ1341nfJges7nQ3DwM6I4l2ilAoYhRIgARIWWHqNFCkSJA0BhUEt8iFB7jm8pQL5t8UIWgdgpmnnCkqDcrwKGlbBJErWiamqTG2RyHwZGR2UcofRlUjC8TvJnjY4PzZZNtGjhkbLCyIKufQp2j4zYosEICJdKZ83xUTaBilNJ4DNpB2E3pvvgMP8Zf4Otf+y06LuPmxlVMEvD+7IBltWyaSQNJoAOqZUl1OmOl1SfupBhTEF/ZoO4qkALlBcFxjZ9ZbLXHG9/479jtp3z+lc9jL73AV+8/5snUMMvOmNeHkBhWepfZHVxHJwmj40cYIfjCldusdAd8a+9tfuet3+HGz3+RIIiojcFZz9ODfTY3BiRJQlkUqMEKeZYRtVqcHuxj6oqk3aGddlAC4jjh8pVLzKdTnG3ya9NOGykVUgbEoebKVUWZ5zhboZQmm8+o6xqhDEkUIGwB2ZR08zqlsxBECCGoi4IyL7HGN58TESCUbiLEpECgG1XdeYyzhFZgXdCsjhiBdR5X19SmpKry5kFKRQidoy20yVjpS0oBVT2BSlAXIVYE2EXJdDqnyGuqOsc6wzizFKZEawVCkE0qNle67K7ESJWzLOWf1B37Ahf4I+OCiF7ghwZ5XXLnySlPxyU66FAVirfeeYsH9z7CBQGHk4zVfoJSAmtqOqmmsAHHp5bLvSGbbsnbWiJCkFqA9KgAgtgTaDAWrAFfeaRrVFgpBVEkSbRCSkkoGne2tRW+MlRSglYEWiI0XL52jZeef56nx8eMJxWnoylFWdDpSZyxiNY63sPKYA13OSdoRURhggo0tspIg5IoCtne3OHqtUtMJ2MWpaeyAaOjx/TaMcO1mxTOEoY5D4slu7evIKqCN84KiqpAjCsKV1NYz9rmGsN2StpqkZchh+MFXoX0+glTk+OFRweKhAhfKZwNcDrBSSCCqzvXOBqfcHR6QBwHTY5hYLBTi5jG2DLntDglOy3ptWOmZxlLWpRC43wGQpNnCwrrwEtWemtUlWeSj2jFXTr9VfZOzthcbZGuXKKlWvREwXC7QBrN5KMAWSuq2qC3JbUSWBnhhEKYmM7uTTY/9xLttTZaRCjdp91ZIwoTimLO+tUXicIue/feZnzyCLE8IZYJKgmZC00hQx4FNUdJzfOtkCuHAUEt0QQokaBkSiC7BDJEiQjpNc5WBB601EhqrJLIIGjqQIXBBgFSVHgnUCcQrEiqsBlzCxpnuxQBUkq8ceBrKrPA+316vQJzFiJtC+ECpHTgTbNrqCUyEmRixGj+AO3t+fphiPPLZn3E1uA1geo3w2LvCLygPW6hKkk1mfPw/e9RzicMlOBTwxUuXb3By/Y5Xj/+kLvTfarAIVoBPdGhijV+WXH9xwbsXB9wOIl5eDxp+s5ryA4yFtMMb2qoDffzKR+lb/D5tTWiZJ/3Ju9jlMWKGl8b5qf7LMoDdqIOX/nd36b/7E2C/IxPpbtc+9wv8fbJY8Igoq4rOt0BTx7sg6iJL+9iakMYRs3YW0ge3rmHVJL+cEAUxyStFlEcky8XlFVBfzAgigIOD54ymUzp9fvErTY2LwmUopW2OTs6YH37EmmaYq3j6PiYUHWo6wXFYkZsIWp3kCogTtuUqkAoCbMMJQNANOo2AUpopFAgZENGRZNna6yE2lNVNXVdsZgfN486UQdTzLBlhqgrFrZiJ6pxhaUyFadLzVwlTDPIzp4Q64AgFERJTDdNyEpIQomUjllRY4wl0JpIODZ6LYYRrK5c7Ihe4E8/LojoBX4oIIBACw6PFhgU7SDm+OyEvb2PuH//I05OpsyXGXuhpteJ6AwS7h9nnI5LLm1uMlzb4d7iDkKBanwtKCWIIkEce6T01JbzSkiPdOdZl17gbWOc0QKSQKKUwsWSOhQQClQo0NqjpEAnm6QrXbpogq7n2q0WRbHk5OSUZZ6zub6BUBE6DthaX2Vl0Mdr3exq2op2aNne2mUymdKOA9bW1jkr4WD/CQN2COKU8XzG6kaHftcjfYRSiicHhqTbobO6Qpll2NEJm+vrrK6uMez3mC/mTKYLOiVI6Xlsx8zFHN329NsSpwVm6vBLgclyilzzyu3rPNffov/CX+J33/gGDx59gMMRCINzS6IVTSEcE1GyzCb02pt0fYeT+ZLnb3+eO3ffYplNm+xMJFGUEIVtTucn+CDmL/+Vv87dd98gilJWBgOwjiAS9N0GXWuYm4qj+0vq0tFdT8n3HZUx+DSlffsFhp/+NBuXdkk1KL9EolFhC68DhNQEIiH0IUm6ys1Xf55CzcnvP+D0N79F2Z6hwoCZhToKsYHnPVWDU9w6SNAkBLKHUl2CoNsoVzJEEeBMhna+2TD2jaKlggDnHSDPTXAObQRMHOyHhKsaq2pKNQdVIIRHixgtHcbXeFezqEeU3UNcCLJICdUaOmhGvpIapypqsSAvJxTFiDQMsFRYt8QLA1Y19ZGyhdZtnLco7+gUbdaWKyweP+TX3/iH/JP/9assy5JACt56+hQftrn5zDNs93+M3z27y1uTj7ChJ271SDs9tF7y7M8+x9bOOr3HU6Zv1UwmC+xZjV/CUHWodMk8LjGR44O45PrT7/GFref58OxDTrMp1jls3TQPHeTvkak2alSxdtdS9nPuHj3gJ7I5L1/7DG+ND5gvRlS15cYLt4mkp9Pr4zw4U+O8J0kStnY2qauSoq7QWjAdnaACTRSneCkYHT4mbnVI0hQvNVEcMzo5IssWdLoDNja2kEHE+PSY4foOSmsmkylRGFAc3GOaGeIyQwlBEEpkXeO8oSiKxiikE6S0aBWiVYw+J6FCCKQKQQQYp7AG6tIQKMPSHGOKMdY48AqFp8hzAlETJyG5NZzNa4w3xIEgMyWxmxMEIUknRsYtHh0tmCwsWS1otdqsxiHtZcEiz+mlMbuDBGdrlIQ4Vn9yN+0LXOCPiAsieoEfCigZsr3yHGUgWOn16LdWmM/HLMsKnaxw/fpK0wutYW2tQxjFHBxM2NjU3NhZxWoo0it4tYvSIJVAaYgTQRACeCrtKfH4GIRtjAqhErRCSRhqQi2JlCIIAwglNhBNdqAEYQXStnh6dMb+yRgdSfK8Il6URLEmsyGlMOhEY61mMc+pipLB2iqVdVRFgZMRSxHwZGwpCs3DkzGDXpdFXlFmniyriFpdZNpn/2jOSqdLbzNhPJrQ27rKp7eu8PT4hFbSZdBfYXU4II4jur0+aXdAuztnuJYTtmIeLksqUyCUJ4kFgVKYricSgkBqyspQC8mBXJAw4dWXP83qypCzyRmz+YIgDGgNHBkzNqqKsjAMklV6osd0POXazhWC1DHNZ1SFxRHRTtpsrGzQGfdpD7a4fu0qT5884Is/e4tBK+TkdI8sz5BRSOCHSDdj60aB0QCSIIno7+7Svn4D1x80GYqnGUY0MUdWWJwcY0+XFARgLb7IUYXBYYlbIe3wEmk9J5stKdZqttYkuYPaNlFM5RDKjRTvIoxM0DKhUglCBkghG3MSKcLROMhxmH7Ncfz0vHte4IPG3e6cA+cQC0kg4qb5SabUsaWOKgwDMDfADBrDFJ7CaFQPbOYRpo8tPNbVWKOpsBgdgevTT18m0B4pu/i6jXcabzzWKZTqYEyMa+IrccZzyoij+3eZHx4TbW6ilcQLR94a8qjbISsKOq2Y7e5VXmx12a/PQGtkSyF0zeMnnuW8YjnyeNsi1IowFKy+tkbgQo7yM2K/oL0WM6bidTvjdr3gyuoNytFDrHNNzut5qL9HEIYRFTF0u+zHc75WPmXn0etMTI+wnTLPc4JWRhhocrPAOYepS4qiJIxirLWMzs7QKqCqHB7o9gacTc4wdclklMGoIknTxoAYpOioB6WjspKnR6MmcslZiv+jvfsLkeuswzj+fXb2T9PtJtm1JYa0Jqm0Qq7aGCQXbW+UNAna+AckIjS2BREULCISCUhvq+iFKBbFYivVFtFibsRGEb1Kaxu3TWq7zSZGNGw2/RO6QdPu7MzPi/NOcrLd3XbWzbznwPOBl3n33TPD8/44OXn3nDkzrVdBxWfInn3tHM3/BGdm2rTPnGP4mquZa56n1Z4m2kGjf4jzMxdoMEwD0a9B+qK4JF98PlcQ7QE6378FIvr6ijPagpGRdcWNeHNNWm8NMTIyRkNtNNdijrdpjbbo62/Qbg0y1AxWvzVL9F/N4NAgb15octXYWkaGZ9m4ZoQ1w6toNudoR3BhtsmqoQFWNcRA3xwzrVkGWu8HDmc5Zpu9V3qXj280qwRJ54GJ3DmW4VqW+OrSiqtrdufurbrmhvpm7yb3xoi47kqGMft/+Iyo1cVERGzLHaJbkp6tY26ob3bn7q265ob6Zq9rbrOF+JY6MzMzM8vCC1EzMzMzy8ILUauLH+cOsEx1zQ31ze7cvVXX3FDf7HXNbfYOvlnJzMzMzLLwGVEzMzMzy8ILUas8STslTUialLQ/d54ySTdI+pOkv0t6UdJX0/gDkk5LGk9td+k530xzmZB0Z8bspyQdTfmeTWNjkg5JOp4eR9O4JH0/5X5B0tZMmT9Uqum4pBlQhgQjAAAEbElEQVRJ91e13pIelnRW0rHSWNc1lrQvbX9c0r5Mub8j6eWU7UlJa9P4JkkXSrV/qPScD6d9bDLN7Yp+1c8iubveN3p9zFkk9xOlzKckjafxytTbbEVEhJtbZRvFR8afAG4EBoHngS25c5XyrQe2pv4I8AqwBXgA+PoC229JcxgCNqe5NTJlPwVcO2/s28D+1N8PPJj6u4HfUXzJ1Xbg6QrUvgGcATZWtd7AHcBW4NhyawyMASfT42jqj2bIvQPoT/0HS7k3lbeb9zrPpLkozW1Xhtxd7Rs5jjkL5Z73++8C36pavd3cVqL5jKhV3UeAyYg4GRGzwOPAnsyZLoqIqYg4kvrngZeADUs8ZQ/weES8HRH/ACYp5lgVe4BHUv8R4JOl8UejcBhYK2l9joAlHwVORMQ/l9gma70j4i/AGwtk6qbGdwKHIuKNiDgHHAJ29jp3RDwVEXPpx8PA9Uu9Rsq+OiIOR0QAj3JprlfEIvVezGL7Rs+POUvlTmc1Pwv8cqnXyFFvs5XghahV3QbgX6Wf/83SC71sJG0CbgWeTkNfSZcxH+5cfqVa8wngKUnPSfpiGlsXEVOpfwZYl/pVyt2xl8v/c656vTu6rXEV53AvxRm3js2S/ibpz5JuT2MbKLJ25Mzdzb5RtXrfDkxHxPHSWNXrbfaeeSFqtgIkXQP8Grg/ImaAHwEfBG4BpigurVXNbRGxFdgFfFnSHeVfprMqlfxYDUmDwF3Ar9JQHer9DlWu8WIkHQDmgMfS0BTwgYi4Ffga8AtJq3PlW0At942Sz3H5H1xVr7dZV7wQtao7DdxQ+vn6NFYZkgYoFqGPRcRvACJiOiJaEdEGfsKly8GVmU9EnE6PZ4EnKTJOdy65p8ezafPK5E52AUciYhrqUe+SbmtcmTlI+gLwceDzaRFNurT9euo/R/H+yptTxvLl+yy5l7FvVKne/cCngSc6Y1Wvt1m3vBC1qvsrcJOkzeks2F7gYOZMF6X3b/0UeCkivlcaL79/8lNA527Yg8BeSUOSNgM3Udxg0FOShiWNdPoUN6IcS/k6d2XvA36b+geBu9Od3duBN0uXl3O47CxR1es9T7c1/j2wQ9Jouqy8I431lKSdwDeAuyLiv6Xx6yQ1Uv9GihqfTNlnJG1P/07u5tJce5m7232jSsecjwEvR8TFS+5Vr7dZ13LfLeXm9m6N4m7iVyj+8j+QO8+8bLdRXFp9ARhPbTfwc+BoGj8IrC8950CaywSZ7mqluCP4+dRe7NQVeB/wR+A48AdgLI0L+GHKfRTYlrHmw8DrwJrSWCXrTbFYngKaFO/Zu285NaZ4T+Zkavdkyj1J8d7Jzn7+UNr2M2kfGgeOAJ8ovc42ioXfCeAHpC9R6XHurveNXh9zFsqdxn8GfGnetpWpt5vbSjR/s5KZmZmZZeFL82ZmZmaWhReiZmZmZpaFF6JmZmZmloUXomZmZmaWhReiZmZmZpaFF6JmZmZmloUXomZmZmaWhReiZmZmZpbF/wDJ1r/Wu0E/LAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQAC6hBC_0Ec"
      },
      "source": [
        "1st approach to update the last layer with fine tuning the others parameters of the network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcGSEoiSjRI4",
        "outputId": "9e753bc7-4766-4f29-c25f-31c769e29499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        }
      },
      "source": [
        "def train_model(model, criterion, optimizer, schedular, path, num_epochs = 25):\n",
        "  since = time.time()\n",
        "  best_model = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.\n",
        "  for e in range(num_epochs):\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "      running_loss = 0.\n",
        "      running_corrects = 0.\n",
        "      for images, labels in dataloaders[phase]:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          out = model(images)\n",
        "          #print(out.shape, 'out', out.dtype)\n",
        "          _, preds = torch.max(out, 1)\n",
        "          #print(phase, 'preds')\n",
        "          loss = criterion(out, labels)\n",
        "          if phase == 'train':\n",
        "            #print('dd')\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          running_loss += loss.item() * images.size(0)\n",
        "          running_corrects += (preds == labels.data).sum()\n",
        "      if phase == 'train':\n",
        "        schedular.step()\n",
        "      epoch_loss = running_loss / data_size[phase]\n",
        "      epoch_acc = running_corrects / data_size[phase]\n",
        "\n",
        "      print('epoch:', e + 1, 'Loss:', epoch_loss, 'Acc:', epoch_acc, 'phase', phase)\n",
        "\n",
        "      if phase == 'val' and epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "  print('Training Complete')\n",
        "  time_lapse = time.time() - since\n",
        "  print('time lapsed:', time_lapse)\n",
        "  print('Best val acc:', best_acc)\n",
        "\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  return model\n",
        "\n",
        "model = models.resnet18(pretrained = True) # We will use pre trained weights\n",
        "num_fetrs = model.fc.in_features # no of input features from the last layer or no of neurons in last layer\n",
        "model.fc =nn.Linear(num_fetrs, 2)\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-3)\n",
        "path = '/content/drive/My Drive/ColabNotebooks/results/state_dict_model.pt'\n",
        "step_lr_schedular = lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.5)\n",
        "model = train_model(model, criterion, optimizer, step_lr_schedular, path)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 Loss: 0.7022977207527786 Acc: tensor(0.5451) phase train\n",
            "epoch: 1 Loss: 0.621767980990067 Acc: tensor(0.6732) phase val\n",
            "epoch: 2 Loss: 0.6100992257477807 Acc: tensor(0.6926) phase train\n",
            "epoch: 2 Loss: 0.5268073171571969 Acc: tensor(0.8105) phase val\n",
            "epoch: 3 Loss: 0.5164351360719712 Acc: tensor(0.8279) phase train\n",
            "epoch: 3 Loss: 0.46454091555152843 Acc: tensor(0.8562) phase val\n",
            "epoch: 4 Loss: 0.4780131031255253 Acc: tensor(0.8115) phase train\n",
            "epoch: 4 Loss: 0.4190218394098718 Acc: tensor(0.8562) phase val\n",
            "epoch: 5 Loss: 0.4462089352920407 Acc: tensor(0.8238) phase train\n",
            "epoch: 5 Loss: 0.3813404037671931 Acc: tensor(0.9281) phase val\n",
            "epoch: 6 Loss: 0.40841398913352217 Acc: tensor(0.8607) phase train\n",
            "epoch: 6 Loss: 0.35594138736818354 Acc: tensor(0.9346) phase val\n",
            "epoch: 7 Loss: 0.39471580747698176 Acc: tensor(0.8648) phase train\n",
            "epoch: 7 Loss: 0.32402273174984003 Acc: tensor(0.9412) phase val\n",
            "epoch: 8 Loss: 0.357173855187463 Acc: tensor(0.9016) phase train\n",
            "epoch: 8 Loss: 0.30532529541090425 Acc: tensor(0.9412) phase val\n",
            "epoch: 9 Loss: 0.3211319148540497 Acc: tensor(0.9098) phase train\n",
            "epoch: 9 Loss: 0.2851712514762006 Acc: tensor(0.9477) phase val\n",
            "epoch: 10 Loss: 0.32564780751212696 Acc: tensor(0.9016) phase train\n",
            "epoch: 10 Loss: 0.269358282482702 Acc: tensor(0.9412) phase val\n",
            "epoch: 11 Loss: 0.33324223854502694 Acc: tensor(0.8770) phase train\n",
            "epoch: 11 Loss: 0.2597866742049946 Acc: tensor(0.9477) phase val\n",
            "epoch: 12 Loss: 0.28790924685900326 Acc: tensor(0.9262) phase train\n",
            "epoch: 12 Loss: 0.2607552259186514 Acc: tensor(0.9412) phase val\n",
            "epoch: 13 Loss: 0.28039270150856893 Acc: tensor(0.9303) phase train\n",
            "epoch: 13 Loss: 0.25946156709801915 Acc: tensor(0.9477) phase val\n",
            "epoch: 14 Loss: 0.2634549082302656 Acc: tensor(0.9180) phase train\n",
            "epoch: 14 Loss: 0.2522236882081998 Acc: tensor(0.9477) phase val\n",
            "epoch: 15 Loss: 0.27828146541704896 Acc: tensor(0.9262) phase train\n",
            "epoch: 15 Loss: 0.24749651794729668 Acc: tensor(0.9542) phase val\n",
            "epoch: 16 Loss: 0.3091343875791206 Acc: tensor(0.8893) phase train\n",
            "epoch: 16 Loss: 0.23837353357302596 Acc: tensor(0.9477) phase val\n",
            "epoch: 17 Loss: 0.27136419737925294 Acc: tensor(0.9057) phase train\n",
            "epoch: 17 Loss: 0.2415621455977945 Acc: tensor(0.9477) phase val\n",
            "epoch: 18 Loss: 0.2753868410821821 Acc: tensor(0.9057) phase train\n",
            "epoch: 18 Loss: 0.23762687836207597 Acc: tensor(0.9477) phase val\n",
            "epoch: 19 Loss: 0.27501760203330244 Acc: tensor(0.9385) phase train\n",
            "epoch: 19 Loss: 0.23362167481503454 Acc: tensor(0.9542) phase val\n",
            "epoch: 20 Loss: 0.270566998446574 Acc: tensor(0.9180) phase train\n",
            "epoch: 20 Loss: 0.23517591384501238 Acc: tensor(0.9477) phase val\n",
            "epoch: 21 Loss: 0.2604299358657149 Acc: tensor(0.9221) phase train\n",
            "epoch: 21 Loss: 0.2270943550697339 Acc: tensor(0.9477) phase val\n",
            "epoch: 22 Loss: 0.2793531999236248 Acc: tensor(0.8811) phase train\n",
            "epoch: 22 Loss: 0.22767655668305417 Acc: tensor(0.9477) phase val\n",
            "epoch: 23 Loss: 0.2337103334606671 Acc: tensor(0.9303) phase train\n",
            "epoch: 23 Loss: 0.22615848140778885 Acc: tensor(0.9477) phase val\n",
            "epoch: 24 Loss: 0.26294247170940777 Acc: tensor(0.9221) phase train\n",
            "epoch: 24 Loss: 0.2295011356959935 Acc: tensor(0.9542) phase val\n",
            "epoch: 25 Loss: 0.22266048217406037 Acc: tensor(0.9344) phase train\n",
            "epoch: 25 Loss: 0.2247009275395886 Acc: tensor(0.9477) phase val\n",
            "Training Complete\n",
            "time lapsed: 1997.4812426567078\n",
            "Best val acc: tensor(0.9542)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TotHXo01_ux_"
      },
      "source": [
        "2nd approac to update the last layer only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5XbF5j-41Oc",
        "outputId": "78c89ff7-6465-4455-afb9-d51ba620973c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "# We need to freeze all the parameters aexcept the final layers\n",
        "RseNet = models.resnet18(pretrained = True) # We will use pre trained weights\n",
        "for param in RseNet.parameters():\n",
        "  param.requires_grad = False\n",
        "num_fetrs = RseNet.fc.in_features # no of input features from the last layer or no of neurons in last layer\n",
        "RseNet.fc =nn.Linear(num_fetrs, 2)\n",
        "\n",
        "RseNet = RseNet.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(RseNet.parameters(), lr = 1e-3)\n",
        "path = '/content/drive/My Drive/ColabNotebooks/results/state_dict_model2nd.pt'\n",
        "step_lr_schedular = lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.5)\n",
        "RseNet = train_model(RseNet, criterion, optimizer, step_lr_schedular, path, num_epochs = 5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 Loss: 0.7454308222551815 Acc: tensor(0.5246) phase train\n",
            "epoch: 1 Loss: 0.7012728643573187 Acc: tensor(0.5098) phase val\n",
            "epoch: 2 Loss: 0.6726972021040369 Acc: tensor(0.5943) phase train\n",
            "epoch: 2 Loss: 0.653304292096032 Acc: tensor(0.6275) phase val\n",
            "epoch: 3 Loss: 0.6025231226545865 Acc: tensor(0.6598) phase train\n",
            "epoch: 3 Loss: 0.5790938286999472 Acc: tensor(0.7190) phase val\n",
            "epoch: 4 Loss: 0.5498653493943761 Acc: tensor(0.7623) phase train\n",
            "epoch: 4 Loss: 0.5459031754849004 Acc: tensor(0.7255) phase val\n",
            "epoch: 5 Loss: 0.5270588358894723 Acc: tensor(0.7459) phase train\n",
            "epoch: 5 Loss: 0.4974967788637074 Acc: tensor(0.8105) phase val\n",
            "Training Complete\n",
            "time lapsed: 185.44696688652039\n",
            "Best val acc: tensor(0.8105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nEgkk-TnaQk"
      },
      "source": [
        "# Tensorboard\n",
        "#Lecture 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBPOtvUwQi8q",
        "outputId": "7a38bea3-1b22-4eb9-c829-83d3bfd5ccfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#---------------------------- TENSORBOARD ---------------------------------------#\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "logs_base_dir = '/content/drive/My Drive/ColabNotebooks/results'\n",
        "#logs_base_dir = 'runs'\n",
        "writer = SummaryWriter(logs_base_dir) # just a path of folder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "in_size = 28 * 28\n",
        "hid_size = 500\n",
        "out_size = 10\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root = './data', \n",
        "                                        train = True, \n",
        "                                        download = True, \n",
        "                                        transform = transforms.ToTensor())\n",
        "\n",
        "test_data = torchvision.datasets.MNIST(root = './data', \n",
        "                                       train = False, \n",
        "                                       download = True, \n",
        "                                       transform = transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 64, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 64, shuffle = True)\n",
        "\n",
        "examples = iter(train_loader)\n",
        "images, labels = examples.next()\n",
        "\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i + 1)\n",
        "  plt.imshow(images[i][0], cmap = 'gray')\n",
        "#plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpa-plXVzIY1"
      },
      "source": [
        "# --------------------- TENSORBOARD ----------------------------#\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "writer.add_image('mnist_images', img_grid)\n",
        "writer.flush()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyeWcRV3zpgq",
        "outputId": "dffea5ee-cce7-45f7-c856-cee9c84331ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "!tensorboard --logdir= 'logs_base_dir'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-23 12:45:26.454882: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
            "                   [--host ADDR] [--bind_all] [--port PORT]\n",
            "                   [--purge_orphaned_data BOOL] [--db URI] [--db_import]\n",
            "                   [--inspect] [--version_tb] [--tag TAG] [--event_file PATH]\n",
            "                   [--path_prefix PATH] [--window_title TEXT]\n",
            "                   [--max_reload_threads COUNT] [--reload_interval SECONDS]\n",
            "                   [--reload_task TYPE] [--reload_multifile BOOL]\n",
            "                   [--reload_multifile_inactive_secs SECONDS]\n",
            "                   [--generic_data TYPE]\n",
            "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
            "                   [--debugger_data_server_grpc_port PORT]\n",
            "                   [--debugger_port PORT]\n",
            "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
            "                   {serve,dev} ...\n",
            "tensorboard: error: invalid choice: 'logs_base_dir' (choose from 'serve', 'dev')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiQO-IpRquc0"
      },
      "source": [
        "# Saving and Loading models\n",
        "# Lecture 17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEhKhPeBqxnr",
        "outputId": "954aafd9-e77e-498c-c587-4268064cd038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "class Check_model(nn.Module):\n",
        "  def __init__(self, in_size, hid_size, out_size):\n",
        "    super(Check_model, self).__init__()\n",
        "    self.lin1 = nn.Linear(in_size, hid_size)\n",
        "    self.lin2 = nn.Linear(hid_size, out_size)\n",
        "\n",
        "  def forward(slf, x):\n",
        "    out = self.lin1(x)\n",
        "    out = self.lin2(out)\n",
        "    return out\n",
        "\n",
        "model = Check_model(4, 5, 2)\n",
        "\n",
        "'''\n",
        "# ------------------------ 3 ways to rmember  -------------------------------- #\n",
        "torch.save(arg, path) # arg can be a dictionary, model or tensor and path is 'dir/some_name.pth'\n",
        "torch.load(path)\n",
        "torch.load_state_dict(arg)\n",
        "'''\n",
        "\n",
        "# ----------------------- Different way of saving -----------------------------#\n",
        "#lazy way to save model\n",
        "path = '/content/drive/My Drive/ColabNotebooks/results/model1.pth'\n",
        "torch.save(model, path)\n",
        "MODEL = torch.load(path)\n",
        "MODEL.eval()\n",
        "for param in MODEL.parameters():\n",
        "  print(param)\n",
        "\n",
        "#Reccomended way of doing it\n",
        "path2 = '/content/drive/My Drive/ColabNotebooks/results/model_state_dict1.pth'\n",
        "torch.save(model.state_dict(), path2)\n",
        "LOADED_MODEL = Check_model(4, 5, 2)\n",
        "LOADED_MODEL.load_state_dict(torch.load(path2))\n",
        "LOADED_MODEL.eval()\n",
        "for p in LOADED_MODEL.parameters():\n",
        "  print(p)\n",
        "\n",
        "print(model.state_dict())\n",
        "print(LOADED_MODEL.state_dict())\n",
        "\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "path3 = '/content/drive/My Drive/ColabNotebooks/results/check_point1.pth'\n",
        "check_point = {'epoch': 100, 'model_state':model.state_dict(), 'optim_state':optimizer.state_dict()} # must be a dictnary\n",
        "torch.save(check_point, path3)\n",
        "\n",
        "loaded_check_point = torch.load(path3)\n",
        "num_epoch = loaded_check_point['epoch']\n",
        "MODEL3 = Check_model(4, 5, 2)\n",
        "OPTIMIZER = torch.optim.SGD(MODEL3.parameters(), lr = 0)\n",
        "MODEL3.load_state_dict(check_point['model_state'])\n",
        "OPTIMIZER.load_state_dict(check_point['optim_state'])\n",
        "print(OPTIMIZER.state_dict())\n",
        "\n",
        "'''\n",
        "# If we use gpu to save and load model\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "torch.save(model.state_dict(), path)\n",
        "\n",
        "device = torch.device('cpu')\n",
        "new_model = model(*args, **kwargs)\n",
        "model.state_dict(torch.load(path, map_location = device))\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2258,  0.4592,  0.0175,  0.1997],\n",
            "        [-0.2059,  0.2313, -0.2581, -0.4328],\n",
            "        [-0.4853,  0.4210, -0.3003,  0.2248],\n",
            "        [ 0.1996, -0.1872,  0.4029, -0.0737],\n",
            "        [ 0.4091, -0.1881, -0.4611,  0.3159]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.4364, -0.0566,  0.4972,  0.2467,  0.1768], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0793, -0.0280,  0.3283, -0.3943,  0.3286],\n",
            "        [ 0.4442,  0.4052,  0.2962, -0.2473, -0.3162]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1672,  0.0996], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.2258,  0.4592,  0.0175,  0.1997],\n",
            "        [-0.2059,  0.2313, -0.2581, -0.4328],\n",
            "        [-0.4853,  0.4210, -0.3003,  0.2248],\n",
            "        [ 0.1996, -0.1872,  0.4029, -0.0737],\n",
            "        [ 0.4091, -0.1881, -0.4611,  0.3159]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.4364, -0.0566,  0.4972,  0.2467,  0.1768], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0793, -0.0280,  0.3283, -0.3943,  0.3286],\n",
            "        [ 0.4442,  0.4052,  0.2962, -0.2473, -0.3162]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1672,  0.0996], requires_grad=True)\n",
            "OrderedDict([('lin1.weight', tensor([[ 0.2258,  0.4592,  0.0175,  0.1997],\n",
            "        [-0.2059,  0.2313, -0.2581, -0.4328],\n",
            "        [-0.4853,  0.4210, -0.3003,  0.2248],\n",
            "        [ 0.1996, -0.1872,  0.4029, -0.0737],\n",
            "        [ 0.4091, -0.1881, -0.4611,  0.3159]])), ('lin1.bias', tensor([-0.4364, -0.0566,  0.4972,  0.2467,  0.1768])), ('lin2.weight', tensor([[-0.0793, -0.0280,  0.3283, -0.3943,  0.3286],\n",
            "        [ 0.4442,  0.4052,  0.2962, -0.2473, -0.3162]])), ('lin2.bias', tensor([-0.1672,  0.0996]))])\n",
            "OrderedDict([('lin1.weight', tensor([[ 0.2258,  0.4592,  0.0175,  0.1997],\n",
            "        [-0.2059,  0.2313, -0.2581, -0.4328],\n",
            "        [-0.4853,  0.4210, -0.3003,  0.2248],\n",
            "        [ 0.1996, -0.1872,  0.4029, -0.0737],\n",
            "        [ 0.4091, -0.1881, -0.4611,  0.3159]])), ('lin1.bias', tensor([-0.4364, -0.0566,  0.4972,  0.2467,  0.1768])), ('lin2.weight', tensor([[-0.0793, -0.0280,  0.3283, -0.3943,  0.3286],\n",
            "        [ 0.4442,  0.4052,  0.2962, -0.2473, -0.3162]])), ('lin2.bias', tensor([-0.1672,  0.0996]))])\n",
            "{'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3]}]}\n",
            "{'state': {}, 'param_groups': [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3]}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya6vaedq0H4e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}